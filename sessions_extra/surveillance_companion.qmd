---
title: Surveillance
description: Companion satellite to the surveillance Fetch-R module
# image: ../img/core/06_epicurves/logo.svg
categories:
  - Satellite
  - Surveillance
---

```{r setup}
#| include: false
#| eval: true
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

library(here)        # For paths
library(rio)         # Import and export data
library(tidyverse)   # Data manipulation
library(zoo)  

# Surveillance data
data_surv_raw <- import(here("data", "surveillance_module", "raw", 
                             "data_ids_sem20_2022.xlsx"))

# Laboratory data
data_lab_raw <- import(
  here("data", "surveillance_module", "raw", "data_labo_sem20_2022.xlsx"), 
  skip = 7  # Skip the first seven lines
) 


data_surv <- data_surv_raw |> 
  mutate(
    
    # Format strings to lower case
    pays    = tolower(pays),
    prov    = tolower(prov),
    zs      = tolower(zs),
    maladie = tolower(maladie),
    
    
    # Remove excess spaces with the str_squish() function from 
    # the stringr package, very usefull!
    prov = str_squish(prov),
    zs   = str_squish(zs),
    
    
    # Remove spaces or "-" with the str_replace() function from 
    # the stringr package
    prov = str_replace(prov, pattern = "-", replacement = "_"), 
    prov = str_replace(prov, pattern = " ", replacement = "_"),
    zs   = str_replace(zs,   pattern = "-", replacement = "_"), 
    zs   = str_replace(zs,   pattern = " ", replacement = "_")
  )

data_lab <- data_lab_raw |> 
  mutate(
    
    # Clean strings
    zs = tolower(zs),    # Format strings to lower case
    zs = str_squish(zs), # Remove excess spaces
    zs = str_replace(zs, "-", "_"),  # Replace - by _
    zs = str_replace(zs, " ", "_"),  # Replace space by _
    
    # Recode igm modalities
    igm_rougeole = case_when(
      igm_rougeole == 'pos' ~ 'positif', 
      igm_rougeole == 'neg' ~ 'negatif', 
      .default = igm_rougeole),
    
    igm_rubeole = case_when(
      igm_rubeole == 'pos' ~ 'positif', 
      igm_rubeole == 'neg' ~ 'negatif', 
      .default = igm_rubeole)
  )

data_surv_weeks <- data_surv |> 
  select(prov, zs, numsem, totalcas, totaldeces) |>
  complete(
    nesting(prov, zs),
    numsem = seq(min(data_surv$numsem, na.rm = TRUE), 
                 max(data_surv$numsem, na.rm = TRUE)),
    fill = list(totalcas = 0, 
                totaldeces = 0)
  ) 


# Analyse
data_alert <- data_surv_weeks |>
  
  # Select the 4 health zones
  filter(zs %in% c("dilolo", "kowe" ,"kampemba", "lwamba")) |>
  
  # Order by province, zs and week
  arrange(prov, zs, numsem) %>%
  
  # Binary indicator for 20 cases (weekly indicator by ZS)
  mutate(
    cases20 = case_when(
      totalcas >= 20 ~ 1, 
      .default = 0)) |>
  
  
  # Cumulative indictors, need to be calculated by ZS
  mutate(
    
    # Group by Province and ZS
    .by = c(prov, zs),
    
    # Cumulative cases over 3 week window (zoo::rollapply)
    cumcas = rollapply(totalcas, 
                       width = 3,        # Window width
                       sum,              # function to apply
                       na.rm = TRUE,     # Arguments to pass to the function (here, sum)
                       align = "right",  
                       partial = TRUE),
    
    # Binary indicator for 35 cumulative cases
    cumcases35 = case_when(cumcas >= 35 ~ 1, 
                      .default = 0),
    
    
    # Combined alert indicator
    # The operator | is a logical OR. Here we test:
    # is c20 equal to 1 OR is cum35 equal to 1
    alert = case_when(
      (cases20 == 1 | cumcases35 == 1) ~ 1, 
      .default = 0)
  )

zs_alert <- data_alert |>
  filter(numsem == 20)  |>
  filter(alert == 1) |>
  pull(zs)
```

## Objectives

- Reuse skills aquired in the FETCH-R modules (import, clean and visualise data)
- More specifically, analyze **alert data** to help decide which alerts to prioritize for further field investigation.

## Introduction

This satellite is a companion to the case study _Measles emergency response in the Katanga region (DRC)_ from the FETCH Surveillance module and may thus not make sense as a standalone document.

From an R point of view, this tutorial builds on skills acquired throughout the FETCH-R modules, introduces a couple of useful generalist functions, and some more specialized ones. 

::: {.callout-tip}
Do not hesitate to refer to past sessions and your own scripts to remind yourself of some functions!
:::


## Setup (Question 2)

Since this is part of a specific module, you will create a new RStudio project. We refer you [this main session](../sessions_core/02_import_data.qmd) for help creating a project and importing your data.

### Setup a new project

:::  {.setup}
1.  Create a folder `surveillance_case_study` associated with the FETCH Surveillance module. Add the following subfolders in it:

- üìÅ data
- üìÅ clean
- üìÅ raw
-   üìÅ R
-   üìÅ outputs

2.  Create an [RStudio project](../sessions_core/02_import_data.qmd#rstudio-projects) at the root of the `surveillance_case_study` folder.

3.  Download the raw data.

```{r}
#| echo: false
#| eval: true

downloadthis::download_link(
  link = 'https://github.com/epicentre-msf/repicentre/raw/main/data/clean/moissala_linelist_clean_EN.rds',
  button_label = "Download raw data",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)
```

<br>
4. Unzip the archive and save the two Excel files it contains in the subfolder `data/raw`.  


5. Create a new script called `import_clean.R` and save it in the `R` subdirectory. Add a section to load the following packages: `{here}`, `{rio}`, and `{tidyverse}`.
:::


### Import data in R

**Reminder from the case study**: you requested access to the routine surveillance data and the laboratory data to the DRC MoH. The MoH agreed to share it with you on a weekly basis. The first dataset you received is of week 20 in 2022 (the data we are working on is simulated).

:::  {.look}
If you have not done it already, open the raw data files in Excel (or another equivalent application) to inspect them.
:::

The surveillance dataset is pretty straightforward to import. The lab dataset is slightly trickier: the data headers do not start at line one like the surveillance dataset. Fear not, the `skip` argument from the `import()` function is made for that sort of case:

```{r}
# DO NOT RUN (PSEUDO-CODE)
import(
  here("data", "raw", "example_file.xlsx"), 
  skip = 3  # Skip the first three lines and start importing from line four.
) 
```

:::  {.write}
1. Add a section to your script dedicated to data importation. 

2. Import the surveillance data and store it into a `data_surv_raw` data frame. Then, import the lab data and save it in a `data_lab_raw` data frame.

3. Verify that the import went well for both data frames (Viewer, check the dimensions or start and tail of data frames).
:::



## Cleaning (Question 2 and 3)

### Surveillance data (Q2)

Now that the data is correctly imported, we are going to perform some more checks, as usual, before a bit of cleaning.

#### Quick checks

During the case study you won't have time to **inspect** and **clean** *all* columns in the imparted time, so for now we will focus on key columns: `zs`, `numsem`, `totalcas` and `totaldeces`.

::: {.callout-note}
If you work on this tutorial in your own time, inspect the quality of the other columns and  cross-check information of several columns. We refer you to the discussion of the case study for more checks to perform.
:::

::: {.write}
Add a section for the exploration and cleaning of the surveillance data into your script. 
<br>
Now, explore the surveillance data frame and answer the following questions:

-   What are the column names?
-   How many provinces are in the dataset? Is this coherent with what you expect?
-   How many health zones are in the dataset?
-   What is the range of weeks?
-   What is the min of `totalcases`?
-   What is the max of the `totaldeces`?
-   Do you notice missing data for these columns? Are the strings of text clean?
:::


#### Clean strings

Ok, now that we have a better idea of what is the state of the data, let's do some cleaning. We are going to write a cleaning pipeline like we did in the main modules (check out your code for the end of the [cleaning modules](../sessions_core/04_data_verbs_conditional.qmd) to see an example final pipeline).


:::  {.callout-tip}
To facilitate debugging your pipeline, add actions one by one and check that each action does what you want before adding another one.
:::

We are going to perform a couple of actions on the columns containing text to remove potential problems:

-   transform them to lower casse
-   remove potential extra spaces
-   replace `-` and spaces by `_`.

Because you may not have the time to do all of text colums, pick the `zs` or the `prov` column for the following action.

:::  {.write}
Start a cleaning pipeline with a `mutate()` that turns the chosen column to lower casse.
:::

Now, we are going to introduce two handy functions for more text cleaning. The first one is the `str_squish()` function from the `{stringr}` package ([help page here](https://stringr.tidyverse.org/reference/str_trim.html)), that removes spaces at the start or end of the strings, and replace multiple spaces in the middle of a string by a single space.

```{r}
#| eval: true

examples <- c(" Trailing spaces     ",
              "Multiple     spaces",
              " Everything     here ")

str_squish(examples)
```

The other function, `str_replace` (also from the [`{stringr}` package](https://stringr.tidyverse.org/reference/str_replace.html?q=str_replace#null)) does what you expect from its name: replace something in a string by something else. It has a `pattern` argument that take the bit of text to be replaced, and a `replacement` arguments that takes the bit of text to use as replacement:

```{r}
#| eval: true

str_replace(
  "HAUT-KATANGA",    # A string of text (or a column, if used in a mutate)
  pattern = "-",     # The bit to replace
  replacement = "_"  # The replacement
)
```

:::  {.write}
Add steps to your mutate to:

-   Remove all unwanted spaces from your chosen column
-   Change the `-` and to `_` in the column (in two steps)

The head of these columns should now be:

```{r}
#| eval: true
#| echo: false

data_surv |> select(pays, prov, zs, maladie) |> head()
```

Store the clean(ish) data frame in a `data_surv` data frame.
:::

#### Save the clean data

:::  {.write}
Use the `{rio}` package to export `data_surv` to a `.rds` file called `IDS_clean` in the `data/clean` subfolder of your project.
:::

### Laboratory data (Q2)

We are going to follow the same steps as before for the lab data, and focus for now on the columns `zs`, `igm_rougeole` and `igm_rubeole`.

#### Quick checks

:::  {.write}
Perform data checks on the colums names and dimensions. What are the categories for `igm_rougeole` and `igm_rubeole`? What do you need to do to clean these columns?
:::

#### Clean and recode strings

:::  {.write}
1. Start a new cleaning pipeline to clean the lab data. As before, for one of the text column, change it to lower casse, remove the extra spaces and replace the or `-` by `_`.

2. Recode at least one of  `igm_rougeole` or `igm_rubeole` columns so that the categories are `negatif`, `positif` and `indetermine`.

3. Store the cleaner version in a `data_lab` data frame

The head of the cleaned columns should now be:

```{r}
#| eval: true
#| echo: false
data_lab |> select(zs, igm_rougeole, igm_rubeole) |> head(10)
```
:::



::: {.callout-tip collapse=true}
You can use the `case_when()` function to recode the IGM columns.
:::


#### Save the clean data

:::  {.write}
Export the `data_lab` data frame to a `.rds` file called `lab_clean` in the `data/clean` subfolder of your project.
:::


### Going further

This is the end of the steps for question 2! If you finished in advance and there is still time, reuse the functions we just saw to clean the other text columns in both datasets and recode both IGM column in the lab dataset.

If you still have time, perform more checks on the data:

- Display the ZS for which the numbers by age group add up to a different number than the total (if any)
- Are there any ZS for which the number of deaths is higher than the total number of cases?
- Are there duplicated lines (fully duplicated, or several values for ZS and week)?
- Are there unrealistic case numbers?


### Complete surveillance dataset (Q3)

During the case study and the data checks, you realized that some weeks are missing from the surveillance dataset. You discussed the possible reasons for it, and the associated problems. Here we are going provide code to create a dataset that contains all weeks (assuming that missing weeks had zero cases and deaths).

To do that, we will use the function [`complete()` from the `{tidyr}` package](https://tidyr.tidyverse.org/reference/complete.html), to add the missing lines and fill the columns containing numbers (`totalcas` and `totaldeces`) with zeros.

Look at the simplified example below: the Kikula health zone has no row for week 2:

```{r}
#| eval: true

# Create simplified data frame for the example, with three weeks
example_df = data.frame(
  prov = rep("haut_katanga", 5),
  zs = c("likasi", "likasi", "likasi", "kikula", "kikula"),
  numsem = c(1, 2, 3, 1, 3),
  totalcas = c(2, 1, 3, 1, 2))

example_df
```

We use the following code to cross province and health zone and make sure that all their combinations have all the possible week values. Since the weeks range from one to three in that toy example, we pass a vector with weeks ranging from one to three

```{r}
#| eval: true

# Complete the missing week in kikula
example_df |> 
  complete(
    nesting(prov, zs),
    numsem = seq(1, 3),  # vector from 1 to 3
    fill = list(totalcas = 0)
  ) 
```

Now both health zones within provinces have values for all three weeks.

It would be good to not have to check the minimum and maximum week ourselves when the data is changing from week to week. To automatize, we can remplace hardcoded values by the smallest and largest week number in the `numsem` column to get the range of weeks in the dataset:


```{r}
#| eval: true

# Complete the missing week in kikula
example_df |> 
  complete(
    nesting(prov, zs),
    numsem = seq(min(numsem, na.rm = TRUE),   # vector ranging from smallest to largest week numbers in dataset
                 max(numsem, na.rm = TRUE)),
    fill = list(totalcas = 0)
  ) 
```


::: {.write}
1. Start a new pipeline that takes the `data_surv` data frame and keeps only the columns `prov`, `zs`, `numsem` and `total cas`. 

2. Add a new step to your pipeline and paste the following code to complete the data frame:

```{r}
complete(
  nesting(prov, zs),
  numsem = seq(min(numsem, na.rm = TRUE), 
               max(numsem, na.rm = TRUE)),
  fill = list(totalcas   = 0, 
              totaldeces = 0 # also add zero to the totaldeces column
  )
) 
```

3. Store the result of the pipeline in a data frame called `data_surv_weeks`. The head of that data frame looks like:

```{r}
#| eval: true
#| echo: false
data_surv_weeks |> head(10)
```

4. When you are done, export that data frame to a `.rds` file called `IDS_data_weeks_clean` in the `data/clean` subfolder of your project.
:::


## Done!

Congratulation, you are done with the cleaning of the surveillance datasets! You can download the solutions for this part of the tutoriel.

```{r}
#| echo: false
#| eval: true

downloadthis::download_link(
  link = 'https://github.com/epicentre-msf/repicentre/blob/main/solutions/extra/surveillance_module_solutions.R',
  button_label = 'Solution File',
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)
```

In the rest of the case study you will analyse the data.



## Defining alerts (Question 4)

### Preparing the dataset (Q4)

We are going to carry on preparing the datasets for the analyses.


#### Subset ZS

To simplify the work, we are going to focus on four health zones: Dilolo, Kampemba, Kowe, and Lwamba.

::: {.write}
Start a new pipeline, which first step is to select only lines of the `data_surv_weeks` with data for the the Dilolo, Kampemba, Kowe, and Lwamba health zones. 
:::


#### Simple weekly indicator

We are first going to calculate the first simple indicator: "does a health zone has 20 or more suspected cases in one week?". This indicator is binary and only considers data in a given health zone and week, which corresponds to individual rows of our data frame.

::: {.write}
Add a `mutate()` to your pipeline, to create a `cases20` column that contains `1` if a given health zone has 20 cases or more in that week, and zero otherwise.

The top of the data frame created by the pipe thus far looks like this:

```{r}
#| echo: false
#| eval: true

data_analyses %>% head(10)
```
:::

#### Cumulative indicator

The second indicator you want to calculate is whether a health zone has more than 35 cumulated suspected cases within three weeks. This is a bit more complicated than the previous case: within heath zone you need to calculate the sum of cases by groups of three weeks, but the groups are not fixed, they are *rolling* across time. We are getting in the teritory of *moving* averages/sums/etc.

##### Cumulative sum

There are many functions that allow us to apply functions in a [cumulative way]{.hovertip bs-toggle='tooltip' bs-title='Checkout the cumsum() and cumprod() functions!'}. Today we are going to show you the `rollapply()` function from the `{zoo}` package, as it is versatile and powerfull. As its name suggests, the `rollapply()` function applies a function in a rolling way to a vector or a column of a dataframe.


If we want to do a cumulative sum of cases over three weeks, we want to apply the `sum()` function over windows of three weeks.

```{r}
#| eval: true
example_vect <- rep(1, time = 10)
example_vect

rollapply(
  data  = example_vect,
  width = 3,       # Width of the window  
  FUN   = sum,     # Function to apply, here the sum   
  align = "right"  # The final windows is aligned to the right
)
```

In the example above we inputed a vector of ten values (only ones) and obtained a vector of lenght height, containing the sums. Obviously the function has a way of dealing with the extremities, and the size of the output is smaller than the size of the input, which would be a problem in a mutate, where we want to create a new columns in a data frame, with thus the same length as the other columns.

Fortunately there are ways to control the behavior of the first extremity (since we have aligned the windows to the right, the last extremity is already dealt with) You have two solutions:

- Fill with NA when there is not enough values to calculate a window of three
- Allow partial sums (some values represent less than three weeks)

The argument `fill = NA` allow the first solution: it pads the extremities with NA (in our case, since we aligned right)

```{r}
#| eval: true
rollapply(
  data  = example_vect,
  width = 3,       # Width of the window  
  FUN   = sum,     # Function to apply, here the sum   
  align = "right", # Windows are aligned to the right
  fill = NA
)
```

Here, the first and second NA correspond to the first two weeks: the function cannot apply the `sum()` yet as the window is not complete yet. In some cases, it is a reasonnable way of dealing with incomplete windows. In our case, we can do better: if there are 40 cases in week 1, we would consider it a cause for alert, because whatever happens in the next two weeks, the indicator is already above 35 in week one. We thus want the cumulative sum to be calculated, from week one. The `partial = TRUE` argument allows this:

```{r}
#| eval: true
rollapply(
  data  = example_vect,
  width = 3,       # Width of the window  
  FUN   = sum,     # Function to apply, here the sum   
  align = "right", # Windows are aligned to the right
  partial = TRUE)
```


This is close to what you need. A last point: you surely remembers that arithmetic operations in R return `NA` if some of the values are `NA` and we usually need to pass the argumpent `na.rm = TRUE` to the functions for them to ignore missing values.

So if we changed our input vector we would obtain this:

```{r}
#| eval: true

example_vect_missing <- c(1, 1, 1, NA, 1, 1)

rollapply(
  data  = example_vect_missing,
  width = 3,       # Width of the window  
  FUN   = sum,     # Function to apply, here the sum   
  align = "right", # Windows are aligned to the right
  partial = TRUE   # Allows calcul to be made even if window is less than three
)
```

This is not what we want. But we can pass the `na.rm = TRUE` argument to `rollapply()` so that it passes it to `sum()`

```{r}
#| eval: true

rollapply(
  data  = example_vect_missing,
  width = 3,       # Width of the window  
  FUN   = sum,     # Function to apply, here the sum   
  align = "right", # Windows are aligned to the right
  partial = TRUE,  # Allows calcul to be made even if window is less than three
  na.rm = TRUE     # Extra unamed argument to be passed to the sum function
)
```


Now that we understand how to use the `rollapply()` function we can use it to calculate the cumulative sum over three weeks.

This is how to do it for one health zone:

```{r}
#| eval: true
example_df = data.frame(
  prov = "Haut Katanga",
  zs   = "Dilolo",
  numsem   = 1:10,
  totalcas = rep(1, times = 10))

example_df %>% 
  mutate(cumcas = rollapply(
    data  = totalcas,
    width = 3,          # Width of the window  
    FUN   = sum,        # Function to apply, here the sum   
    align = "right",    # Windows are aligned to the right
    partial = TRUE,     # Allows calcul to be made even if window is less than three
    na.rm = TRUE        # Extra unamed argument to be passed to the sum function
  )
  )
```

##### By health zone

Now, we want to do this cumulative sum *by health zone*. This is not that complicated: we are going to sort our dataframe properly by health zone and week, and use the `.by` argument to tell the `mutate()` function to perform the action *by health zone*.

::: {.callout-note}
You may remember from the [aggregation session](../sessions_core/05_summary_table.qmd#sec-stratify) how se *summarised* by groups using the `.by` argument in the `summarize()` function. This exactly the same idea, except that instead of returning one value by group (as `summarize()` does), we want to return one value per row (as `mutate()` does).

As a little reminder of how `summarize()` + `by()` works, here is how we would calculate the total number of patients and deceased by province over the whole dataset:

```{r}
#| eval: true
data_surv_weeks %>% 
  summarize(
    .by = prov,  # Do things by province
    cases_tot = sum(totalcas, na.rm = TRUE),
    dead_tot = sum(totaldeces, na.rm = TRUE)
  )
```

:::

::: {.write}
1. Add a step to your previous pipeline to sort the data frame by province, health zone and week with the `arrange()` function.

2. Then add the following code to calculate the cumulative sum:

```{r}
  mutate(
    .by = c(prov, zs)
    cumcas = rollapply(
      data  = totalcas,
      width = 3,          # Width of the window  
      FUN   = sum,        # Function to apply, here the sum   
      align = "right",    # Windows are aligned to the right
      partial = TRUE,     # Allows calcul to be made even if window is less than three
      na.rm = TRUE        # Extra unamed argument to be passed to the sum function
    )
  )
```

3. Add a new step to calculate a binary indicator, `cumcases35` that is `1` if the cumulative sum of cases for that week is equal or above 35 and `0` if not. This can be in the same mutate as above: the grouping will not affect this calculation, that only looks at a single value in each row.

4. Add a new column `alert`, that is `1` if either the `cases20` indicator or the `cumcases35` indicator is `1` and zero otherwise. You can use the `|` operator, that is a logical OR.

5. When the pipe is working, assign the result to a `data_alert` data frame.

`data_alert` should look like this:

```{r}
data_alert %>% head(10)
```

:::

### Health zones in alert (Q4)

After all this work we can *finally* investigate which health zones are in alert in the last week of our dataset (the *now* of the case study, and our latest data, week 20)!

::: {.write}
Filter your data frame to only keep the 20th week. Which health zones are in alert?

Create a vector `zs_alert` that contains the name of the health zones in alert, so that we can use it to filter data from these health zones later.
:::


## Draw the epicurve (Question 4)

Let us draw the epicurves of health zones currently in alert (in alert during week 20).

We have drawn very similar curves in the [epicurve session](../sessions_core/06_epicurves.qmd). Here again we will use the `ggplot()` function with the `geom_col()` geom to create a barplot showing the distribution of cases. Since we already have the number if cases per week we will not have to add a step to *count* it ourselved like we did in previous sessions.


::: {.write}
Draw an epicurve for one of the health zone in alert. 

The graph should look like this (but maybe for another health zone):

```{r}
data_alert |>
  filter(zs %in% c("kampemba")) |>
  ggplot(aes(x = numsem, 
             y = totalcas)) + 
  geom_col(fill = "#2E4573") + 
  theme_bw(base_size = 15) + 
  # facet_wrap(vars(zs)) +   # One graph by ZS
  labs(x = "Week",
       y = "N cases",
       title = "Kampemba ZS (in alert)")
```

:::

The `facet_wrap()` function allows us to plot several subplots in the same graph (see the [faceting satellite](../sessions_extra/faceting.qmd) for more information on faceting):

```{r}
#| eval: true

data_alert |>
  filter(zs %in% zs_alert) |>
  ggplot(aes(x = numsem, 
             y = totalcas)) + 
  geom_col(fill = "#2E4573") + 
  theme_bw(base_size = 15) + 
  facet_wrap(vars(zs)) +   # One graph by ZS
  labs(x = "Week",
       y = "N cases",
       title = "Health zones in alert")

```


## Key indicators for health zones (Question 6)

From the case study:
> You are the only epidemiologist in the team, and you cannot investigate both alerts. You therefore perform an additional detailed data analysis to decide which of the two health zones you are going to investigate.

Let's gather more data on both alerts to help you decide which one to investigate.


### Week of the first alert

::: {.write}
Use the `summarize()` function to display the first week the alert was raised for each health zone in alert. Which health zone started first?

You can store this data frame in `first_alert`.
:::


### Surveillance data indicators

We are going to go back to the full surveillance dataset that contains more columns of interest (it also has missing weeks, but these weeks have no influence on the following indicators).

::: {.write}
1. Add a column `cunder_5` to data_surv` that contains the the number of cases less than five months.

2. Derive, for each health zone in alert, the following indicators:

- The total number of cases
- The total number of deaths
- The total number of less than five year olds
- The CFR in percentage
- The proportion of reported cases under five, also in percentage

The result should loopk like this: 

```{r}
#| echo: false
#| eval: true


data_surv |>
  filter(zs %in% zs_alert) |>
  mutate(
  cunder_5 = c011mois + c1259mois) %>%
  summarise(
    .by = zs,
    nb_cas       = sum(totalcas, na.rm = TRUE),
    nb_deces     = sum(totaldeces, na.rm = TRUE),
    nb_under_5   = sum(cunder_5, na.rm = TRUE),
    cfr          = (nb_deces / nb_cas) * 100,
    prop_under_5 = (nb_under_5 / nb_cas) * 100
  )
```

3. Once you are happy save this data frame in an object called `table_surv` 
:::


### Lab data indicators 

Now we are going to use the laboratory data to derive a couple more indicators.

::: {.write}
For each health zone in alert, derive the following indicators:

- Number of patients tested for measles
- Number of positives for measles
- Proportion of positives for measles
- Number of patient tested for rubeole
- Number of positive for rubeole
- Proportion of positive for rubeole

Save the result in an object called `table_lab` 
:::

