[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "This page will (eventually) contain external resources to continue your R learning journey."
  },
  {
    "objectID": "sessions_extra/data_exploration.html",
    "href": "sessions_extra/data_exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Perform quick exploration of an imported dataset\nProduce frequency tables for variables"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#objectives",
    "href": "sessions_extra/data_exploration.html#objectives",
    "title": "Data Exploration",
    "section": "",
    "text": "Perform quick exploration of an imported dataset\nProduce frequency tables for variables"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#setup",
    "href": "sessions_extra/data_exploration.html#setup",
    "title": "Data Exploration",
    "section": "Setup",
    "text": "Setup\nDependencies. This extra session assumes that you have completed the sessions introduction to R and R studio, and data importation.\n\nFor this session we will work with our raw Moissala measles linelist which can be downloaded here:\n\n\n\n  Course Folder\n\n\n\n Make sure it is appropriately stored in data/raw of your project. Then open a new script called data-exploration.R, and make sure packages {here}, {rio} and {dplyr} are loaded. Finally, import the data into R as an object called df_linelist."
  },
  {
    "objectID": "sessions_extra/data_exploration.html#data-exploration",
    "href": "sessions_extra/data_exploration.html#data-exploration",
    "title": "Data Exploration",
    "section": "Data Exploration",
    "text": "Data Exploration\nRight after importing some data into R, we might want to take a look at it. When talking of data exploration we usually want to do a few things:\n\nExamine dimensions of the data (ie: how many rows and how many columns)\nLook at columns names\nVisualise the first or last few rows\nDetermine the type of the variables\nDetermine the range of values in continuous variables\nObserve the possible values in each categorical variable\n\nThis process is crucial and will allow us to familiarize ourselves with our data and identify issues that will be adressed during the data cleaning step."
  },
  {
    "objectID": "sessions_extra/data_exploration.html#basic-exploration",
    "href": "sessions_extra/data_exploration.html#basic-exploration",
    "title": "Data Exploration",
    "section": "Basic Exploration",
    "text": "Basic Exploration\nThe very first thing you want to know about your data is the dimensions, which refers to the number of rows and number of columns that make up your data. There are several ways to get this information in R:\n\nLook at your environment pane in RStudio and check for your data - the number next to it (5230x25) tells us it is a dataframe with 5230 rows and 25 columns.\nUse dim() on your data to return a vector with both the number of rows and number of columns\nAlternatively, use ncol() to get the number of columns and nrow() for the number of rows\n\nIt’s good to remember these numbers so you can quickly spot if there are unexpected changes to your data during your analysis (ie: more/fewer rows or columns than expected).\n\nUsing the method of your choice, get the dimensions of your dataframe df_linelist."
  },
  {
    "objectID": "sessions_extra/data_exploration.html#variable-names",
    "href": "sessions_extra/data_exploration.html#variable-names",
    "title": "Data Exploration",
    "section": "Variable Names",
    "text": "Variable Names\nBecause we are going to use the variable names very often during our analysis, we want to get familiar with them pretty early on. Also, we need to identify the ones that will need to be renamed during our data cleaning. The function names() returns a vector of all the variable names in our dataframe:\n\nnames(df_linelist)\n\n [1] \"id\"                   \"full_name\"            \"sex\"                 \n [4] \"age\"                  \"age_unit\"             \"region\"              \n [7] \"sub_prefecture\"       \"village_commune\"      \"date_onset\"          \n[10] \"date_consultation\"    \"hospitalisation\"      \"date_admission\"      \n[13] \"health_facility_name\" \"malaria_rdt\"          \"fever\"               \n[16] \"rash\"                 \"cough\"                \"red_eye\"             \n[19] \"pneumonia\"            \"encephalitis\"         \"muac\"                \n[22] \"vacc_status\"          \"vacc_doses\"           \"outcome\"             \n[25] \"date_outcome\"        \n\n\n\nWhat do you think of the names in your dataset? Can you already spot some variables names you would like to rename?"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#inspecting-your-data",
    "href": "sessions_extra/data_exploration.html#inspecting-your-data",
    "title": "Data Exploration",
    "section": "Inspecting Your Data",
    "text": "Inspecting Your Data\nIt is also nice to inspect your data, it may be easier for you to spot some inconsistencies, variables with a lot of missing values, and it will allow you to see what values to expect in each of them. You can print your data in the console by:\n\nRunning the df_linelist object alone (careful though, you may not want to do this if you have a large dataset)\nUse the head() function to see the top 6 rows (you can increase this number using the argument n)\nUse the tail() function to see the last 6 rows (again, you can increase this number using the argument n)\n\nThese methods will only print the first 40 rows of your data at most because that’s the limit of your console. Alternatively, you can use View() to see your data in a tabular form. This will open a new window with your data displayed like like an Excel spreadsheet. Note, this command only displays the data, it doesn’t allow you to modify it.\n\n\n\n\n\n\nTip\n\n\n\nBe very careful with View() on large dataset as this may crash your RStudio session. To avoid this, you can print the output in the console.\n\n\n\nCan you display the first 15 rows of your data? What happen when you change the width of your console pane and run the command again?"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#variable-type",
    "href": "sessions_extra/data_exploration.html#variable-type",
    "title": "Data Exploration",
    "section": "Variable Type",
    "text": "Variable Type\nWe now want to check the type of the different variables. This is important as part of data cleaning involves making sure that numerical variables are type numeric, dates Date, and categorical variables are factor or character. You have already seen the class() function, to check the type of a vector. In R, each variable of a dataframe is a vector. We can extract all the values of that vector using the $ sign, and pass it to the class() function:\n\nclass(df_linelist$age)\n\n\nTry extracting all the values from the sex variable. What is the type of this variable?\n\nYou can also use str() on your dataframe to check the class of all the variables at once:\n\nstr(df_linelist)\n\n\nUse str() to check the data type of each column. Does anything look odd? Remember that you can also use functions like is.character() and is.numeric() if you’d like to test the type of a particular column."
  },
  {
    "objectID": "sessions_extra/data_exploration.html#exploring-continuous-variables",
    "href": "sessions_extra/data_exploration.html#exploring-continuous-variables",
    "title": "Data Exploration",
    "section": "Exploring Continuous Variables",
    "text": "Exploring Continuous Variables\nNow that you know how to extract the values from a variable, you may want to explore some of these values from the numeric variables to check for inconsistencies. Let’s look for some summary statistics for these, and Base R provides many handy functions:\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\nReturns\n\n\n\n\nmin()\nMinimum value\nmin(x)\nSingle minimum value\n\n\nmax()\nMaximum value\nmax(x)\nSingle maximum value\n\n\nmean()\nArithmetic average\nmean(x)\nAverage value\n\n\nmedian()\nMiddle value\nmedian(x)\nMiddle value\n\n\nrange()\nMin and max\nrange(x)\nVector of (min, max)\n\n\nIQR()\nInterquartile range\nIQR(x)\nQ3 - Q1\n\n\nquantile()\nSpecified quantiles\nquantile(x, probs = c(0.25, 0.75))\nRequested quantiles\n\n\nsd()\nStandard deviation\nsd(x)\nStandard deviation\n\n\nvar()\nVariance\nvar(x)\nVariance\n\n\nsum()\nSum of values\nsum(x)\nSum\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThese functions require you to explicitly remove missing values (NA) using the argument na.rm = TRUE\n\n\nYou can extract the values of a variables using $, and pass them to any of those functions.\n\nUse the $ syntax to get:\n\nThe minimum value of age\nThe maximum of muac\n\nAny problems?"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#exploring-categorical-variables",
    "href": "sessions_extra/data_exploration.html#exploring-categorical-variables",
    "title": "Data Exploration",
    "section": "Exploring Categorical Variables",
    "text": "Exploring Categorical Variables\nFinally, let’s look at the values in our categorical variables. For this we can use frequency tables. This is handy as:\n\nIt allows us to quickly see the unique values in a categorical variable\nThe number of observations for each of those categories\n\nThis is done using the function count() from the package {dplyr}, which accepts the a dataframe and the name of one (or more!) column(s) as arguments. It will then count the number of observations of each unique element in that column. For example, let’s see the possible values of the variable sex:\n\ncount(df_linelist, sex)\n\nThe output is a new, smaller dataframe containing the number of patients observed stratified by sex. It seems like this variable requires some recoding… We will do that in a later session.\n\nUsing your linelist data, look into the values for the outcome variable. How does it look?\nNow, try adding the argument sort = TRUE to the count() function. What did this argument do?"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#done",
    "href": "sessions_extra/data_exploration.html#done",
    "title": "Data Exploration",
    "section": "Done!",
    "text": "Done!\nWell done taking a first look at your data!\n\n\n\n Solution File"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "{repicentre}",
    "section": "",
    "text": "Welcome to {repicentre}\nAn open source platform to learn R for humanitarian contexts. What would you like to do?\n\n\n\n\n\nLearn Follow a linear path starting with the basics  Start\n\n\n\n\n\nExplore Browse our full catelogue of self paced tutorisals  Start\n\n\n\n\n\nExpand Go even further with a list of external resouces  Start"
  },
  {
    "objectID": "template.html",
    "href": "template.html",
    "title": "Session Title",
    "section": "",
    "text": "Objective 1\nObjective 2"
  },
  {
    "objectID": "template.html#objectives",
    "href": "template.html#objectives",
    "title": "Session Title",
    "section": "",
    "text": "Objective 1\nObjective 2"
  },
  {
    "objectID": "template.html#setup",
    "href": "template.html#setup",
    "title": "Session Title",
    "section": "Setup",
    "text": "Setup\n\nDescription of something participants need to setup, primarily used at the beginning of a section but can also be used for tasks like setting up an Rproject file, folder structure, etc."
  },
  {
    "objectID": "template.html#main-section-1",
    "href": "template.html#main-section-1",
    "title": "Session Title",
    "section": "Main Section 1",
    "text": "Main Section 1\nYour main section(s) can (and probably should) be boken down into subsections.\n\nSubsection 1\n\n\nSubsection 2"
  },
  {
    "objectID": "template.html#done",
    "href": "template.html#done",
    "title": "Session Title",
    "section": "Done!",
    "text": "Done!\nThis last header let’s students know that they are done with the main material for the day. It should also include a link to the solutions (hosted on github). For example:\n\n\n\n Solution File\n\n\n\nMake sure this link references the main."
  },
  {
    "objectID": "template.html#going-further",
    "href": "template.html#going-further",
    "title": "Session Title",
    "section": "Going Further",
    "text": "Going Further\nAfter your main content is done you should have a section called called “Going Further” for students who finish the main content early. It should include: 1. A mention of one or two satellite sessions that would be relevant extensions of the current material 2. A section with “extra exercise questions” (these don’t need to use the “action blocks” (see below) and can just be a number list as shown below.\n\nExtra Exercises\n\nDo this.\nThen do that."
  },
  {
    "objectID": "template.html#markdown-reminders",
    "href": "template.html#markdown-reminders",
    "title": "Session Title",
    "section": "Markdown Reminders",
    "text": "Markdown Reminders\nThe rest of this document is a reminder on qmd syntax and a basic style guide. Enjoy.\n\nText Formatting\n\nItalic and Bold will turn out like this\nBlock quotes will look like this:\n\n\nThis is a blockquote made using &gt;\n\n\nTooltips can be done using spans (please do not use asides or footnotes)\n\n\n\nCode\nInline coding will turn out like this\nCode blocks will appear like this:\n\n# comment\nprint('hello world')\n\nWarning: For these tutorials, code blocks are not evaluated by default. If you want to evaluate them, you must indicate it specifically.\n\n# comment\nprint('hello back!')\n\n[1] \"hello back!\"\n\ntest &lt;- function(x) {\n  if (x &gt; 1) {\n    return(x)\n  } else {\n    print('nothing to see here')\n  }\n}\n\nNote. We are no longer using solution blocks, instead a single code file will be available at the end of each session contiaining code that runs through all the exercises.\n\n\nCallouts\nIMPORTANT: please do not use callouts not explicitly defined here; they have not been included in the css and therefore will not render well in the final document.\n\n\n\n\n\n\nNote\n\n\n\nThis is a callout using {.callout-note}\n\n\n\n\n\n\n\n\nTip\n\n\n\nComment about a genral tip / trick or best practice.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWarning / comment on something really important.\n\n\n\n\nAction Boxes\nThese are used for things participants are expected to actually do, ie: exercises. They are split into three categories.\n\nDescription of something participants need to setup, primarily used at the beginning of a section but can also be used for tasks like setting up an Rproject file, folder structure, etc.\n\n\nDescription of something participants should observe, investigate, etc.\n\n\nDescription of a coding exercise that participants are expected to complete.\n\n\n\nTabsets\n\nOneTwoThree\n\n\nContent that will show under the first tab\n\n\nContent that will show under the second tab\n\n\nContent that will show under the third tab\n\n\n\n\n\nImages\nYou can insert images by referring to their relative path using markdown syntax or HTML. Note that the markdown syntax does not allow you to modify image size. In either case, make sure to add alt text for accessibility.\nMarkdown style syntax:\n\n\n\nexample image alt text\n\n\nHTML style syntax (with specification of desired size):\n\n\n\nLinking to Other Pages\nEasy, use relative paths within a standard href, ie: link to home page."
  },
  {
    "objectID": "about.html#hey-there",
    "href": "about.html#hey-there",
    "title": "About",
    "section": "Hey There",
    "text": "Hey There\nWelcome to {repicentre}, an open source site developed by Epicentre to support folks learning R for humanitarian contexts. The site is composed of self paced tutorials and has two main options for learning:\n\nLinear. Designed for people with zero prior experience in R, the linear course will walk you through core R concepts using a case study about measles in Chad. The course covers the following concepts:\n\nData Structures and the RStudio Interface\nData Importation\nData Manipulation\nData Cleaning\nData Aggregation\nData Visualization\n\nChoose Your Own Adventure. If you have a bit more experience or if you are looking for a particular subject, feel free to explore the full range of tutorials. Tutorials are tagged with categories and designed to be self contained."
  },
  {
    "objectID": "about.html#recommendations-and-requests",
    "href": "about.html#recommendations-and-requests",
    "title": "About",
    "section": "Recommendations and Requests",
    "text": "Recommendations and Requests\nIs there a topic that you would like to see a tutorial on that isn’t currently available? Great! Feel free to let us know by opening an issue on the GitHub repository associated with this website. If you aren’t familiar with how to open an issue, please get in touch with Cat Eisenhauer instead."
  },
  {
    "objectID": "about.html#contributing",
    "href": "about.html#contributing",
    "title": "About",
    "section": "Contributing",
    "text": "Contributing\nWould you like to help write or maintain some tutorials? Increadible! Please get in touch with Cat."
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html",
    "href": "sessions_core/04_data_verbs_conditional.html",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "",
    "text": "Understand basic conditional logic statements\nLearn how to filter a data frame using filter()\nLearn how to recode variables using case_when()"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#objectives",
    "href": "sessions_core/04_data_verbs_conditional.html#objectives",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "",
    "text": "Understand basic conditional logic statements\nLearn how to filter a data frame using filter()\nLearn how to recode variables using case_when()"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#setup",
    "href": "sessions_core/04_data_verbs_conditional.html#setup",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "Setup",
    "text": "Setup\nDependencies. This session assumes that you know the basics of data manipulation with {dplyr}. If you need a refresher on this, please review the third session in the learning pathway.\n\nThis session will work with the raw Moissala linelist data, which can be downloaded here:\n\n\n\n  Download Data\n\n\n\n Make sure this dataset is saved into the appropriate subdirectory of your R project and create a new script called filtering_and_recoding_practice.R in your R directory. Add an appropriate header and load the following packages: {here}, {rio}, and {tidyverse}.  Finally, add an import section where you use {here} and {rio} to load your data into an object called df_raw."
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#using-conditional-logic-to-filter-data",
    "href": "sessions_core/04_data_verbs_conditional.html#using-conditional-logic-to-filter-data",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "Using Conditional Logic to Filter Data",
    "text": "Using Conditional Logic to Filter Data\nIn the last session we learned a lot of the core data verbs in {dplyr} for basic manipulation tasks like selecting particular variables of interest and modifying them to better suit our needs. Beyond selecting variables of interest, another common task we have as epidemiologists is selecting observations of interest; ie: filtering our data to look at particular observations that meet a certain criteria.\nFortunately, {dplyr} has our back with the conveniently named function, filter(). To understand how to use it, however, we will need to learn a bit about how to construct conditional logic statements in R. This will be the focus of our session today.\n\nThis Equals That\nThe basic syntax of filter() is pretty simple:\n\n# DO NOT RUN (PSEUDO-CODE)\ndf_raw |&gt;\n  filter([conditional logic statement]) # Keep lines where statement is TRUE\n\nBut what is a conditional logic statement? These are statements that ask “Is this thing true?”. The simplest conditional logic statement asks “does this variable equal this value?”. For example, “was this patient hospitalized?”. In R, we can ask if one value equals another using ==.\nTo create a filter asking, for each observation, whether the value of hospitalization is equal to yes we can then use the following syntax:\n\ndf_raw |&gt;\n  filter(hospitalisation == 'yes')\n\nWhat filter() is doing here is going down each row of our dataset and asking: “for this row, is the value of hospitalisation equal to \"yes\"?”. It then returns only the rows where the answer to this question is TRUE.\n\nCreate a filter that selects all of the patients who had a fever, ie: where the value of fever was \"Yes\". The head of fever should look like this:\n\n\n  fever\n1   Yes\n2   Yes\n3   Yes\n4   Yes\n5   Yes\n6   Yes\n\n\nTake a look at your output and then take a look at the head of df_raw. Why does df_raw still contain patients who didn’t present with fever?\n\n\n\nThis Does Not Equal That\nChecking if something is the same is great, but a lot of the time we might have another question in mind. For example, we might want to know how many patients didn’t recover, whether this was because they died or because they left against medical advice.\nIn this case, instead of writing == we will instead use !=. So, for example if we want to select all observations where patients didn’t recover we would write:\n\ndf |&gt;\n  filter(outcome != 'recovered')\n\n\nCreate a filter that selects patients who did not have a card confirmed vaccination status. The head of vacc_status should look like this:\n\n\n  vacc_status\n1          No\n2  Yes - oral\n3          No\n4          No\n5          No\n6          No\n\n\nHint. Remember that you can use count() to check what the options were for vacc_status.\n\n\n\nGreater Than / Less Than\nThe other common question we have is whether a value was greater or less than a particular threshold. For example, how many patients were under 5 years old? Here we will use &lt; and &gt; to evaluate whether a variable is less than or greater than a particular value, respectively.\nFor example, to ask how many patients were less than 60 months old we can write:\n\ndf_raw |&gt;\n  filter(age &lt; 60)\n\n\nCreate a filter that selects all patients with severe accute malnutrition (ie: patients with a MUAC less than 110). The head of muac should look like this:\n\n\n  muac\n1   80\n2   88\n3   60\n4   85\n5   86\n6   68\n\n\nNow create another filter that selects patients who are over 15 years old. The head of your age column should look like this:\n\n\n  age\n1 348\n2 348\n3 312\n4 432\n5 444\n6 324\n\n\n\nSometimes, instead of asking if something is less or greater than a particular value, we want to ask if it is less than or equal to that value. Easy, we just need to add an equal sign! We write &lt;= for “less than or equal to” and &gt;= for “greater than or equal to”. Careful here, the = must come after &lt; or &gt;, not before.\nSo if we want to ask for how many patients were 10 years of age or younger, we can write:\n\ndf_raw |&gt;\n  filter(age &lt;= 120)\n\n\nCreate a filter that selects all patients with a normal nutrition status, ie: patients with a MUAC greater than or equal to 125. The head of muac should look like this:\n\n\n  muac\n1  244\n2  232\n3  210\n4  220\n5  152\n6  155\n\n\n\n\n\nFilters with Multiple Conditions\nWant to combine several logic statements in a single filter? Easy. We can create a filter with multiple conditions by simply separating each condition with a comma:\n\n# DO NOT RUN (PSEUDO-CODE)\ndf |&gt;\n  filter([condition 1],\n         [condition 2],\n         [condition 3])\n\nSo for example, let’s say we want to select all patients under five who were hospitalized. In this case we can write:\n\ndf_raw |&gt;\n  filter(age &lt; 5,\n         hospitalised = \"true\")\n\n\nCreate a filter that selects all patients with severe accute malnutrition who were hospitalized in the Koumra health facility. The head of id, sub_prefecture, hospitalisation, and muac should look like this:\n\n\n    id sub_prefecture hospitalisation muac\n1 8624         KOUMRA             yes  103\n2 8939         KOUMRA             yes   67\n3 9957         KOUMRA             yes   71\n\n\nHint. This filter has a condition on both hospitalisation status, sub_prefecture, and muac.\n\n\n\nSummary of Basic Logic Statements\nGood job working through a quick tour of logic statements in R! Here is a handy table to help you remember the main logic statements we have learned so far:\n\n\n\nStatement\nR\n\n\n\n\nIs A the same as B?\nA == B\n\n\nIs A not the same as B\nA != B\n\n\nIs A greater than B?\nA &gt; B\n\n\nIs A greater than or equal to B?\nA &gt;= B\n\n\nIs A less than B?\nA &lt; B\n\n\nIs A less than or equal to B?\nA &lt;= B"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#recoding-with-case_when",
    "href": "sessions_core/04_data_verbs_conditional.html#recoding-with-case_when",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "Recoding with case_when()",
    "text": "Recoding with case_when()\nAs we have seen, conditional logic statements are incredibly useful when trying to filter our data, but you will find that they have many other uses as well. One of their other major use cases for us as epidemiologists is when we need to recode our data. This is where the {dplyr} function case_when() is here to help us.\nThe syntax of case_when() is a little more advanced than what we have seen so far, but we will go slowly and break it down. Once you get the hang of it, case_when() will become a very powerful part of your R toolbelt.\nWe will almost always use case_when() inside of a mutate(), because we will use it either to recode an existing variable or to create a new one. The basic syntax works like this:\n\n# DO NOT RUN (PSEUDO-CODE)\ndf |&gt;\n  mutate(column_name = case_when([first condition] ~ [value when condition is TRUE],\n                                 [second condition] ~ [value when second condition is TRUE],\n                                 .default = [default value])\n\nOk, that’s a lot. Let’s break it down.\nSo the first thing to notice is that, with the exception of the last line, each line inside of case_when() has the following format:\n\n[condition] ~ [value when condition is TRUE]\n\nSo for example, if we want our case_when() to say that anytime a patient had a MUAC less than 110 we want to have a value of \"SAM\", we would have something like this:\n\nmuac &lt; 110 ~ 'SAM'\n\nWe can add multiple possible outcomes by adding additional lines. In this case, our next condition might check if the patient is moderately but not severly malnourished using the statement muac &lt; 125 ~ 'MAM'.\nThe last line, with the argument .default gives the value we want case_when() to use when none of the above conditions have been met. In this case, we might give the value 'Normal'.\nTo put this together, if we wanted to create a variable that classifies the malnutrition status of patients using their MUAC, we would write:\n\ndf_raw |&gt;\n  mutate(malnut = case_when(muac &lt; 110 ~ 'SAM',\n                            muac &lt; 125 ~ 'MAM',\n                            .default = 'Normal'))\n\n\nTry running the above code to see if it successfully creates a new column malnut with the malnutrition status of each case. You should get something like this:\n\n\n  muac malnut\n1  244 Normal\n2  232 Normal\n3  123    MAM\n4  210 Normal\n5   80    SAM\n6  220 Normal\n\n\n\nBe careful. The order of your statements is important here. What case_when() will do is go through each statement from top to bottom and assign the first value that is TRUE. So in our above example, case_when() will ask the following questions in sequence:\n\nDoes this patient have SAM (is muac &lt; 110)? If so, assign the value \"SAM\"\nIf the patient didn’t have SAM, do they have MAM (is muac &lt; 125)? If so, assign the value `“MAM”\nIf none of the above conditions were true, assign the default value \"Normal\"\n\n\nTry reordering the first and second conditions in the above case_when() so that you first check if muac &lt; 125. The head of your new data frame should now look like this:\n\n\n  muac malnut\n1  244 Normal\n2  232 Normal\n3  123    MAM\n4  210 Normal\n5   80    MAM\n6  220 Normal\n\n\nNotice anything different? Save this new data frame to a tmp object and inspect it to see if we still have any patients classified as \"SAM\". Can you figure out why this no longer gives the correct classification?\n\n\n\n\n\n\n\nNote\n\n\n\nThe .default argument in case_when() is not obligatory. If you don’t include it then case_when() will use NA by default.\n\n\nAs we saw in our above example, case_when() is an easy way of creating new variables based on the values of an existing column. This can be used to classify status (as we saw with malnutrition) or to regroup variables into categories (like age groups).\n\nUse case_when() to create a new variable age_group with three categories: \"&lt; 5 Years\", \"5 - 15 Years\", and \"&gt; 15 Years\". Patients missing age data should be assigned a default value of \"Unknown\". Be careful with your ordering! The head of your new column should look like this:\n\n\n  age     age_group\n1  36     &lt; 5 Years\n2   5     &lt; 5 Years\n3 156 5 - 15  Years\n4   8     &lt; 5 Years\n5   7     &lt; 5 Years\n6   4     &lt; 5 Years\n\n\n\n\nThe %in% operator\nSo now we can regroup variables into categories, great. But we can also use case_when() to standardize the values we see in a variable.\n\nUsing count() inspect the categorical variables in df_raw to check if any have inconsistencies in their coding.\n\nIn our dataset, we see that there are some issues in the way sex was coded. For example, female patients are coded as f, female and femme. That simply won’t do. Thankfully, we can use case_when() to recode this variable. This time, instead of creating a new variable we will directly update sex:\n\ndf_raw |&gt;\n  mutate(sex = case_when(sex == \"f\" ~ \"Female\",\n                         sex == \"female\" ~ \"Female\",\n                         sex == \"femme\" ~ \"Female\",\n                         sex == \"m\" ~ \"Male\",\n                         sex == \"male\" ~ \"Male\",\n                         sex == \"homme\" ~ \"Male\",\n                         .default = \"Unknown\"))\n\nWell, that works, but it seems awfully repetitive. It would be easier if we could just list all the options that we want to reassign to “Female” and “Male” respectively. This is where the %in% operator is here to help. The %in% operator will check if a value is in a vector of options using the following basic syntax:\n\n# DO NOT RUN (PSEUDO-CODE)\n[value] %in% [vector_of_options]\n\nSo, for example, we could check if the value \"f\" is in the options \"f\", \"female\" using the following:\n\n\"f\" %in% c(\"f\", \"female\")\n\n\nTry running the above statement. What is the data type of your outcome?\n\nSee how the outcome of the above statement is a boolean, ie: a logic outcome? That means we can use it as a condition in case_when()! This means that our verbose code above can now be written as:\n\ndf_raw |&gt;\n  mutate(sex = case_when(sex %in% c(\"f\", \"female\", \"femme\") ~ \"Female\",\n                         sex %in% c(\"m\", \"male\", \"homme\") ~ \"Male\",\n                         .default = \"Unknown\"))\n\nMuch nicer.\n\nUse case_when() and the %in% operator to create a new column vacc_status_strict that has the value \"Yes\" for cases with card confirmed vaccination status, \"No\" for cases who said they were unvaccinated, and \"Unverified\" otherwise. The head of your new column should look like this:\n\n\n  vacc_status_strict\n1         Unverified\n2                 No\n3         Unverified\n4                 No\n5                 No\n6                 No"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#a-last-bit-of-cleanup",
    "href": "sessions_core/04_data_verbs_conditional.html#a-last-bit-of-cleanup",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "A Last Bit of Cleanup",
    "text": "A Last Bit of Cleanup\nNow that we know how to leverage case_when() and conditional logic (in addition to what we learned in the last session, we can actually put together a decent cleaning pipeline. I hope you kept your code from last time handy…\n\nUsing what you have learned above and what you practiced in the last session, create a basic data cleaning pipe that creates a new data frame, df, after doing the following:\n\nRemove the variables full_name and age_unit\nRename the following variables:\n\nage becomes age_months\nsub_prefecture becomes prefecture\nvillage_commune becomes village\nhealth_facility_name becomes facility\n\nAdd a variable age_years with patient age in years\nUpdate region and prefecture to use title case\nUpdate all date columns to use Date type\nCreate a new variable age_group age to include the groups: &lt; 6 months, 6 - 11 months, 12 - 59 months, 5 - 15 years, and &gt; 15 years (patients with unknown age should have a value “Unknown”)\nRecode sex to have only the values: Female, Male, and Unknown\nRemove any duplicate observations\n\nThe head of your final data should look something like this:\n\n\n  id    sex age_months  region prefecture        village date_onset\n1  1 Female         36 Mandoul   Moissala Sangana Koïtan 2022-08-13\n2  2 Female          5 Mandoul   Moissala      Mousdan 1 2022-08-18\n3  3 Female        156 Mandoul   Moissala     Djaroua Ii 2022-08-17\n4  6   Male          8 Mandoul   Moissala     Monakoumba 2022-08-22\n5  7   Male          7 Mandoul   Moissala      Tétindaya 2022-08-30\n6 10   Male          4 Mandoul   Moissala      Danamadja 2022-08-30\n  date_consultation hospitalisation date_admission\n1        2022-08-14             yes     2022-08-14\n2        2022-08-25             yes     2022-08-25\n3        2022-08-20            &lt;NA&gt;           &lt;NA&gt;\n4        2022-08-25              no           &lt;NA&gt;\n5        2022-09-02              no           &lt;NA&gt;\n6        2022-09-02             yes     2022-09-02\n                         facility malaria_rdt fever rash cough red_eye\n1 Hôpital du District de Moissala    negative    No &lt;NA&gt;   Yes      No\n2 Hôpital du District de Moissala    negative    No   No   Yes      No\n3                      CS Silambi    negative   Yes &lt;NA&gt;    No      No\n4 Hôpital du District de Moissala    negative    No   No    No    &lt;NA&gt;\n5                      CS Silambi    negative  &lt;NA&gt;   No   Yes     Yes\n6                    Moissala Est    negative   Yes   No    No    &lt;NA&gt;\n  pneumonia encephalitis muac vacc_status vacc_doses   outcome date_outcome\n1        No           No  244        &lt;NA&gt;       &lt;NA&gt; recovered   2022-08-18\n2      &lt;NA&gt;           No  232          No       &lt;NA&gt;      &lt;NA&gt;   2022-08-28\n3        No         &lt;NA&gt;  123  Yes - oral       &lt;NA&gt; recovered         &lt;NA&gt;\n4        No           No  210          No       &lt;NA&gt; recovered         &lt;NA&gt;\n5        No           No   80          No       &lt;NA&gt; recovered         &lt;NA&gt;\n6        No           No  220          No       &lt;NA&gt; recovered   2022-09-03\n   age_years      age_group\n1  3.0000000 12 - 59 months\n2  0.4166667     &lt; 6 months\n3 13.0000000   5 - 15 years\n4  0.6666667  6 - 11 months\n5  0.5833333  6 - 11 months\n6  0.3333333     &lt; 6 months\n\n\n\nAmazing! Let’s look at how to save this (mostly) clean dataset. Here, we will use the function export() from {rio} and here() from {here} to specify where to save our output:\n\ndf |&gt;\n  export(here('data', 'clean', 'measles_linelist_clean.xlsx'))\n\nNotice here that we are putting our data in the appropriate clean subfolder of data.\n\n\n\n\n\n\nTip\n\n\n\nIn the above example we save our data as an xlsx, which is helpful if you want to be able to open the clean data in Excel. Often, however, we might prefer to use a file with the extension .rds instead. This is a file type specific to R and is more robust to issues related to encoding or date formatting than files like xlsx or csv. To save your above file as an rds all you need to do is change the extension:\n\ndf |&gt;\n  export(here('data', 'clean', 'measles_linelist_clean.rds'))"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#done",
    "href": "sessions_core/04_data_verbs_conditional.html#done",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "Done!",
    "text": "Done!\nVery well done. You’ve learned how to use basic data verbs, conditional logic, and create a basic data cleaning pipeline.\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#going-further",
    "href": "sessions_core/04_data_verbs_conditional.html#going-further",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "Going Further",
    "text": "Going Further\n\nExtra Exercises"
  },
  {
    "objectID": "sessions_core/05_summary_table.html",
    "href": "sessions_core/05_summary_table.html",
    "title": "Summary Tables",
    "section": "",
    "text": "Create contingency tables with count()\nCompute summary statistics by group using summarize()\nReview how to subset rows using filter() and create/modify variables with mutate()\nCreate ordered categorical variables"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#objectives",
    "href": "sessions_core/05_summary_table.html#objectives",
    "title": "Summary Tables",
    "section": "",
    "text": "Create contingency tables with count()\nCompute summary statistics by group using summarize()\nReview how to subset rows using filter() and create/modify variables with mutate()\nCreate ordered categorical variables"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#setup",
    "href": "sessions_core/05_summary_table.html#setup",
    "title": "Summary Tables",
    "section": "Setup",
    "text": "Setup\nDependencies. This session assumes that you know how to use RStudio that you are able to import data and that you know th basic data handling verbs that we have seen in the core sessions so far. If you need a refresher on either of these topics, we encourage you to review the core sessions in the learning pathway.\n\nThis session will use the clean version of the Moissala measles dataset.\n\n\n\n  Course Folder\n\n\n\n Open your RStudio Project and create a new script in the R folder called tables.R with appropriate metadata and a “Packages” section that imports: {rio}, {here} and {tidyverse}. Add an “Import Data” section that loads the cleaned version of the measles linelist."
  },
  {
    "objectID": "sessions_core/05_summary_table.html#introduction-data-aggregation",
    "href": "sessions_core/05_summary_table.html#introduction-data-aggregation",
    "title": "Summary Tables",
    "section": "Introduction: Data aggregation",
    "text": "Introduction: Data aggregation\nOK so let’s recap, you have just performed one of the most important tasks of an epidemiologist: the data cleaning. Now that you have clean and standardized data, you can get into the real business and start analysing them. Analyses typically start with some tables and summaries that describe our data:\n\nUnivariate frequency tables to count occurrences of different values\nSummary statistics of numerical variables (mean, median, standard deviation)\nCross-tabulations to examine relationships between categorical variables\nGroup-wise summaries to compare statistics across different subsets of the data"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#counting-multiple-columns-contingency-tables",
    "href": "sessions_core/05_summary_table.html#counting-multiple-columns-contingency-tables",
    "title": "Summary Tables",
    "section": "Counting Multiple Columns (Contingency Tables)",
    "text": "Counting Multiple Columns (Contingency Tables)\nDuring the data exploration session, you have learned to create a frequency table for a single categorical variable using the count() function. This is nice, but we often want to count the number observations based on two (or more!) variables.\nThese tables are called contingency tables. For example, knowing the number of patients by sub_prefecture is great but we might want to stratify by both sub_prefecture and age_group to see if certain areas have unusually old patients. Doing this is easy, you just need to pass multiple column names to count():\n\ndf_linelist |&gt;\n  count(sub_prefecture, age_group)\n\n\nCreate a new summary table counting the number of patients stratified by sub_prefecture and hospitalisation. What happens if you change the order of the arguments given to count()?  Now, using count(), answer the following questions:\n\nHow many patients were female? What is the proportion?\nWhat are all the possible values of the outcome variable?\nHow many patients between 1 - 4 years have recovered?"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#filtering-out-nas",
    "href": "sessions_core/05_summary_table.html#filtering-out-nas",
    "title": "Summary Tables",
    "section": "Filtering out NAs",
    "text": "Filtering out NAs\nWhen looking at the categories of outcome, you should have spotted that some patients have missing values (NA):\n\ndf_linelist |&gt;\n  count(outcome) |&gt;\n  mutate(prop = n / sum(n))\n\n\nObserve the output of the code above. How can you also call the proportion of patients who died? Are you happy with this calculation?\n\nThe proportion of cases that died is also referred to as the Case Fatality Ratio (CFR). To precisely calculate the CFR we need to make sure that the denominator only includes patient for whom we are sure of their outcome (ie we need to remove all cases with NA or left aginst medical advice).\nRemember that we can do this using filter(). To filter for missing values (NA) in a variable we can use the small function is.na(outcome). Adding a ! in front will do the opposite: removing missing values from outcome:\n\ndf_linelist |&gt;\n  filter(\n    outcome != \"left against medical advice\", \n    !is.na(outcome)\n  ) |&gt;\n  count(outcome)\n\n\nWhich other conditionnal statement could you use in filter() to obtain the same results\n\nNow that we have removed the patients with unknown outcomes, we can add this before creating our frequency table to get the right CFR.\n\nUsing your filter, update your code to summarize the observed number of patients who survived and died as well as the CFR (proportion who died). Store this new dataframe into an object, cfr_df.\n\n\n\n\n\n\n\nTip\n\n\n\nBonus. A useful “shortcut” function is drop_na() from the package {tidyr} that equates to filter(!is.na()).\n\ndf_linelist |&gt;\n  drop_na(outcome) |&gt;\n  count(outcome)\n\ndrop_na() is particularly useful as you can give it multiple column names to filter by. But be careful that doing so will remove all rows where one or more of those columns have a missing value."
  },
  {
    "objectID": "sessions_core/05_summary_table.html#sec-stratify",
    "href": "sessions_core/05_summary_table.html#sec-stratify",
    "title": "Summary Tables",
    "section": "Summary Table: Statistics by Sub Prefecture",
    "text": "Summary Table: Statistics by Sub Prefecture\nOk now that we have produced some simple frequency and contingency tables we may want to increase the complexity. A common task in epidemiology is to look at summary statistics within subsets of the data.\nFor example, we may be asked to produce patient statistics at the sub-prefecture level, ie: for each sub-prefecture in the data, we need to answer the following questions:\n\nHow many patients consulted?\nWhat is their average age?\nWhat was the earliest date of admission?\nHow many patients have been hospitalized?\nAmong children under 6 months, how many have died?\n\nThis is exactly what the function summarize() has been made for! It allows us to calculate summary statistics on a dataset, and the syntax is similar to that of mutate():\n\n# DO NOT RUN (PSEUDO-CODE)\ndf |&gt;\n  mutate(new_col = function_to_create(existing_col))\n\ndf |&gt;\n  summarize(\n    .by = grouping_variable,\n    new_col = summary_function(existing_col)\n  )\n\nConsider the following code, here we are summarizing the data to calculate the average age across all patients.\n\ndf_linelist |&gt;\n  summarize(mean_age = mean(age))\n\n  mean_age\n1 6.822047\n\n\nNotice that this code yields a single value for average age. No grouping variable was provided so summarize() returned one summary statistic for the whole dataset. To calculate the average age by a specific strata, we need to specify a grouping variable using the .by argument:\n\ndf_linelist |&gt;\n  summarize(\n    .by = sex,  # Make the summary (here, the mean) by sex\n    mean_age = mean(age)\n  )\n\n  sex mean_age\n1   f 6.773824\n2   m 6.869855\n\n\n\nTake a look at the above results. How would you interpret them?\n\nNow that we can use summarize() we can use it to calculate some proper summary statistics by sub-prefecture. Let’s start by calling an empty summarize() and grouping the data on sub_prefecture.\n\nRun the following code:\n\ndf_linelist |&gt;\n  summarize(\n    .by = sub_prefecture\n  )\n\nWhat happens when you run these lines?\n\n\nCounts\nWe first want to look at the number of cases in each sub_prefecture. This can be done using the helper function n():\n\ndf_linelist |&gt;\n  summarize(\n    .by = sub_prefecture,\n    n_patients = n() # Count stuffs\n  )\n\n\nOk now let’s build a summary table for each sub_prefecture. First start by replicating the above lines\n\n\n\nSummarizing Continuous Variables\nWe can then use the mean(), median(), min(), max() functions (and other) to produce summaries for continuous variables. For example the average age:\n\ndf_linelist |&gt;\n  summarize(\n    .by = sub_prefecture,\n    n_patients = n(),\n    mean_age = mean(age)\n  )\n\n\nAdd the minimum date of admission to your table for each of the sub_prefecture? Are you happy with the results?\n\n\n\n\n\n\n\nTip\n\n\n\nRemember that with the arithmetic functions such as mean(), median(), min(), max(), you need to explicitly tell R to remove NA.\n\n\n\n\nCounting with a Condition\nWe may also be interested in looking at the number of patients (rows) that fit a condition: the number of patients that were female. Counting by a logical condition can be done with the following syntax:\n\n# DO NOT RUN (PSEUDO-CODE)\nsummarize(\n  sum_category = sum(LOGIC_TEST, na.rm = TRUE)\n  )\n\nThis sum allows us to count all the lines where our condition was met (returns TRUE). For example:\n\ndf_linelist |&gt;\n  summarize(\n    .by = sub_prefecture,\n    n_female = sum(sex == \"f\", na.rm = TRUE)\n  )\n\n\nAdd a variable to your table that counts the number of patients that have been hospitalized. (ie: rows that have yes in variable hospitalisation)\n\n\n\nOther Statistics\nSometimes we want to produce a more complicated statistic, for example the mean age of all hospitalized patients. Here the syntax is a bit different:\n\n# DO NOT RUN (PSEUDO-CODE)\ndf |&gt;\n  summarize(\n    mean_category = mean(col_to_use[LOGIC_TEST], na.rm = TRUE)\n    )\n\nHere, we have:\n\nStated what summary statistic we want to use (mean())\nIndicated which column we want to calculate that statistic on (col_to_use)\nCreated a condition of which observations in that column to use in the calculation ([LOGIC_TEST])\n\nTo give a concrete example, if we wanted to compute the mean of the age variable but only for hospitalized patients (ie: in rows where hospitalisation == \"yes\") we would write:\n\ndf_linelist |&gt;\n  summarize(\n    .by = sub_prefecture,\n    n_patients = n(),\n    mean_age_hosp = mean(age[hospitalisation == \"yes\"], na.rm = TRUE)\n  )\n\nThe use of a logical test in the example above is called logical indexing, where a condition is essentially being used to filter which observations you want to consider when performing a calculation. Logical indexing is very powerful but can also take some getting used to, so don’t worry too much if it isn’t perfectly clear at this stage.\n\nCan you use this syntax to calculate the mean age of female patients in your table?\n\nThat is looking great! We are starting to get a pretty exhaustive grouped summary table with a lot of useful information by sub_prefecture! An extra challenge for you:\n\nCHALLENGE: Could you add a variable to your table that counts the number of patients that died among the ones that are &lt; 6 months old.\n Hint. You want to count rows (so use sum()) that fill a specific condition for outcome (outcome == \"dead\"), but only when age_group == \"&lt; 6 months\"\n\n\n\nUse the Output\nFinally, remember that summarize() returns a dataframe that we can then further manipulate (eg: with filter() and mutate()).\n\nAdd a mutate() after producing your summary table to calculate:\n\nThe proportion of hospitalized patients per sub-prefecture\nThe proportion of female patients per sub-prefecture\n\n\nThe head of your final table should look like this:\n\n\n  sub_prefecture n_patients mean_age min_admission n_female n_hosp\n1       Moissala       1808 6.842920    2022-08-14      923    612\n2          Bouna       1376 6.555959    2023-01-11      669    412\n3       Bedjondo        534 7.073034    2023-06-09      251    184\n4       Bekourou        496 6.836694    2023-06-17      251    164\n5         Bedaya        435 7.098851    2023-07-04      209    147\n6         Koumra        253 7.106719    2023-08-14      138     84\n  mean_age_hosp mean_age_female n_death_u6m prop_female prop_hosp\n1      5.485294        6.748646          71   0.5105088 0.3384956\n2      5.665049        6.633782          61   0.4861919 0.2994186\n3      5.211957        6.948207          22   0.4700375 0.3445693\n4      6.042683        6.840637          25   0.5060484 0.3306452\n5      6.156463        7.105263          17   0.4804598 0.3379310\n6      6.261905        6.456522           7   0.5454545 0.3320158"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#done",
    "href": "sessions_core/05_summary_table.html#done",
    "title": "Summary Tables",
    "section": "Done!",
    "text": "Done!\nYou should be proud of yourselves, making summary tables is an important skill to an epidemiologist, making it in R is very efficient! Don’t forget to save your code!\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#going-further",
    "href": "sessions_core/05_summary_table.html#going-further",
    "title": "Summary Tables",
    "section": "Going Further",
    "text": "Going Further\n\nExtra Exercises\n\nMake a summary table that summarizes:\n\nThe number of patients\nThe proportion of male\nThe number of deaths\nThe CFR\nThe number of deaths among patients that had pneumonia\nin all the different age groups !\n\nMake a table that shows the proportion of patients by age with any measles vaccine (by oral recall or card) and those with 1 or 2 doses.\nMake a table that compares the proportion of hospitalised and non-hospitalised patients with positive malaria RDT, fever, rash, cough, red eye, pneumonia, encephalitis, and “red” or “yellow” MUAC (less than 125 mm).\nCalculate the mean days from first symptom onset to consultation by sub-prefecture.\nCalculate the mean time spent in hospital (i.e. days from admission to outcome) by outcome (i.e. in those who recovered and those who died).\n\n\n\nAdditional Resources\n\nThe EpiR Handbook chapter on grouping data\nOnce you have tables, you can extensively customize them for display/publication using {gt} package:\n\nWebsite of gt\nBook about gt"
  },
  {
    "objectID": "sessions_core/06_epicurves.html",
    "href": "sessions_core/06_epicurves.html",
    "title": "Basic Data Visualization",
    "section": "",
    "text": "Grasp the very basics of data visualization in R using {ggplot2}\nBuild a basic epicurve"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#objectives",
    "href": "sessions_core/06_epicurves.html#objectives",
    "title": "Basic Data Visualization",
    "section": "",
    "text": "Grasp the very basics of data visualization in R using {ggplot2}\nBuild a basic epicurve"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#introduction",
    "href": "sessions_core/06_epicurves.html#introduction",
    "title": "Basic Data Visualization",
    "section": "Introduction",
    "text": "Introduction\nThis session is a short introduction to data visualization using the popular {ggplot2} package. Keep in mind that visualization in general and even {ggplot2} in particular are huge subjects that we can’t cover in a single core session. This tutorial is intended as a taster to give you a feel for how plotting is typically done. To do that, we will come back to one of our most beloved epidemiological plots: the epicurve.\nOur final plot will look like this:"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#setup",
    "href": "sessions_core/06_epicurves.html#setup",
    "title": "Basic Data Visualization",
    "section": "Setup",
    "text": "Setup\nDependencies. This session assumes that you know how to use RStudio that you are able to import data and that you know th basic data handling verbs that we have seen in the core sessions so far. If you need a refresher on either of these topics, we encourage you to review the core sessions in the learning pathway.\n\nThis session will use the clean version of the Moissala measles linelist data.\n\n\n\n  Course Folder\n\n\n\n Open your RStudio project and create a new script called epicurves.R with appropriate metadata. Load the following packages: {here}, {rio}, {dplyr}, {lubridate}, and {ggplot2}. Add a section to your script called # IMPORT DATA where you import the clean course dataset (moissala_linelist_clean_EN.rds)."
  },
  {
    "objectID": "sessions_core/06_epicurves.html#paradigms-of-plotting",
    "href": "sessions_core/06_epicurves.html#paradigms-of-plotting",
    "title": "Basic Data Visualization",
    "section": "Paradigms of Plotting",
    "text": "Paradigms of Plotting\nIn R, and indeed in everything, there are a lot of ways to approach data visualization. Two of the biggest paradigms are :\n\nThe All-In-One: this approach is characterized by having a single, typically somewhat complex, function that handles all aspects of building a plot. Base R as well as a variety of specialized packages tend to use this approach.\nLayered (or modular): here, instead of creating a plot with a single function, we will use separate functions to add (or modify) different features of a plot (such as the primary shapes, labels, error bars, themes, etc). This is the strategy used by packages like {ggplot2}, {highcharter}, or {echarts4r}.\n\nAn in depth discussion of why one might use one approach versus another is beyond the scope of this course, though we will note that most modern visualization packages tend to use a layered model. With that in mind, let’s take a look at the types of layers we are talking about in our “layered” approach.\n\nBreaking it Down: A Visualization and its Parts\nFor the purpose of this tutorial we will talk about only four visualization components (layers):\n\nCanvas / Data\nPrimary Shapes\nLabels\nTheme\n\nTo illustrate these components, let’s look at a basic schematic of an epicurve:\n\n\n\n\n\nThe most conceptually complex of the above layers is probably the canvas itself. Much as an artist needs to buy a canvas and conceptualize what they want to paint before they start painting, so too does a user of {ggplot2}. Creating the canvas is where we tell R that we want to start making a plot and what parts of the data that plot will use. Here, for example, we will tell R “I want to make a plot where the x axis represents dates (or weeks sometimes) and the y axis represents cases”. Once that canvas is set up we can start adding other layers in the same way that an artist would begin adding paint, their signature, or a frame.\nNow, let’s look at the syntax for these layers in {ggplot2} and how to put them together.\n\n\nGetting Started with {ggplot2}\nThe method of building a ggplot is relatively simple and takes the form:\n\nCreate a canvas using a duo of functions ggplot(aes(...))\nAdd things to the canvas\n\n{ggplot2} takes the idea of “adding something to the canvas” very literally: each new layer will be introduced to your plot using the + sign.\nThe general syntax of a ggplot is then:\n\n# DO NOT RUN (PSEUD-CODE)\ndf |&gt;                      # pipe in your data \n  ggplot(aes(x = ...,      # step 1: create canvas\n             y = ...)) +\n  layer_one(...) +         # step 2: add a first layer\n  layer_two(...) +         # step 3: add another layer\n  ...                      # continue adding layers...\n\nThe number of layers you add depends on how complex you want your plot to be. In our case, we will be adding three layers to our canvas with the following functions:\n\n# DO NOT RUN (PSEUD-CODE)\ndf |&gt;                    # pipe in your data\n  ggplot(aes(x = ...,     # step 1: create canvas\n             y = ...)) +\n  geom_col(...) +         # step 2: add shapes (bars)        \n  labs(...) +             # step 3: add titles\n  theme_classic(...)      # step 4: add a nicer theme\n\nWe can update our above schematic of an epicurve with these functions as follows:\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that in the above example, our very first line is actually our dataset being piped into the ggplot() function. This makes sense since {ggplot2} needs to know what data you’d like to visualize. But be careful, make sure that this line ends in a pipe (|&gt;) and not in a + sign like t |&gt; he other ones.\n\n\nIn the next part of the tutorial we will go through each of these steps (layers) individually using our course dataset to make your first epicurve."
  },
  {
    "objectID": "sessions_core/06_epicurves.html#sec-epicurve-steps",
    "href": "sessions_core/06_epicurves.html#sec-epicurve-steps",
    "title": "Basic Data Visualization",
    "section": "Building Your First ggplot",
    "text": "Building Your First ggplot\n\nPreparing Your Data: Aggregate by Day\nUltimately we would like to plot an epicurve of daily cases. You may have noticed, our current data is daily, but of course several cases may occur on some days. To plot an epicurve we will need to aggregate data by day. Fortunately, you already learned how to summarize data in previous sessions.\n\nUsing count(), create a new dataframe called df_cases that summarizes the total number of cases observed per day. The head of this data frame should look like this:\n\n\n  date_onset n\n1 2022-08-13 1\n2 2022-08-17 1\n3 2022-08-18 1\n4 2022-08-22 1\n5 2022-08-30 2\n6 2022-09-01 1\n\n\n\nGreat! Now we are ready to make our epicurve. In the following steps, you’ll be asked to use df_cases to plot a classic epicurve of the number of daily admissions. To demonstrate the functions you’ll be using, I will plot the curve of the number of daily hospitalizations as an example. To do that, I’ve built myself another dataframe, df_outcome, which looks like this:\n\n\n  date_admission patients\n1     2022-08-14        1\n2     2022-08-25        1\n3     2022-09-02        1\n4     2022-09-06        1\n5     2022-09-09        1\n6     2022-09-10        1\n\n\n\n\nSet up a Canvas: Initialize a Plot\nThe first step is creating your canvas by specifying your dataset and the names of the columns you’d like to visualize. This is done using ggplot(aes(...)) with the following syntax:\n\n# DO NOT RUN (PSEUD-CODE)\ndf_data |&gt;\n  ggplot(aes(x = x_axis_variable_name,\n             y = y_axis_variable_name))\n\nFor an epicurve of hospitalizations, I’d like to plot the days (date_admission) on the x-axis and the number of patients hospitalized (patients) on the y-axis. Let’s update our pseudo-code to do that:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients))\n\n\n\n\n\n\n\n\nFabulous, take a look at that big beautiful box of potential. This is our empty canvas. In RStudio this plot should show up in the panel on the bottom right of the screen.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nJust like with {dplyr}, we write our column names without quotation marks. This is unsurprising as {ggplot2}, like {dplyr}, is a member of the {tidyverse} and therefore uses similar syntax.\n\n\nNow, you may be wondering what is this aes() function that we’ve nested inside of ggplot()? The short answer is that aes() creates an AESthetic mapping that tells {ggplot2} which columns of our data should be represented by which visual elements of our plot (like the axes, for example).\nAesthetic mappings create a map that defines how data elements (variables) are to be represented by visual elements (like axes, colors, and sizes). For example, here we are mapping the days to the x-axis and the number of patients to the y-axis. We could also imagine, for example, an epicurve where bars are colored based on whether patients lived or died. This would be an example where the variable outcome is being mapped to the visual element of color.\nFor now it is enough to know that aes() is the place where you will define your x-and y-axis.\n\nCreate a new section in your script called # PLOT EPICURVE. Then create an empty canvas for your epicurve using df_cases.\n\nAt this point, your plot should look like this:\n\n\n\n\n\n\n\n\n\nExcellent! Now let’s add some bars.\n\n\nPlot the Bars\nNow that we have our canvas, it’s time to add some shapes. In {ggplot2}, the shapes plotted on a figure are called geometries. Geometries are the primary visual representation of your data and should feel pretty familiar. A few common types of geometries include:\n\nBar Plots (geom_col() or geom_bar())\nHistograms (geom_hist())\nScatterplots (geom_point())\nLine Plots (geom_line())\nBoxplots (geom_boxplot())\n\nToday, we’re doing epicurves so we are most interested in learning how to make a bar plot. In our case, we will be using geom_col(). Remember that adding a new layer (in this case a geometry) to our ggplot is as simple as using a +, so we can add bars to the epicurve of hospitalized cases in the following way:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col()\n\n\n\n\n\n\n\n\nBrilliant! That sure looks like an epicurve to me. Though it does look a bit…grey. If we’d like to update the color of our bars (called the fill), we simply need to add the fill argument to geom_col().\nLet’s give it a try:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\")\n\n\n\n\n\n\n\n\n\nUpdate your epicurve plot to add bars with the color #E4573.\n\nYour plot should now look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the {ggplot2} framework, layers must be added to an existing canvas. This means that running geom_col() by itself will not produce any visual output. This, however, makes sense. Continuing with our analogy of ggplots being like paintings, running geom_col() by itself would be like having paint with no canvas to put it on.\n\n\nLooking good. Now it’s time to make our plot just a bit more informative and just a bit more attractive by adding labels and a nicer theme.\n\n\nAdd Some Labels\nA good plot needs some good labeling; n is hardly an informative axis title. Fortunately, {ggplot2} makes adding labels easy with the function labs(). This function will accept a variety of arguments allowing you to add a variety of label/title elements to your plot, for example:\n\nAxis Titles (x = and y =)\nPlot Title (title =)\nCaption\n\nAs for other layers, we can include a label layer by adding labs() to our current plot with the + sign:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date\",\n       y = \"Daily Patients\",\n       title = \"Measles Hospitalizations in Mandoul Region (Chad)\")\n\n\n\n\n\n\n\n\n\nUpdate your epicurve plot to add some reasonable axis labels and a nice title.  Extra Credit! Try adding a data source using caption.\n\nYour plot might now look like (for example):\n\n\n\n\n\n\n\n\n\n\n\nAdd a Theme\nIf we wanted to, we could stop here if our goal is to produce an informal plot. Ideally, however, it would be nice to use a somewhat more attractive theme and to increase the text size. To do this, we will add one last layer to our plot: a theme layer. Much like how geometries in {ggplot2} all start with geom_, all themes start with theme_. There are several themes available to you and you can check out what they look like on the {ggplot2} website.\nToday, we will use theme_classic(), which offers a simple but elegant output:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date\",\n       y = \"Daily Patients\",\n       title = \"Measles Hospitalizations in Mandoul Region (Chad)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nOk, nice. But we’d also like to increase the size of that tiny font. To do that we can adjust the base_size argument:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date\",\n       y = \"Daily Patients\",\n       title = \"Measles Hospitalizations in Mandoul Region (Chad)\") +\n  theme_classic(base_size = 17)\n\n\n\n\n\n\n\n\nThat looks better! Keep in mind that the font size needed will depend on what the plot is going to be used for (i.e. a presentation, an informal review, or a final report). Similarly, the exact theme you will want to use is ultimately a subjective choice. While there are guidelines, data visualization is as much an art as a science.\n\nAdd one final layer to your plot that adds a theme of your choice with an appropriate base_size.\n\n\n\nSave your plot\nIf you would like to save your epicurve, you can click on the “Export” button in the plot panel of RStudio:"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#done",
    "href": "sessions_core/06_epicurves.html#done",
    "title": "Basic Data Visualization",
    "section": "Done!",
    "text": "Done!\nVery well done team! You have build your first epicurve!\n\n\n\n Solutions file"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#go-further",
    "href": "sessions_core/06_epicurves.html#go-further",
    "title": "Basic Data Visualization",
    "section": "Go Further",
    "text": "Go Further\n\nExtra Exercises\n\nUse the theme_minimal() on one of your graph, with a base size font of 18.\nGo to this site, pick a color and update the color of your bars.\n\n\n\nChallenge Exercises\n\nInstead of aggregating by date, count the number of patients by sub-prefecture. Try to adapt your epicurve code to create a barplot of the number of patients by sub-prefecture.\n\n\n\nSatellites\n\nWeekly Epicurves\nFaceting"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#resources",
    "href": "sessions_core/06_epicurves.html#resources",
    "title": "Basic Data Visualization",
    "section": "Resources",
    "text": "Resources\n\nA full book on using {ggplot2}\n\nA whole chapter on epicurves"
  },
  {
    "objectID": "sessions_companion/survey_basic.html",
    "href": "sessions_companion/survey_basic.html",
    "title": "Standard Mortality Survey",
    "section": "",
    "text": "Calculate person time at risk\nUse {srvyr} to estimate mortality rates"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#objectives",
    "href": "sessions_companion/survey_basic.html#objectives",
    "title": "Standard Mortality Survey",
    "section": "",
    "text": "Calculate person time at risk\nUse {srvyr} to estimate mortality rates"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#introduction",
    "href": "sessions_companion/survey_basic.html#introduction",
    "title": "Standard Mortality Survey",
    "section": "Introduction",
    "text": "Introduction\nThis session focuses on how to do a basic analysis of data from a retrospective mortality survey using the MSF standard mortality survey protocol. We will be using a case study wherein a survey was conducted following a cholera epidemic in Haiti in 2010.\nThis session assumes you have completed the basic learning pathway for R and are able to:\n\nImport data\nPerform basic cleaning using case_when()\nAggregate data using count() and summarize()\nProduce tables using gt()\n\nIf you need to revisit or learn any of these topics, please refer to the core sessions of the learning pathway."
  },
  {
    "objectID": "sessions_companion/survey_basic.html#setup",
    "href": "sessions_companion/survey_basic.html#setup",
    "title": "Standard Mortality Survey",
    "section": "Setup",
    "text": "Setup\n\nThis session uses a specific case study. Download and unzip the associated folder then open the main.R script from the R folder:\n\n\n\n Download\n\n\n\n\nThe folder you have downloaded contains a (mostly) empty R script as well as Excel files for the Kobo form used in the survey and the data collected with it.\n\nTake a minute to open and investigate both the Kobo form and the raw data. What is contained in the different tabs of the dataset?"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#import",
    "href": "sessions_companion/survey_basic.html#import",
    "title": "Standard Mortality Survey",
    "section": "Import",
    "text": "Import\nOur dataset has two tabs, the first contains household level data and the second contains individual data. For now, we are most concerned with data about the individuals but we will ultimately need both. Let’s load it all into R (as well as the packages we will be using in today’s session).\n\nIn your script (main.R), add an appropriate header for the file and create a section that loads the following packages:\n\nhere\nrio\ngt\nsrvyr\ntidyverse\n\nThen create a new section called Import and use rio to import the second sheet of your dataset into an object called df_raw. We don’t need all of the columns of this data, use select() to select only the following:\n\nsex\nage\nborn\nborn_date\njoined\njoined_date\nleft\ndied\ndied_date\ndied_cause\n_parent_index renamed as hh\n\nThen create a second object called df_hh containing the first sheet of your dataset keeping only the following columns:\n\ninterview_date\nclst_id\n_index renamed as hh\npresent\nconsent\n\nHint. Remember that when using select() you can quickly rename something by using an =, for example: hh = '_parent_index'."
  },
  {
    "objectID": "sessions_companion/survey_basic.html#first-look-and-recoding",
    "href": "sessions_companion/survey_basic.html#first-look-and-recoding",
    "title": "Standard Mortality Survey",
    "section": "First Look (and Recoding)",
    "text": "First Look (and Recoding)\nGreat! Now that we’ve loaded our data let’s take a first look at our data. One of the first things we can do is check the structure of our data:\n\ndf_raw |&gt;\n  str()\n\nWe might also want to quickly check how many individuals in the dataset had died as our survey focuses on mortality:\n\ndf_raw |&gt;\n  count(died)\n\n\nUse count() to determine how many participants you had by sex.\n\nHm, 1 and 2 for sex are a bit ambiguous. It might be helpful to recode our categorical data to use more meaningful labels. For example:\n\ndf &lt;- df_raw |&gt;\n  # recoding\n  mutate(sex = case_when(sex == 1 ~ 'Male',\n                         sex == 2 ~ 'Female`,\n                         .default = NA))\n\n\nCreate a new section in your script called Cleaning. This section will have a “cleaning pipe” that will take df_raw, perform several cleaning steps, and store the resulting dataframe into an object called df.  Using case_when(), create a new step in your cleaning pipe that recodes the categorical variables in your dateset. You can use the above recoding for sex. For the variables born, joined, left, died use the recoding:\n\n0 = No\n1 = Yes\n99 = Unknown\n\nFor died_cause use the recoding:\n\n1 = Diarrhoea\n2 = Fever\n3 = Respiratory Disease\n4 = Accident\n5 = During Delivery\n6 = Other\n99 = Unknown\nNA = Did Not Die\n\nThe head of df should look like this:\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh\n1 Did Not Die  1\n2 Did Not Die  1\n3 Did Not Die  1\n4 Did Not Die  1\n5 Did Not Die  1\n6 Did Not Die  1\n\n\nNow that we have nicer labels, let’s explore our data a bit more. For example:\n\nHow many people died of each potential cause?\nLook at the combinations of died, left, joined, and born. Which combinations are the most common? Does this make sense?\nWho died more, males or females? Who was more at risk?\n\n Hint. Remember that you can give multiple column names to count() in order to create contingency tables."
  },
  {
    "objectID": "sessions_companion/survey_basic.html#cleaning",
    "href": "sessions_companion/survey_basic.html#cleaning",
    "title": "Standard Mortality Survey",
    "section": "Cleaning",
    "text": "Cleaning\n\nDates as (Simple) Dates\nLet’s finish tidying up our data for analysis. One ting we need to do is make sure our data is all of the right type. We have already recoded everything for the categorical variables but we haven’t yet looked at dates.\n\nclass(df_raw$born_date)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nIt looks like our dates are of the type POSITct, let’s convert them to a more simple date format using ymd() from {lubridate}.\n\nAdd a step to your cleaning pipleing that uses ymd() from {lubridate} to convert the dates for born_date, joined_date, left_date, and died_date to simple dates. \nThe class() of df$born_date should now be Date:\n\nclass(df$born_date)\n\n[1] \"Date\"\n\n\n\n\n\nFixing Logical Issues\nA well designed Kobo form can go a long way to ensure we are collecting good quality data right from the start. For instance, we can make certain questions required to avoid missingness. We can also create “constraints” that will produce error messages when entered data violates preset rules; for example we might create a constraint that does not let the surveyor enter a date of death that falls outside the recall period.\n\nOpen the Kobo survey Excel file retrospective-mortality_kobo.xlsx and take a look at the column “constraint”. What were the constraints created in this file? Can you think of anything that wasn’t accounted for?\n\nDespite all of the protections we set up in the collection process there will always be a bit of cleaning needed. For example, while we have made sure that all dates fall within the recall period we didn’t create checks for other illogical relationships between dates. Let’s take a look, for instance, to see if anyone was born after they had died:\n\ndf_raw |&gt; \n  filter(died_date &lt; born_date)\n\n\nUse filter() to check for instances of people who joined the family after they had died. How many times did this happen?\nThe head of your output should look like this:\n\n\n  sex age born_date joined_date left_date  died_date born joined left died\n1   1  23      &lt;NA&gt;  2010-12-18      &lt;NA&gt; 2010-12-08    0      1    0    1\n  died_cause   hh\n1          1 1524\n\n\n\nWell that’s not great. How should we fix this? The exact best practice here is subject to a bit of debate but we recommend that you retain the date for death and remove the date for birth / joined (ie: reassign it to NA). Alternatively, if you catch this error during data collection you can ask the interviewer about the error. It may be the case that it was a typo and they remember the appropriate dates. Where possible, try to do this type of oversight on a daily basis so that issues can be corrected in real-time.\nIf we can only keep one date, why do we give preference to the date of death? The date of death is the more important variable for the purpose of this particular survey and is also a rare event, meaning that any lost dates might have a disproportionate impact on results. Additionally, we might expect that the date of death is more likely to be reliable compared to other dates (such as the exact date of birth or when someone joined / left the household).\n\n\n\n\n\n\nImportant\n\n\n\nIn this instance, there were only a couple cases of “birth or joining the household after death” out of a dataset of over 18,000 people so removing their birth / join dates isn’t a huge deal. If errors like this are more common, however, it may signal an important problem with the form design and / or the training of the surveyors. Quickly looking for problems like this (even in Excel) after the pilot and during the data collection phase can help bring your attention to any issues while you still have time to fix them.\n\n\n\nUsing mutate() and case_when(), add a step to your cleaning pipe to replace the problematic birth / joined dates with NA. How can you check if this worked correctly?\n\nCan you see any other issues in the data? I see two:\n\nThere are a few people who were born within the recall period but have an age greater than 0\nOne person died after having left the household\n\n\nHow would you handle these two issues? Think about the types of problems that might have produced them and the consequences of different cleaning strategies on your final results.  Bonus. Do you think either of these could have been prevented through a better Kobo design?\n\nLet’s consider the issue of being born in recall with an age above 0 first. How to handle ages below 1 can be tricky and surveyors should be explicitly trained on whether they “round up” or “round down”. Alternatively, modern surveys will tend to ask for age in months for individuals under a certain limit (typically 12, 23, or 59 months). Recording age in months for young children is particularly important in surveys that focus on issues like vaccination, malnutrition, or mortality where the health issues of interest are (potentially) associated with infants or children &lt; 5. A constraint could also have been added to the Kobo form to avoid this issue.\nFor the purpose of this survey, we don’t have any information on months so the best we can do is impose a consistent rule that anything &lt; 12 months should be recorded as 0. This means that if a child was born during recall (which is a period of &lt; 12 months) then their age must be 0.\n\nAdd a step to your pipeline that ensures that anyone born in recall has an age of 0. If you have done this correctly, you should be able to filter df to look only at individuals born in recall and verify that their age is 0:\n\ndf |&gt;\n  filter(born == 'Yes') |&gt;\n  pull(age) |&gt;\n  unique()\n\n[1] 0\n\n\n\nThe second issue is a bit more complex. Let’s take a look at the individual(s) in question:\n\ndf |&gt;\n  filter(left_date &lt; died_date)\n\n     sex age born_date joined_date  left_date  died_date born joined left died\n1   Male  25      &lt;NA&gt;        &lt;NA&gt; 2010-11-02 2011-03-08   No     No  Yes  Yes\n2 Female   3      &lt;NA&gt;  2011-01-05 2010-11-20 2011-03-28   No    Yes  Yes  Yes\n3 Female  60      &lt;NA&gt;  2011-03-08 2011-02-15 2011-03-15   No    Yes  Yes  Yes\n  died_cause   hh\n1  Diarrhoea   88\n2  Diarrhoea  236\n3  Diarrhoea 2861\n\n\n\nExamine the three individuals in the above output. Are all of them problematic? Why or why not?\n\nThe 3 year old and 60 year old don’t pose a problem, they simply left and then rejoined the household. The 25 year old, on the other hand seems to have left and then died without ever coming back in between. What should we do? Let’s think about why something like this might have appeared in our data. There are two main options:\n\nMaybe the person did rejoin the household but the participant forgot to mention it\nPerhaps the participant did not fully understand that the survey would only consider deaths when someone was still a member of the household when they died\n\nIf possible, we might discuss with the surveyor who conducted this interview to determine which option was more likely. In the absence of any additional information, however, we will probably need to go with option two. If we do that then we will need to recode this person as having lived rather than died as they were stil alive at the time that they left the household.\n\nAdd another step to your pipeline that recodes this individual as having lived, ie: their died value should be reset to 'No' and their date of death should be removed. If you check again for people who left the household prior to dying you should now see only two people:\n\n\n     sex age born_date joined_date  left_date  died_date born joined left died\n1 Female   3      &lt;NA&gt;  2011-01-05 2010-11-20 2011-03-28   No    Yes  Yes  Yes\n2 Female  60      &lt;NA&gt;  2011-03-08 2011-02-15 2011-03-15   No    Yes  Yes  Yes\n  died_cause   hh\n1  Diarrhoea  236\n2  Diarrhoea 2861\n\n\nNote. The decision to remove a death from the dataset is debatable. Remember that because deaths are rare events the addition / removal of one can have a disproportionate impact on mortality calculations. To minimize issues like this one, make sure to spend sufficient time when training surveyors to make sure they fully understand core concepts like the recall period and the idea of a “continuous household”. Giving specific examples like this one during training can help surveyors to navigate these issues appropriately when they come up during data collection."
  },
  {
    "objectID": "sessions_companion/survey_basic.html#joining-household-level-data",
    "href": "sessions_companion/survey_basic.html#joining-household-level-data",
    "title": "Standard Mortality Survey",
    "section": "Joining Household Level Data",
    "text": "Joining Household Level Data\nOur individual level data is looking nice but they are completely detached from our household level data (remember df_hh from the start of the tutorial?). For example, we might like to know the interview date associated with each individual as well as the cluster they were in. To do this, we need to perform a join.\nAn in depth look at joins is beyond the scope of this session, but in essence joins are used to take the data from one dataframe and add it (row-wise) to another dataframe based on a variable that is shared by both datasets (such as an id). For example, here we want to go row by row in our individual level data (df) and add columns with the related household level information for each person (from df_hh). To do this, we will use the function left_join() from {dplyr}:\n\ndf |&gt;\n  left_join(df_hh) |&gt;\n  head()\n\nJoining with `by = join_by(hh)`\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh interview_date clst_id present consent\n1 Did Not Die  1     2011-03-29       1       1       1\n2 Did Not Die  1     2011-03-29       1       1       1\n3 Did Not Die  1     2011-03-29       1       1       1\n4 Did Not Die  1     2011-03-29       1       1       1\n5 Did Not Die  1     2011-03-29       1       1       1\n6 Did Not Die  1     2011-03-29       1       1       1\n\n\nNotice that here R has used the column hh (household id) as the common variable between the datasets; you can see a message indicating this right above the output of head().\n\n\n\n\n\n\nNote\n\n\n\nWhat does the “left” in left_join() mean? In a simple sense, left joins involve one dataset that data is added to (Dataset A) and another that data is taken from (Dataset B). Dataset A is the “core dataset” and the output will always include all of it’s rows. Rows from Dataset B will be kept if and only if left_join() finds an appropriate row in Dataset A to which they can be added. In our data, for example, rows of data on households which had no members (and thus do not appear in df) will not be included in the output of the above join.  In a left_join() R will always consider the first argument to be the core dataset (Dataset A); ie:\n\n# PSEUDO-CODE\nleft_join(dataset_a, dataset_b)\n\n\n\n\nAdd a final step in your cleaning pipe that uses left_join() to add the household level data to each of the rows in df and then converts interview_date to use a basic date format (as you did with born_date etc). Your final pipe should now do the following:\n\nUse df_raw as an input\nRecode categorical variables\nConvert dates to simple y-m-d format\nFix illogical data issues\nJoin household indicators\n\nIf everything went well, the head of df should look like this:\n\nhead(df)\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh interview_date clst_id present consent\n1 Did Not Die  1     2011-03-29       1       1       1\n2 Did Not Die  1     2011-03-29       1       1       1\n3 Did Not Die  1     2011-03-29       1       1       1\n4 Did Not Die  1     2011-03-29       1       1       1\n5 Did Not Die  1     2011-03-29       1       1       1\n6 Did Not Die  1     2011-03-29       1       1       1"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#mortality-calculations",
    "href": "sessions_companion/survey_basic.html#mortality-calculations",
    "title": "Standard Mortality Survey",
    "section": "Mortality Calculations",
    "text": "Mortality Calculations\nWith cleaning and joining out of the way we can finally move on to the fun part, analysis. We want to calculate the following:\n\nCrude Mortality Rate\nUnder Five Mortality Rate\nDiarrhoea Specific Mortality Rate\n\n\n(On paper) Write out the formula for each of these indicators. Do we already have all the necessary variables in our dataset for the calculations?\n\nThese indicators are rates, meaning they require a denominator in person-time at risk. Our dataset doesn’t have a column for this yet. Let’s fix that.\n\nPerson Time at Risk\nFor our survey, each individual’s person-time at risk is the time when they were:\n\nAlive and\nPart of the household\n\nMost people were alive and part of the household for the full recall period. For these people their time at risk is the full recall period. There are, however, a number of other options. For example:\n\n\n\n\n\n\nMost of these cases can all be handled the same way, in fact all but the last one. Take a minute and try to work out on paper a forumla for person time that we could use. Bonus points if you are able to convert this into code.\n\nComing up with a good formula here is not trivial, so let’s go through it together. Let’s imagine a person who joined the household in late 2010 and then died in February of 2011. If we put this person’s data into a dataframe we might have something like this:\n\nexample &lt;- data.frame(\n  date_interview = as.Date('2011-04-07'),\n  born = 'No',\n  date_born = NA,\n  joined = 'Yes',\n  date_joined = as.Date('2010-12-08'),\n  left = 'No',\n  date_left = NA,\n  died = 'Yes',\n  date_died = as.Date('2011-02-13')\n)\n\nexample\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07   No        NA    Yes  2010-12-08   No        NA  Yes\n   date_died\n1 2011-02-13\n\n\nTo calculate this person’s time at risk, we need to get “when their person time started” and “when their person time ended”. Then we take the difference between those two dates. For the start of someone’s person time, we need to pull the date when they where born / joined the household or (if they were present for the full preiod) the start date for the recall period. We can do this using case_when():\n\nrecall_start &lt;- as.Date('2010-10-17')\n\nexample |&gt;\n  mutate(pt_start = case_when(born == 'Yes' ~ date_born,\n                              joined == 'Yes' ~ date_joined,\n                              .default = recall_start))\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07   No        NA    Yes  2010-12-08   No        NA  Yes\n   date_died   pt_start\n1 2011-02-13 2010-12-08\n\n\nSimilarly, their time at risk ends when they die / leave or at the end of recall (when they were interviewed):\n\nexample |&gt;\n  mutate(pt_end = case_when(left == 'Yes' ~ date_left,\n                            died == 'Yes' ~ date_died,\n                            .default = date_interview))\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07   No        NA    Yes  2010-12-08   No        NA  Yes\n   date_died     pt_end\n1 2011-02-13 2011-02-13\n\n\nPutting it together, we can then calculate the total person time at risk as the difference between when the person time ended and when it started:\n\nexample |&gt;\n  mutate(pt_start = case_when(born == 'Yes' ~ date_born,\n                              joined == 'Yes' ~ date_joined,\n                              .default = recall_start),\n         pt_end = case_when(left == 'Yes' ~ date_left,\n                            died == 'Yes' ~ date_died,\n                            .default = date_interview),\n         pt = pt_end - pt_start)\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07   No        NA    Yes  2010-12-08   No        NA  Yes\n   date_died   pt_start     pt_end      pt\n1 2011-02-13 2010-12-08 2011-02-13 67 days\n\n\n\nCreate a new section in your code called Calculate Person Time and initialize an object called recall_start with the date 2010-10-17. Add a code block adapting the above to create a pt column in df that calculates person time at risk. The head of df should now look like this:\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh interview_date clst_id present consent   pt_start     pt_end\n1 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n2 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n3 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n4 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n5 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n6 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n        pt\n1 163 days\n2 163 days\n3 163 days\n4 163 days\n5 163 days\n6 163 days\n\n\n\nLet’s use range() to look at the maximum and minimum values of pt:\n\nrange(df$pt)\n\nTime differences in days\n[1] NA NA\n\n\nLooks like the value for person time at risk is sometimes missing. This happens when someone, for example, was born / joined / left / died but where the date information for that event is missing. How should we handle this? One option is to leave the value missing, meaning that person doesn’t contribute any person time at risk to the subsequent calculations of mortality. Alternatively, we can take the first availble value for which we have a date. So, for example, if we don’t know when someone was born we will use the start of the recall period as the begining of their person time at risk.\n\nWhat are the pros and cons of these two options? How would you adjust your above code to acheive option two?\n\nOption one artificially reduces the denominator of our mortality calculations, thus resulting in an overestimate of mortality. The second option will do the opposite. For today’s analysis we will go with option one and leave our code as is (missing values and all). Let’s look at our range again, this time ignoring the missing values:\n\nrange(df$pt, na.rm = TRUE)\n\nTime differences in days\n[1] -141  172\n\n\nNow we get numbers, but it looks like we have some negative values. What’s going on? Think back to the figure at the begining of this section. While most cases can be managed with our current calculation, it doesn’t account for individuals who left and then rejoined the household because this individuals will have joined_date &gt; left_date.\n\nThink about these individuals who leave and rejoin a household and the dates involved. Can you think of an equation for their person time at risk? How do you think this might be coded?\n\nFor these individuals, instead of taking a difference (between the end and start of time at risk), we instead need to calculate two chunks of time (before they left and after they returned) and then add them together. Here’s how we can do it:\n\ntmp &lt;- df |&gt;\n  mutate(\n    pt = case_when(\n      joined_date &gt; left_date & born == 'Yes' ~ (left_date - born_date) + (interview_date - joined_date),\n      joined_date &gt; left_date ~ (left_date - recall_start) + (interview_date - joined_date),\n      .default = pt\n    )\n  )\n  \nrange(tmp$pt, na.rm = TRUE)\n\nTime differences in days\n[1]   0 172\n\n\n\nAdapt your person-time code pipe to include this correction for individuals who left and rejoined the household. Then add a line remove the columns pt_start and pt_end as we won’t be using them anymore (and they won’t be accurate for individuals who left and returned). The head of df should now look like this:\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh interview_date clst_id present consent       pt\n1 Did Not Die  1     2011-03-29       1       1       1 163 days\n2 Did Not Die  1     2011-03-29       1       1       1 163 days\n3 Did Not Die  1     2011-03-29       1       1       1 163 days\n4 Did Not Die  1     2011-03-29       1       1       1 163 days\n5 Did Not Die  1     2011-03-29       1       1       1 163 days\n6 Did Not Die  1     2011-03-29       1       1       1 163 days\n\n\nBonus. Why did our above case_when need to have a separate case for individuals born during recall?\n\nNice, we are just about ready to calculate mortality! Notice that right now our values for person time are represented as a time difference (difftime class) in number of days. For our onward calculations it would be better if this were a simple numeric type.\n\nAdd a final step in your person-time pipe that convers pt to a numeric type using as.numeric().\n\n\n\nMortality Calculations\nNow we are (finally) ready to calculate mortality rates. We could do a basic calculation of this directly using the totals of number of deaths and cumulative person-time at risk:\n\nsum(df$died == 'Yes') / sum(df$pt, na.rm = TRUE) * 10000\n\n[1] 0.5405712\n\n\n\nHow would you interpret this mortality rate? Is it high?  In 2010, the baseline mortality rate in Haiti was 9 deaths per 1,000 person-years. Knowing this, calculate the excess mortality observed during this epidemic (expressed in excess deaths per 10,000 person-days)?  Hint. Start by converting the baseline rate to be represented in deaths per 10,000 person-days.\n\nSo far so good, but we haven’t included any confidence intervals in our calculation nor have we taken our survey design into account. To do this, we will make use of the {srvyr} package. This package was built for complex analysis of survey data and provides statistical methods to adjust for design effect and finite population size. An in depth discussion of design effect and how to adjust for it is beyond the scope of this lesson but, in essence, design effects are introduced when we use a sampling process that is not fully random. For example, the use of cluster sampling in this survey creates a design effect as we might expect people within a cluster to be more similar to each other than they are to other randomly selected people in the population. When we adjust for design effect we widen our confidence intervals (reduce our precision) to account for this non-random similarity.\nTo perform these adjustments {srvyr} needs to know a few things:\n\nThe id of the sample units requiring adjustment (in this case cluster ids)\nThe size of the population (needed to resolve both design affect and account for finite population size)\nThe weight of each cluster\n\nThe weight of the cluster is the product of two fractions:\n\nTotal population size / sample size and\nExpected cluster / true size of the given cluster\n\nIn principle, each of our clusters should have had 32 households. In practice, true cluster size may have deviated in some cases. We can use the function n_distinct() inside summarize() to add a column with the actual cluster size associated with each individual:\n\ndf |&gt;\n  summarize(.by = clst_id,\n    hh_count = n_distinct(hh)\n  ) |&gt;\n  head()\n\n  clst_id hh_count\n1       1       32\n2       2       32\n3       3       32\n4       4       32\n5       5       32\n6       6       32\n\n\n\nCreate a new section of your code called Calculate Mortality. Write a pipe that uses the above summarize statement to calculate number of households observed per cluster and then uses mutate() to add create columns weight and pop respectively containing the weights and total population size (in 2010, this was 228,425 people). Store the output of this pipe into an object called df_wt. The head of df_wt should look like this:\n\n\n  clst_id hh_count   weight    pop\n1       1       32 12.38546 228425\n2       2       32 12.38546 228425\n3       3       32 12.38546 228425\n4       4       32 12.38546 228425\n5       5       32 12.38546 228425\n6       6       32 12.38546 228425\n\n\nHint. The formula for weight is (population_size / sample_size) * (32 / hh_count).  Now, use left_join() to join the newly created weight and population data onto df. The head of df should now look like this:\n\n\nJoining with `by = join_by(clst_id)`\n\n\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh interview_date clst_id present consent  pt hh_count   weight\n1 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n2 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n3 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n4 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n5 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n6 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n     pop\n1 228425\n2 228425\n3 228425\n4 228425\n5 228425\n6 228425\n\n\n\nFor {srvyr} to use our newly added variables and perform calculations we need to create a “survey object”. This is a special class of dataframe that is specific to {srvyr} and is created using the function as_survey_design():\n\ntmp &lt;- df |&gt;\n  as_survey_design(\n    ids = clst_id,\n    wt = weight,\n    fpc = pop\n  )\n\n\nTry running the above code. What is the class of tmp? Does this object behave like a normal dataframe? Try doing some basic manipulations; for example:\n\nPull the data from the age column\nFilter to see only individuals who died\n\n\nAs you can see, once we apply as_survey_design(), we don’t have a normal dataframe anymore. So, we should store its output in a separate object, for example tmp or df_srvy. This ensures that df itself remains a standard dataframe available for other calculations, visualizations, etc.\n{srvyr} offers a number of functions to calculate indicators on survey data, most of the time used inside a summarize(). In our case, we will use the function survey_ratio() to calculate crude and specific mortality rates. The basic syntax of of survey_ratio() is pretty simple, for example we can use the following to calculate crude mortality rate:\n\ndf |&gt;\n  as_survey_design(\n    ids = clst_id,\n    wt = weight,\n    fpc = pop\n  ) |&gt;\n  summarize(\n    cmr = survey_ratio(\n      numerator = (died == 'Yes') * 10000,\n      denominator = pt,\n      vartype = 'ci',\n      deff = TRUE,\n      na.rm = TRUE\n    )\n  )\n\nSimple enough but let’s break down the arguments:\n\nnumerator is the numerator of our ratio, in this case the number of people who died (individuals for whom died was 'Yes') times 10,000 (to get a final result in 10,000 person-days)\ndenominator is the denominator of our ratio, in this case the amount of person time at risk (pt)\nvartype any variable(s) we want included to estimate error, here we chose confidence interval ('ci') but we could also ask for standard error ('se')\ndeff indicates whether we want an estimate of design effect to be included\nna.rm indicates whether {srvyr} should ignore missing values when performing the calculation\n\nThe output of this code is a new dataframe with our point estimate (cmr), confidence interval (cmr_low and cmr_upp), and the associated design effect.\n\nIn the above example we calculated crude mortality. Because this survey is associated with a particular outbreak (of cholera), we might also be interested in the disease specific mortality attributable to diarrhoea. Using the crude mortality code as a model, write code to calculate the diarrhoea specific mortality rate. You should find the following:\n\n\n# A tibble: 1 × 4\n   dsmr dsmr_low dsmr_upp dsmr_deff\n  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 0.369    0.257    0.482      2.64\n\n\nWe would also like to calculate under 5 mortality rate. In this case, we calculate crude mortality on the subset of our population that is under 5, ie: we need to filter our dataframe down to children under 5. Write some code to calculate under 5 mortality, remembering that you’ll need to filter prior to creating your survey design object. You should find the following:\n\n\n# A tibble: 1 × 4\n   u5mr u5mr_low u5mr_upp u5mr_deff\n  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 0.677    0.363    0.990      1.14\n\n\nHow would you interpret all of the above mortality rates? Take a minute to outline how you might present these findings. Are there any other indicators you might want to calculate for a more complete investigation?"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#done",
    "href": "sessions_companion/survey_basic.html#done",
    "title": "Standard Mortality Survey",
    "section": "Done!",
    "text": "Done!\nWell done, you have now worked through how to import, clean, and calculate mortality rates from basic mortality survey data.\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#going-further",
    "href": "sessions_companion/survey_basic.html#going-further",
    "title": "Standard Mortality Survey",
    "section": "Going Further",
    "text": "Going Further\n\nExtra Exercises\n\nEarlier, we calculated excess mortality rate, ie: how much higher our observed mortality was compared to baseline. Another indicator that we often present is the number of excess deaths observed during the recall period. How would you calculate this?\nUse {ggplot2} to create a bar plot of deaths over time.\nTake a look at the documentation for {srvyr} and see if you can use survey_mean() to calculate proportional mortality by cause of death.\nUse {gt} to create an attractive output table of the proportional mortality data you generated above.\nIn the cleaning section we corrected for cases where someone was born in the recall period but had an age &gt; 0. How could this have been prevented with a Kobo constraint?"
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html",
    "href": "sessions_companion/surveillance_companion.html",
    "title": "Surveillance",
    "section": "",
    "text": "Reuse skills aquired in the FETCH-R modules (import, clean and visualize data)\nMore specifically, analyze surveillance data to detect alerts and to help decide which alerts to prioritize for further field investigation."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#objectives",
    "href": "sessions_companion/surveillance_companion.html#objectives",
    "title": "Surveillance",
    "section": "",
    "text": "Reuse skills aquired in the FETCH-R modules (import, clean and visualize data)\nMore specifically, analyze surveillance data to detect alerts and to help decide which alerts to prioritize for further field investigation."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#introduction",
    "href": "sessions_companion/surveillance_companion.html#introduction",
    "title": "Surveillance",
    "section": "Introduction",
    "text": "Introduction\nWarning. This session is a companion to the case study Measles emergency response in the Katanga region (DRC) from the FETCH Surveillance module and might not make sense as a standalone tutorial.\nFrom an R point of view, this tutorial builds on skills acquired throughout the FETCH-R modules, introduces a couple of useful generalist functions, and some more specialized ones.\n\n\n\n\n\n\nTip\n\n\n\nDo not hesitate to refer to past sessions and your own scripts to remind yourself of some functions!"
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#setup-question-2",
    "href": "sessions_companion/surveillance_companion.html#setup-question-2",
    "title": "Surveillance",
    "section": "Setup (Question 2)",
    "text": "Setup (Question 2)\nSince this is part of a specific module, you will create a new RStudio project. We refer you to the main session for help creating a project and importing your data.\n\nSetup a new project\n\n\nCreate a folder surveillance_case_study associated with the FETCH Surveillance module. Add the following subfolders in it:\n\n\n📁 data\n\n📁 clean\n📁 raw\n\n📁 R\n📁 outputs\n\n\nCreate an RStudio project at the root of the surveillance_case_study folder.\nIf you do not already have the data from the case study download them.\n\n\n\n\n Download raw data\n\n\n\n 4. Unzip the archive if you just downloaded. Save the two Excel files in the subfolder data/raw.  5. Create a new script called import_clean.R and save it in the R subdirectory. Add metadata and a section to load the following packages: {here}, {rio}, and {tidyverse}.\n\n\n\nImport data in R\nReminder from the case study: you requested access to the routine surveillance data and the laboratory data to the DRC MoH. The MoH agreed to share it with you on a weekly basis. The first dataset you received is of week 20 in 2022 (the data we are working on are simulated).\n\nIf you have not done it already, open the raw data files in Excel (or another equivalent application) to inspect them.\n\nThe surveillance dataset is pretty straightforward to import. The lab dataset is slightly trickier: the data headers do not start at line one. Fear not, the skip argument from the import() function is made for this situation:\n\n# DO NOT RUN (PSEUDO-CODE)\nimport(\n  here(\"data\", \"raw\", \"example_file.xlsx\"), \n  skip = 3  # Skip the first three lines and start importing from line four.\n) \n\n\n\nAdd a section to your script dedicated to data importation.\nImport the surveillance data and store it into a data_surv_raw data frame. Then, import the lab data and save it in a data_lab_raw data frame.\nVerify that the import went well for both data frames (Viewer, check the dimensions or start and tail of data frames)."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#cleaning-question-2-and-3",
    "href": "sessions_companion/surveillance_companion.html#cleaning-question-2-and-3",
    "title": "Surveillance",
    "section": "Cleaning (Question 2 and 3)",
    "text": "Cleaning (Question 2 and 3)\n\nSurveillance data (Q2)\nNow that the data is correctly imported, we are going to perform some more checks, as usual, before a bit of cleaning.\n\nQuick checks\nDuring the case study you won’t have time to inspect and clean all columns within the available time, so for now we will focus on key columns: health_zone, week, totalcases and totaldeaths.\n\n\n\n\n\n\nNote\n\n\n\nIf you work on this tutorial in your own time, inspect the quality of the other columns and cross-check information of several columns. We refer you to the discussion of the case study for more checks to perform.\n\n\n\nAdd a section for the exploration and cleaning of the surveillance data into your script.  Now, explore the surveillance data frame and answer the following questions:\n\nWhat are the column names?\nHow many provinces are in the dataset? Is this coherent with what you expect?\nHow many health zones are in the dataset?\nWhat is the range of weeks?\nWhat is the min of totalcases?\nWhat is the max of the totaldeaths?\nDo you notice missing data for these columns? Are the strings of text clean?\n\n\n\n\nClean strings\nNow that we have a better idea of what is the state of the data, let’s start cleaning. We are going to write a cleaning pipeline like we did in the main modules (check out your code for the end of the cleaning modules to see an example final pipeline).\n\n\n\n\n\n\nTip\n\n\n\nTo facilitate debugging your pipeline, add commands one by one, checking each new command before adding a new one.\n\n\nWe are going to perform a couple of actions on the columns containing text to remove potential problems:\n\ntransform them to lower case\nremove potential extra spaces\nreplace - and spaces by _.\n\nBecause you may not have the time to do all of text colums, work on the health_zone or the province column for the following instructions.\n\nStart a cleaning pipeline with a mutate() that turns the chosen column to lower case.\n\nNow, we are going to introduce two handy functions for more text cleaning. The first one is the str_squish() function from the {stringr} package (help page here), that removes spaces at the start or end of the strings, and replace multiple spaces in the middle of a string by a single space.\n\nexamples &lt;- c(\" Trailing spaces     \",\n              \"Multiple     spaces\",\n              \" Everything     here \")\n\nstr_squish(examples)\n\n[1] \"Trailing spaces\" \"Multiple spaces\" \"Everything here\"\n\n\nThe other function, str_replace (also from the {stringr} package) does what you expect from its name: replace something in a string by something else. It has a pattern argument that take the bit of text to be replaced, and a replacement arguments that takes the bit of text to use as replacement:\n\nstr_replace(\n  \"HAUT-KATANGA\",    # A string of text (or a column, if used in a mutate)\n  pattern = \"-\",     # The bit to replace\n  replacement = \"_\"  # The replacement\n)\n\n[1] \"HAUT_KATANGA\"\n\n\n\nAdd steps to your mutate to:\n\nRemove all unwanted spaces from your chosen column\nChange the - and to _ in the column (in two steps)\n\nThe head of these columns should now be:\n\n\n  country     province    health_zone disease\n1     drc haut_katanga mufunga_sampwe measles\n2     drc haut_katanga        sakania measles\n3     drc haut_katanga        mitwaba measles\n4     drc haut_katanga kilela_balanda measles\n5     drc haut_katanga         likasi measles\n6     drc haut_katanga         kikula measles\n\n\nStore the result data frame in a data_surv object.\n\n\n\nSave the clean data\n\nUse the {rio} package to export data_surv to a .rds file called data_ids_2022-w20_clean in the data/clean subfolder of your project.\n\n\n\n\nLaboratory data (Q2)\nWe are going to follow the same steps as before for the lab data, and focus for now on the columns health_zone, igm_measles and igm_rubella.\n\nQuick checks\n\nPerform data checks on the colums names and dimensions. What are the categories for igm_measles and igm_rubella? What do you need to do to clean these columns?\n\n\n\nClean and recode strings\n\n\nStart a new cleaning pipeline to clean the lab data. As before, for one of the text column, change it to lower case, remove the extra spaces and replace the or - by _.\nRecode at least one of igm_measles or igm_rubella columns so that the categories are negatif, positif and indetermine.\nStore the cleaner version in a data_lab data frame\n\nThe head of the cleaned columns should now be:\n\n\n   health_zone igm_measles igm_rubella\n1      kambove    negative    negative\n2      kambove    negative    negative\n3      kambove    negative    positive\n4      kambove    negative    negative\n5      kambove    negative    positive\n6      kambove    negative    negative\n7      kambove    negative    negative\n8      kambove    negative    positive\n9       manika    negative    negative\n10   kamalondo    negative    negative\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou can use the case_when() function to recode the IGM columns.\n\n\n\n\n\nSave the clean data\n\nExport the data_lab data frame to a .rds file called data_lab_2022-w20_clean in the data/clean subfolder of your project.\n\n\n\n\nGoing further\nThis is the end of the steps for question 2! If you finished in advance and there is still time, reuse the functions we just saw to clean the other text columns in both datasets and recode both IGM column in the lab dataset.\nIf you still have time, perform more checks on the data:\n\nDisplay the health zone for which the numbers by age group add up to a different number than the total (if any)\nAre there any health zone for which the number of deaths is higher than the total number of cases?\nAre there duplicated lines (fully duplicated, or several values for health zone and week)?\nAre there unrealistic case numbers?\n\n\n\nComplete surveillance dataset (Q3)\nDuring the case study and the data checks, you realized that some weeks are missing from the surveillance dataset. You discussed the possible reasons for it, and the associated problems. Here we are going providing code to create a dataset that contains all weeks (assuming that missing weeks had zero cases and deaths).\nWe will use the function complete() from the {tidyr} package to add the missing lines and fill the columns containing numbers (totalcases and totaldeaths) with zeros. Due to the constrained time, we will give you the code for now, but check out the details in the Going further section when you have time.\n\n\nStart a new pipeline that takes the data_surv data frame and keeps only the columns province, health_zone, week and total cas.\nAdd a new step to your pipeline and paste the following code to complete the data frame:\n\n\ncomplete(\n  # Use all the existing combinaitions of province and health zone:\n  nesting(province, health_zone),\n  \n  # All the combinations whould have all weeks from the minimum (1) to the maximum (20) of the week column.\n  week = seq(min(week, na.rm = TRUE), \n             max(week, na.rm = TRUE)),\n  \n  # Fill these two columns with zeros for the newly created weeks:\n  fill = list(totalcases  = 0,  \n              totaldeaths = 0 \n  )\n) \n\n\nStore the result of the pipeline in a data frame called data_surv_weeks. The head of that data frame looks like:\n\n\n\n# A tibble: 10 × 5\n   province     health_zone  week totalcases totaldeaths\n   &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 haut_katanga kafubu          1          0           0\n 2 haut_katanga kafubu          2          0           0\n 3 haut_katanga kafubu          3          0           0\n 4 haut_katanga kafubu          4          0           0\n 5 haut_katanga kafubu          5          0           0\n 6 haut_katanga kafubu          6          0           0\n 7 haut_katanga kafubu          7          0           0\n 8 haut_katanga kafubu          8          0           0\n 9 haut_katanga kafubu          9          0           0\n10 haut_katanga kafubu         10          0           0\n\n\n\nWhen you are done, export that data frame to a .rds file called data_ids_2022-w20__weeks_clean in the data/clean subfolder of your project.\n\n\n\n\nGoing further\nThis is the end of question 3; if you are finished in advance, do not hesitate to carry on checking the data and listing potential problems and cleaning the columns. And go read the explanations for the complete() function, or its help page`."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#defining-alerts-question-4",
    "href": "sessions_companion/surveillance_companion.html#defining-alerts-question-4",
    "title": "Surveillance",
    "section": "Defining alerts (Question 4)",
    "text": "Defining alerts (Question 4)\n\nPreparing the dataset\nWe are going to carry on preparing the datasets for the analyses.\n\n\nIf you have not had the time to clean both health zone and province in both datasets, as well as both igm columns in the lab dataset you can import cleaner versions of the data:\n\n\n\n\n Download clean data\n\n\n\n Unzip the archive in your data/clean subfolder\n\nCreate a new script analysis_surv.R in the R subfolder. Add the metadata of the script, a package import section to import the packages {here}, {rio}, {tidyverse}, {lubridate} and {zoo}.\nAdd an import data section and import the clean .rds files in R using the import() function as usual (your cleaned version or the one you just downloaded). Assign the cleaned data to data_surv, data_lab and data_surv_weeks and carry on.\n\n\n\nSubset health zone\nTo simplify the work, we are going to focus on four health zones: Dilolo, Kampemba, Kowe, and Lwamba.\n\nStart a new pipeline from data_surv_weeks. Its first step is to only retain data for the the Dilolo, Kampemba, Kowe, and Lwamba health zones.\n\n\n\nWeekly indicator\nThe first indicator we want to caclulate is whether a health zone has 20 or more suspected cases in one week. This indicator is binary and only considers data in a given health zone and week, which corresponds to individual rows of our data frame.\n\nAdd a mutate() to your pipeline, to create a cases20 column that contains 1 if a given health zone has 20 cases or more in that week, and 0 otherwise.\n The top of the data frame created by the pipe thus far looks like this:\n\n\n# A tibble: 10 × 6\n   province     health_zone  week totalcases totaldeaths cases20\n   &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1 haut_katanga kampemba        1         75           0       1\n 2 haut_katanga kampemba        2         42           0       1\n 3 haut_katanga kampemba        3         46           0       1\n 4 haut_katanga kampemba        4         50           0       1\n 5 haut_katanga kampemba        5         43           0       1\n 6 haut_katanga kampemba        6         33           0       1\n 7 haut_katanga kampemba        7         45           0       1\n 8 haut_katanga kampemba        8         52           0       1\n 9 haut_katanga kampemba        9         38           0       1\n10 haut_katanga kampemba       10         46           0       1\n\n\n\n\n\nCumulative indicator\nThe second indicator you want to calculate is whether a health zone has more than 35 cumulated suspected cases within three weeks. This is a bit more complicated than the previous case: within health zone you need to calculate the sum of cases by groups of three weeks, but the groups are not fixed, they are rolling across time. We are getting in the territory of moving averages/sums/etc.\n\nCumulative sum\nWe are going to use the rollapply() function from the {zoo} package, as it is versatile and powerful. As its name suggests, the rollapply() function applies a function in a rolling way to a vector or a column of a data frame.\nSince we are constrained in time, we are going to provide the code of the rollapply() function to calculate the cumulative sum over three weeks, but check out the details in the Going further section when you have time.\nThis is how to do it for one health zone:\n\n# Create mini example data frame\nexample_df = data.frame(\n  province    = \"Haut Katanga\",\n  health_zone = \"Dilolo\",\n  week        = 1:10,\n  totalcases  = rep(1, times = 10))\n\nexample_df \n\n       province health_zone week totalcases\n1  Haut Katanga      Dilolo    1          1\n2  Haut Katanga      Dilolo    2          1\n3  Haut Katanga      Dilolo    3          1\n4  Haut Katanga      Dilolo    4          1\n5  Haut Katanga      Dilolo    5          1\n6  Haut Katanga      Dilolo    6          1\n7  Haut Katanga      Dilolo    7          1\n8  Haut Katanga      Dilolo    8          1\n9  Haut Katanga      Dilolo    9          1\n10 Haut Katanga      Dilolo   10          1\n\nexample_df |&gt; \n  mutate(cumcas = rollapply(\n    data  = totalcases, # The column to work on\n    width = 3,          # Width of the window  \n    FUN   = sum,        # Function to apply, here the sum   \n    align = \"right\",    # We are counting backward in time\n    partial = TRUE,     # Allows sum to be made even if window is less than three\n    na.rm = TRUE        # Extra unamed argument to be passed to the sum function\n  )\n  )\n\n       province health_zone week totalcases cumcas\n1  Haut Katanga      Dilolo    1          1      1\n2  Haut Katanga      Dilolo    2          1      2\n3  Haut Katanga      Dilolo    3          1      3\n4  Haut Katanga      Dilolo    4          1      3\n5  Haut Katanga      Dilolo    5          1      3\n6  Haut Katanga      Dilolo    6          1      3\n7  Haut Katanga      Dilolo    7          1      3\n8  Haut Katanga      Dilolo    8          1      3\n9  Haut Katanga      Dilolo    9          1      3\n10 Haut Katanga      Dilolo   10          1      3\n\n\n\n\nBy health zone\nNow, we want to do this cumulative sum by health zone. This is not that complicated: we are going to sort our data frame properly by health zone and week, and use the .by argument to tell the mutate() function to perform the action by health zone.\n\n\n\n\n\n\nNote\n\n\n\nYou may remember from the aggregation session how we summarized by groups using the .by argument in the summarize() function. This is exactly the same idea, except that instead of returning one value by group (as summarize() does), we want to return one value per row (as mutate() does).\nAs a little reminder of how summarize() + .by work, here is how we would calculate the total number of patients and deceased by province over the whole dataset:\n\ndata_surv_weeks |&gt; \n  summarize(\n    .by = province,  # Do things by province\n    cases_tot = sum(totalcases, na.rm = TRUE),\n    dead_tot  = sum(totaldeaths, na.rm = TRUE)\n  )\n\n# A tibble: 4 × 3\n  province     cases_tot dead_tot\n  &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n1 haut_katanga      5948       34\n2 haut_lomami       6928       70\n3 lualaba           1485        3\n4 tanganyika        7836      137\n\n\n\n\n\n\nAdd a step to your previous pipeline to sort the data frame by province, health zone and week with the arrange() function, which is a sorting function from {dplyr}\n\n\ndata_surv_weeks |&gt; \n  arrange(province, health_zone, week)\n\n\nThen add the following code to calculate the cumulative sum:\n\n\nmutate(\n  .by = c(province, health_zone),\n  cumcas = rollapply(\n    data  = totalcases,\n    width = 3,          # Width of the window  \n    FUN   = sum,        # Function to apply, here the sum   \n    align = \"right\",    # Windows are aligned to the right\n    partial = TRUE,     # Allows sum to be made even if window is less than three\n    na.rm = TRUE        # Extra unamed argument to be passed to the sum function\n  )\n)\n\n\nNow that the complicated part is over (the computing of the cumulative sum) we are left to summarize the information with a binary indicator, for heach week and health zone. Then we can create a second indicator, that aggregates the result of both the weekly and cumulative indicators, to say if an alert is to be raised.\n\n\nAdd a new step to your pipeline to calculate a binary indicator, cumcases35 that is 1 if the cumulative sum of cases for that week is equal or above 35 and 0 if not.\nAdd a new column alert, that is 1 if either the cases20 indicator or the cumcases35 indicator is 1 and 0 otherwise. You can use the | operator, which is R logical OR (the test will output TRUE if at least one of the condition is TRUE)..\nWhen the pipe is working, assign the result to a data_alert data frame.\n\nThe main columns of data_alert should look like this (other hidden for display):\n\n\n# A tibble: 10 × 7\n   health_zone  week totalcases cases20 cumcas cumcases35 alert\n   &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 kampemba        1         75       1     75          1     1\n 2 kampemba        2         42       1    117          1     1\n 3 kampemba        3         46       1    163          1     1\n 4 kampemba        4         50       1    138          1     1\n 5 kampemba        5         43       1    139          1     1\n 6 kampemba        6         33       1    126          1     1\n 7 kampemba        7         45       1    121          1     1\n 8 kampemba        8         52       1    130          1     1\n 9 kampemba        9         38       1    135          1     1\n10 kampemba       10         46       1    136          1     1\n\n\n\n\n\n\n\nHealth zones in alert\nAfter all this work we can finally investigate which health zones are in alert in the last week of our dataset (the now of the case study, week 20)!\n\nFilter your data frame to only keep the 20th week. Which health zones are in alert?\nCreate a vector hz_alert that contains the name of the health zones in alert, so that we can use it to filter data from these health zones later."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#draw-the-epicurve-question-4",
    "href": "sessions_companion/surveillance_companion.html#draw-the-epicurve-question-4",
    "title": "Surveillance",
    "section": "Draw the epicurve (Question 4)",
    "text": "Draw the epicurve (Question 4)\nLet us draw the epicurves of health zones currently in alert (in alert during week 20).\nWe have drawn very similar curves in the epicurve session. Here again we will use the ggplot() function with the geom_col() geom to create a barplot showing the distribution of cases. Since we already have the number of cases per week we do not need to count it ourselved like we did in the past.\n\nDraw an epicurve for one of the health zones in alert.\n The graph should look like this (but maybe for another health zone):\n\n\n\n\n\n\n\n\n\n\nThe facet_wrap() function allows us to plot several subplots in the same graph (see the faceting satellite for more information on faceting):\n\ndata_alert |&gt;\n  filter(health_zone %in% hz_alert) |&gt;\n  ggplot(aes(x = week, \n             y = totalcases)) + \n  geom_col(fill = \"#2E4573\") + \n  theme_bw(base_size = 15) + \n  labs(x = \"Week\",\n       y = \"N cases\",\n       title = \"Health zones in alert\") +\n  facet_wrap(vars(health_zone))   # One graph by health_zone"
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#key-indicators-question-6",
    "href": "sessions_companion/surveillance_companion.html#key-indicators-question-6",
    "title": "Surveillance",
    "section": "Key indicators (Question 6)",
    "text": "Key indicators (Question 6)\nLet’s gather more data on both alerts to help you decide which one to investigate.\n\n\n\n\n\n\nTip\n\n\n\nThis session builds on summarizing skills seen in the summary session. Do not hesitate to check it or your code if you forgot something.\n\n\n\nWeek of the first alert\n\nUse the summarize() function to display the first week the alert was raised for each health zone in alert. Which health zone started first?\n\n\n\nSurveillance data indicators\nLet us go back to the full surveillance dataset that contains more columns of interest.\n\n\nAdd a column cunder_5 to data_surv that contains the the number of cases less than five years.\nDerive, for each health zone in alert, the following indicators (organized in a single table):\n\n\nThe number of cases\nThe number of deaths\nThe number of less than five year olds\nThe CFR in percentage\nThe percentage of reported cases under five\n\nThe result should look like this:\n\n\n  health_zone n_cases n_deaths n_under_5 p_under_5     cfr\n1    kampemba     730        0       544 0.7452055 0.00000\n2      lwamba     256        2       233 0.9101562 0.78125\n\n\n\n\n\nLab data indicators\nNow we are going to use the laboratory data to derive a couple more indicators.\n\nFor each health zone in alert, derive the following indicators within one table:\n\nThe number of patients tested for measles\nThe number of positives for measles\nThe percentage of positives for measles\nThe number of patients tested for rubella\nThe number of positive for rubella\nThe percentage of positive for rubella\n\nThe result should look like this:\n\n\n  health_zone n_test_meas n_test_meas_pos positivity_measles n_test_rub\n1      lwamba          10               5          0.5000000         10\n2    kampemba          14               4          0.2857143         14\n  n_test_rub_pos positivity_rubella\n1              0         0.00000000\n2              1         0.07142857\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the section on summaries with conditions to remind you of the more advanced summaries."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#done",
    "href": "sessions_companion/surveillance_companion.html#done",
    "title": "Surveillance",
    "section": "Done!",
    "text": "Done!\nCongratulation, you are done!\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#sec-going-further",
    "href": "sessions_companion/surveillance_companion.html#sec-going-further",
    "title": "Surveillance",
    "section": "Going Further",
    "text": "Going Further\n\nExploring the complete() function\nLook at the simplified example below: the Kitenge health zone has no row for week 2:\n\n# Create simplified data frame for the example, with three weeks\nexample_df = data.frame(\n  province    = c(\"haut_katanga\", \"haut_katanga\", \"haut_katanga\", \"haut_lomami\", \"haut_lomami\"),\n  health_zone = c(\"likasi\", \"likasi\", \"likasi\", \"kitenge\", \"kitenge\"),\n  week        = c(1, 2, 3, 1, 3),\n  totalcases  = c(2, 1, 3, 1, 2))\n\nexample_df\n\n      province health_zone week totalcases\n1 haut_katanga      likasi    1          2\n2 haut_katanga      likasi    2          1\n3 haut_katanga      likasi    3          3\n4  haut_lomami     kitenge    1          1\n5  haut_lomami     kitenge    3          2\n\n\nWe use the following code to make sure that all the health zones have all the possible week values. Since the weeks range from one to three in that toy example, we pass a vector with weeks ranging from one to three:\n\n# Complete the missing week in Kitenge\nexample_df |&gt; \n  complete(\n    nesting(province, health_zone), \n    week = seq(1, 3),             # vector from 1 to 3\n    fill = list(totalcases = 0)   # fill new lines with zero (default is NA)\n  ) \n\n# A tibble: 6 × 4\n  province     health_zone  week totalcases\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 haut_katanga likasi          1          2\n2 haut_katanga likasi          2          1\n3 haut_katanga likasi          3          3\n4 haut_lomami  kitenge         1          1\n5 haut_lomami  kitenge         2          0\n6 haut_lomami  kitenge         3          2\n\n\nNow both health zones within provinces have values for all three weeks.\nYou may be wondering why we used nesting(province, health_zone) and not just health_zone. The reason is that there could be two health zones in different provinces with the same name. So we need to keep the province column into account. The nesting() argument tells the function to only use the existing combinations of the two columns in the data frame.\n\n\n\n\n\n\nNote\n\n\n\nIf we were passing both column names to the complete() function, it would try to cross all levels of province to all levels of health_zone, which does not make sense in this case:\n\n# Complete the missing week in kikula\nexample_df |&gt; \n  complete(\n    province, health_zone, \n    week = seq(1, 3),  # vector from 1 to 3\n    fill = list(totalcases = 0)\n  ) \n\n# A tibble: 12 × 4\n   province     health_zone  week totalcases\n   &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1 haut_katanga kitenge         1          0\n 2 haut_katanga kitenge         2          0\n 3 haut_katanga kitenge         3          0\n 4 haut_katanga likasi          1          2\n 5 haut_katanga likasi          2          1\n 6 haut_katanga likasi          3          3\n 7 haut_lomami  kitenge         1          1\n 8 haut_lomami  kitenge         2          0\n 9 haut_lomami  kitenge         3          2\n10 haut_lomami  likasi          1          0\n11 haut_lomami  likasi          2          0\n12 haut_lomami  likasi          3          0\n\n\n\n\nIt would be good to automatically pick the week series, since the data frame is going to change every week. To do that, we can remplace hardcoded values by the smallest and largest week number in the week column to get the range of weeks in the dataset:\n\n# Complete the missing week in kikula\nexample_df |&gt; \n  complete(\n    nesting(province, health_zone),\n    week = seq(min(week, na.rm = TRUE),   # vector ranging from smallest to largest week numbers in dataset\n               max(week, na.rm = TRUE)),\n    fill = list(totalcases = 0)\n  ) \n\n# A tibble: 6 × 4\n  province     health_zone  week totalcases\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 haut_katanga likasi          1          2\n2 haut_katanga likasi          2          1\n3 haut_katanga likasi          3          3\n4 haut_lomami  kitenge         1          1\n5 haut_lomami  kitenge         2          0\n6 haut_lomami  kitenge         3          2\n\n\n\n\nExploring the rollaply() function\nIf we want to do a cumulative sum of cases over three weeks, we want to apply the sum() function over windows of three weeks.\n\nexample_vect &lt;- rep(1, time = 10)\nexample_vect\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\nrollapply(\n  data  = example_vect,\n  width = 3,       # Width of the window  \n  FUN   = sum,     # Function to apply, here the sum   \n  align = \"right\"  # Value at row i is the sum of i, i-1 and i-2.\n)\n\n[1] 3 3 3 3 3 3 3 3\n\n\nWe inputed a vector of ten values and obtained a vector of lenght height, containing the sums. Obviously the function has a way of dealing with the extremities, and the size of the output is smaller than the size of the input. This would be a problem in a mutate() that creates new columns in a data frame, that need to be the same length as the existing columns.\nYou can control the behavior at the extremities:\n\nFill with NA when there is not enough values to calculate a window of three\nAllow partial sums (some values represent less than three weeks)\n\nThe argument fill = NA pads the extremities with NA (on the left in our case, since we aligned right):\n\nrollapply(\n  data  = example_vect,\n  width = 3,       # Width of the window  \n  FUN   = sum,     # Function to apply, here the sum   \n  align = \"right\", # Windows are aligned to the right\n  fill  = NA\n)\n\n [1] NA NA  3  3  3  3  3  3  3  3\n\n\nIt is a reasonnable way of dealing with incomplete windows. In our case however, we can do better: if there were 40 cases in week 1 it would be a cause for alert! We thus want the cumulative sum to be calculated from week one to be able to detect early alerts. The partial = TRUE argument allows this:\n\nrollapply(\n  data    = example_vect,\n  width   = 3,       # Width of the window  \n  FUN     = sum,     # Function to apply, here the sum   \n  align   = \"right\", # Windows are aligned to the right\n  partial = TRUE)\n\n [1] 1 2 3 3 3 3 3 3 3 3\n\n\nThis is close to what we need.\n\n\n\n\n\n\nNote\n\n\n\nKeeping in mind that since the first two weeks have only partial data compared to later weeks, a lack of alert in these weeks does not necessarily means there is no alert, just that we do not have the data to detect it.\n\n\nYou may remember that arithmetic operations in R return NA if some of the values are NA and we usually need to pass the argument na.rm = TRUE to the functions for them to ignore missing values.\nIf we had a slightly less complete vector we would have a problem:\n\nexample_vect_missing &lt;- c(1, 1, 1, NA, 1, 1)\n\nrollapply(\n  data  = example_vect_missing,\n  width = 3,       # Width of the window  \n  FUN   = sum,     # Function to apply, here the sum   \n  align = \"right\", # Windows are aligned to the right\n  partial = TRUE   # Allows sum to be made even if window is less than three\n)\n\n[1]  1  2  3 NA NA NA\n\n\nFortunately we can pass the na.rm = TRUE argument to rollapply() so that it passes it to sum().\n\nrollapply(\n  data  = example_vect_missing,\n  width = 3,       # Width of the window  \n  FUN   = sum,     # Function to apply, here the sum   \n  align = \"right\", # Windows are aligned to the right\n  partial = TRUE,  # Allows sum to be made even if window is less than three\n  na.rm = TRUE     # Extra unamed argument to be passed to the sum function\n)\n\n[1] 1 2 3 2 2 2\n\n\n\n\n\n\n\n\nTip\n\n\n\nHere we applied the sum() function to create a cumulative sum over 3 weeks. But you could, with minimal modifications, apply the mean() function to caclulate a moving average!\n\n\nA last point on the align argument. It defines the position of the rolling windows compared to the value being calculated. The default is that the window is centered: the value i is the sum of values i, i-1 and i+1.\nExample of the three alignements (pading with NA to better see what’s happening):\n\nrollapply(data  = c(5, 10, 1, 2, 5, 10),\n          width = 3, \n          FUN   = sum,\n          align = \"left\",\n          fill = NA)\n\n[1] 16 13  8 17 NA NA\n\nrollapply(data  = c(5, 10, 1, 2, 5, 10),\n          width = 3, \n          FUN   = sum,\n          align = \"center\",\n          fill = NA)  # The default\n\n[1] NA 16 13  8 17 NA\n\nrollapply(data  = c(5, 10, 1, 2, 5, 10),\n          width = 3, \n          FUN   = sum,\n          align = \"right\",\n          fill = NA)\n\n[1] NA NA 16 13  8 17\n\n\nIn our case we want the value for a given week to reflect this week and the week past, so we align the window right, to calculate backward in time (by opposition, if we aligned left we would calculate forwards in time).\n\n\nNicely formatted percentages\nThe percent function from the {scales} packages can add percentage formatting to a value.\n\nscales::percent(0.8556)\n\nIt takes an accuracy arguments that controls the number of decimals:\n\nscales::percent(0.8556,\n                accuracy = 0.1)\n\nYou can wrap it around the values that you calulate in the summary tables to change the proportions into nicely formatted percentages.\n\n\n\n\n\n\nImportant\n\n\n\nOnce you aplied that function the column is treated as text (since we added a % sign) and you will not be able to do further arithmetical operations on it."
  },
  {
    "objectID": "sessions_core/02_import_data.html",
    "href": "sessions_core/02_import_data.html",
    "title": "Data Importation",
    "section": "",
    "text": "Create a RStudio Project\nSet up an organized and well documented code\nInstall and load packages\nWrite robust file paths\n\nImport and inspect data\n\n\n\n\n\n\n\nImportant\n\n\n\nThe principles you learned in the Introduction to R session will apply here as well: we should do our best to ensure that our projects won’t just work today but can also be reused and shared in the future. While doing this is not always easy, there are several best practices that can help us, and one of the most important is to start with a good, organized code base."
  },
  {
    "objectID": "sessions_core/02_import_data.html#objectives",
    "href": "sessions_core/02_import_data.html#objectives",
    "title": "Data Importation",
    "section": "",
    "text": "Create a RStudio Project\nSet up an organized and well documented code\nInstall and load packages\nWrite robust file paths\n\nImport and inspect data\n\n\n\n\n\n\n\nImportant\n\n\n\nThe principles you learned in the Introduction to R session will apply here as well: we should do our best to ensure that our projects won’t just work today but can also be reused and shared in the future. While doing this is not always easy, there are several best practices that can help us, and one of the most important is to start with a good, organized code base."
  },
  {
    "objectID": "sessions_core/02_import_data.html#setting-up-your-project",
    "href": "sessions_core/02_import_data.html#setting-up-your-project",
    "title": "Data Importation",
    "section": "Setting up your Project",
    "text": "Setting up your Project\n\nFolder Structure\n\nIf not done already, download and unzip the course folder. Save the uncompressed folder to a location that is not connected to OneDrive and navigate into it.\n\n\n\n  Course Folder\n\n\n\n\nThis folder gives an example of a typical (and highly recommended) structure for your code projects:\n\n📁 data\n\n📁 clean\n📁 raw\n\n📁 R\n📁 outputs\n\nThis folder will be you working directory for all the sessions of this course. You will create an Rstudio project in it (explanations below), and save all your scripts in /R. The course datasets are already in data/raw.\n\n\nDefinitions\nTo work through this session you need to understand the two following concepts:\nWorking directory. The working directory is the location (folder) where your R session is actively working. If you save a file, for example, it will be saved into this folder by default. Similarly, when you want to open a file, this folder will be shown by default. All relative paths will be relative to this working directory. By default, R usually picks the “Documents” folder as the working directory on Windows machines.\nRoot. The root refers to the top-most folder level of the working directory. If your course folder was called FETCHR, the root would then be directly inside it (as opposed to being inside one of its subfolders like R or Data).\n\n\nRStudio Projects\nAn RStudio Project can be used to make your life easier and help orient RStudio around the various files used in your code\nAs a quick reminder, your interface should look something like this:\n\n\n\n\n\n\nFigure 1: Screenshot of a typical Rstudio interface\n\n\n\n\nOpen RStudio and create a new project by clicking File &gt; New Project &gt; Existing Directory &gt; Browse, navigating into (opening) the course folder, and clicking Create Project.\n\n\nIn the Windows Explorer, look at the course folder. You should now see a new file with the extention .Rproj that has a small blue icon with an R in it.\n\n\n\n\nIcon associated with RStudio projects\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you don’t see this file, it’s probably because it is hidden by default on your computer. To change this setting in Windows Explorer, go into the View menu and select Filename Extensions.\n\n\nWhen you open an RStudio Project, RStudio starts a new R session, opens the associated project files, and sets your working directory to the root of the course folder. At this time, RStudio also displays the subfolders of this directory in the panel on the bottom right.\n\n\n\n\n\n\nTip\n\n\n\nIt is strongly recommended to set up a separate RStudio Project for each of your analyses to ensure that your project files remain organized and manageable.\n\n\nThere are several ways to open an RStudio Project:\n\nUse the RStudio menu File &gt; Open Project and then select the relevant .Rproj file\nClick on the Project: (none) button on the top right of the RStudio interface\nNavigate in the folder explorer to the analysis folder and double click on the file with the .Rproj extension\n\n\n\nRStudio Options\nBefore continuing, let’s update some of RStudio’s problematic default settings:\n\nOpen the global options (Tools &gt; Global Options) and open the tab General (left menu). Make sure that none of the boxes in the sections R Sessions, Workspace, or History are checked.\n\n\n\n\nScreenshot of the Rstudio options\n\n\nWhen checked, these options cause RStudio to save the objects in your environment and reload them as well as any files you previously had open when you open a new R session. While these default may seem like a good idea, it is better to always start your work from a fresh, empty R session to avoid bugs.\n\n\n\n\n\n\nImportant\n\n\n\nRemember that any commands or outputs that is needed for the cleaning and analysis should be saved explicitly in a script in the correct, functional order."
  },
  {
    "objectID": "sessions_core/02_import_data.html#creating-a-new-script",
    "href": "sessions_core/02_import_data.html#creating-a-new-script",
    "title": "Data Importation",
    "section": "Creating a New Script",
    "text": "Creating a New Script\n\nOpen a new script and save it in the R folder of your project under the name import_data.R.\nAdd some metadata to the top of the script as seen in the first session using comments. Be sure to include:\n\nTitle\nAuthor\nCreation Date\nDescription\n\n\nNow you’re ready to start coding!"
  },
  {
    "objectID": "sessions_core/02_import_data.html#sec-packages",
    "href": "sessions_core/02_import_data.html#sec-packages",
    "title": "Data Importation",
    "section": "Packages",
    "text": "Packages\nPackages are collections of functions that extend the functionality of R. You’ll use them a lot, both in this course and in your daily life. Fortunately, as an open source language, R packages can be downloaded and installed for free from the internet.\n\n\n\n\n\n\nNote\n\n\n\nIn R, packages are referenced using {}. For example {ggplot2} is the name of the ggplot2 package that contains new plotting functions such as ggplot(), geom_point() etc…\n\n\n\nInstallation\nWe can install a new package using the function install.packages(), which downloads and installs it into the package library on your computer. This is done once per computer.\n\ninstall.packages(\"here\") # install the {here} package\n\nDon’t forget to wrap the package name in quotation marks when using install.packages(). What happens if you don’t do this?\n\n\n\n\n\n\nNote\n\n\n\nIf you are following this session as part of a course, to avoid any potential internet connectivity issues during the training we already had you install most of the course packages.\nIf are following this tutorial on your own or have not installed the packages yet, you will have to manually install each new package that we encounter.\n\n\n\n\nUsage\nOnce a package is installed we can use it but we have to specify to R that we will be using it every single session. This process is called loading the package and is achieved using the function library().\n\nlibrary(here) # load the \"here\" package\n\n\nUse the library() function to load the packages here and rio, which will be used in the next section.\n\nBased on your computer’s set up and the package you are trying to load, you may get a warning message noting that some functions have been masked or that the current version of the package was built for a different version of R. These messages are not usually a problem but are still important to note.\n\nTry to run the following code. Can you work out what the error means?\n\nlibrary(ggplot)\n\n\nThe above code throws an error because you have asked for a library that doesn’t exist. Remember that R is fickle and case sensitive and many of your errors will come from small typos in the names of functions or objects. Here, for example, we wanted to load the package ggplot2 but wrote ggplot instead.\n\n\n\n\n\n\nTip\n\n\n\nMost of the time, you’ll need to load a number of packages for your script and it is recommended to have a section at the start of your code that loads everything you’ll need in one place:\n\n# Packages ----------------------------\nlibrary(tidyverse)   # data manipulation\nlibrary(lubridate)   # date manipulation\n\nThis practice makes it easy to tell which packages need to be installed to run a script.\n\n\n\nUse comments to create a “Packages” section to your script.\n\n\n\nUpdating Packages\nR has a very active community of developers and it’s pretty common for packages to be updated from time to time as their owners add in new functions and fix existing bugs. In order to update the packages in your library, you can go into the Packages tab of the bottom right panel and click Update. Don’t forget that you’ll need to be connected to the internet during this process.\n\n\n\n\n\n\nImportant\n\n\n\nSometimes packages are updated in a way that might remove or change a function that you used in some of your scripts, causing your code to no longer work. Don’t panic if this happens. The best practice is to adapt your code, although in the worst case scenario you can forcibly install an old version of a package. This is however out of the scope of this session."
  },
  {
    "objectID": "sessions_core/02_import_data.html#data-importation",
    "href": "sessions_core/02_import_data.html#data-importation",
    "title": "Data Importation",
    "section": "Data Importation",
    "text": "Data Importation\n\nUnderstanding File Paths\nTo open a file in R you need to provide a file path. A file path is simply a longer name for a file, that includes not only its name but also its location on your computer. There are several ways of defining these paths, including absolute and relative paths.\n\nAbsolute Paths\nAbsolute paths are specific to your computer and go all the way up to the level of your hard drive. For example, an absolute path may look something like this: D:/OneDrive - MSF/Documents/monitoring/cholera/fancy_project/data/raw/example_linelist.xlsx. Clearly, this path will only work on one specific computer.\nThe use of hard coded absolute paths is strongly discouraged as it makes your code inflexible and prone to break: the paths need to be updated every time your code is shared or the project folder is moved on your computer.\n\n\nRelative Paths\nRelative paths are defined relatively to your current working directory. For example, keeping in mind that our handy .Rproj file set our working directory to the root of our project folder, we could create a relative path that looked like data/raw/example_linelist.xlsx. This means that as long as we maintain the internal structure of our project folder and have an .Rproj file, our code would theoretically run on multiple computers.\n\n\nRobust Paths with the here() function\nThe {here} package has a here() function that really helps defining paths. It has two advantages:\n\nWhen used with RStudio projects, you can give it only the part of the path within the project, (the relative path in other words), and the function uses it to create the absolute path dynamically.\nIt does so using the separator adapted to you operating system, whether it’s /, \\, or //\n\n\nlibrary(here)\nhere(\"data\", \"raw\", \"example_linelist.xlsx\")\n\n[1] \"/tmp/RtmpZ3YPU1/file89d7058623519/data/raw/example_linelist.xlsx\"\n\n\nSee how we only defined the relative path and the function created an absolute path. This way of defining the path will work on your colleagues computer, even if they run on another operating system, as long as you both respect the internal structure of the working directory.\nWe strongly encourage you to use here() whenever you need to create a file path.\n\nRun the above code in the console. What file path does here(\"data\", \"raw\") give you?\n\n\nUsing here(), create a complete file path for the file Moissalla-measles-linelist-EN.xlsx. Keep this path around, we will use it soon.\n\n\n\n\n\n\n\nImportant\n\n\n\nhere() simply creates a file path, it doesn’t actually check if a file exists on your computer: if the file is absent or there is a typo in your code, the command will yield an error when the path is used. If you would like to use a function to check if a file exists, check out the file.exists() function.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe will often want to source multiple data files in a single project. To make that process easier, it can be helpful to create a section at the start of the script, after loading the packages to define paths and store them in variables.\n\n\n\n\n\nImport function\nIn R different file formats are often imported using different, often specialized functions. This can be tedious as it requires you to memorize and load a large number of functions just to get your data imported. To avoid this problem, we recommend that you use the import() function from the package {rio}. This function is able to open a large variety of files (including Excel, csv, Stata, and many others) by recognizing the file extension of your data and calling a relevant specialized function from another package so that you don’t have to.\nBecause import() is actually just calling other functions in the background, it is possible that it will need different arguments depending on the type of file you want to load.\n\n\n\n\n\n\nTip\n\n\n\nTo see the full list of all the file types you can load (and save!) with rio, check out the list of supported formats on their website. In the rest of the lesson we will focus on importing data from Excel .xlsx files.\n\n\n\nImporting from the First Sheet\nIn general, the usage of import() is pretty simple, at minima you need to pass the path of the file to the file argument\n\nimport(file = here(\"data\", \"raw\", \"example_linelist.xlsx\"))\n\nNotice that we have nested the command here() inside the import() command. Nesting functions is absolutely allowed in R and is something you will do all the time. When functions are nested, R will evaluate them in the order of the innermost function (in this case here()) to the outermost (in this case import()). In this way, the output of here() is being used as the input of import().\n\nImport the file Moissalla-measles-linelist-EN.xlsx that is in your raw data subfolder into R using here() and import().\n\nIf your import worked correctly, R will print the data into the console but not save it into the environment because we have not assigned them to an object.\n\n\n\n\n\n\nTip\n\n\n\nYou may not want to have R print very large datasets into the console and assign them directly to an object.\n\n\n\nReimport your data but this time save it to an object called df_linelist.\n\n\n\nImporting Data from Any Sheet\nAs you just saw, R selects the first sheet by default. It is however possible to pass the number (or name) of a specific worksheet in your Excel data to import() using the argument which:\n\nimport(file = here(\"data\", \"raw\", \"example_linelist.xlsx\"),\n       which = 2)  # imports the second sheet\n\nNote that the which argument is specific to the file types that have multiple sheets, such as Excel or .Rdata files. If you try to use it on a .csv file the argument will be ignored."
  },
  {
    "objectID": "sessions_core/02_import_data.html#taking-a-first-look-at-your-data",
    "href": "sessions_core/02_import_data.html#taking-a-first-look-at-your-data",
    "title": "Data Importation",
    "section": "Taking a First Look at your Data",
    "text": "Taking a First Look at your Data\nWe have now imported a dataset into R and assigned it to a dataframe (df_linelist). The natural next step is to inspect this dataset, to check that the import went well, get to know it a bit better, and assess if it requires any cleaning before analysis.\nWe can start by taking a quick look at the first few lines of the dataframe using the function head(). This function takes a dataframe as its first argument and optionally accepts a second argument n indicating the number of lines we would like to see.\n\nhead(df_linelist, n = 10) # Inspect 10 first lines\n\n\nUse head() to examine the 12 first lines of df_linelist.\n\nWe can also check out our data by looking at the Environment tab of the top-right panel. Here, we can see our dataframe in the environment, look at its structure, or open it in the data viewer of RStudio.\n\nClick on the round blue button next to df_linelist in your environment to see its structure. Then click on the name of the dataset to open it in the viewer.\n\nThe data viewer displays dataframes as tables and is a convenient way to quickly look at your data. You can even sort and filter your data in the “View”, though be aware that these actions will not make any changes to the actual object df_linelist. The View can also be opened by passing the dataframe to the function View()."
  },
  {
    "objectID": "sessions_core/02_import_data.html#done",
    "href": "sessions_core/02_import_data.html#done",
    "title": "Data Importation",
    "section": "Done!",
    "text": "Done!\nWell done and don’t forget to save your code.\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_core/02_import_data.html#going-further",
    "href": "sessions_core/02_import_data.html#going-further",
    "title": "Data Importation",
    "section": "Going Further",
    "text": "Going Further\n\nExtra Exercises\n\nUse dim() to take a look at the dimensions of your dataset.\nUse str() to check the data type of each column. Does anything look odd? Remember that you can also use functions like is.character() and is.numeric() if you’d like to test the type of a particular column.\nUsing a function learned in the first session, can you extract the names of the columns of the dataset? Do these results match what you see when you open the data in Excel?\nTry passing your dataframe to the function summary(). What does this function tell you?\n\n\n\nAdditional Resources\n\nThe {rio} website\nMore examples on importing data of various file types"
  },
  {
    "objectID": "sessions_core/01_introduction.html",
    "href": "sessions_core/01_introduction.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Familiarize yourself with RStudio\nLearn how to work with the console\nCreate and execute a script\nCreate basic R objects, including vectors and data frames"
  },
  {
    "objectID": "sessions_core/01_introduction.html#objectives",
    "href": "sessions_core/01_introduction.html#objectives",
    "title": "Introduction to R",
    "section": "",
    "text": "Familiarize yourself with RStudio\nLearn how to work with the console\nCreate and execute a script\nCreate basic R objects, including vectors and data frames"
  },
  {
    "objectID": "sessions_core/01_introduction.html#exercise-format",
    "href": "sessions_core/01_introduction.html#exercise-format",
    "title": "Introduction to R",
    "section": "Exercise Format",
    "text": "Exercise Format\nThese exercises are in the format of a self-paced tutorial containing short explanations of key concepts, examples, and exercises for you to follow. The course uses a “learning by doing” approach, and while this first session will start with a lot of time exploring the RStudio interface, future sessions will focus heavily on having you write your own code.\nInstructions for exercises will be given in the following formats:\n\nThis is a general action block. You will typically see it at the beginning of a session with instructions about the setup for that lesson.\n Example: Open a blank new script and name it my_first_script.R.\n\n\nThis is a code block, it indicates a coding exercise where you will actually write your own code.\n Example: Create an object called region that contains the value \"Mandoul\".\n\n\nThis is an observation block, it will have instructions about something that you are expected to look at or investigate.\n Example: Inspect the RStudio interface.\n\nAs you move through these exercises, you may run into some errors, which occur when R is unable to complete a command. This can happen for many reasons: maybe you misspelled the name of an object, asked R to look for a file that doesn’t exist, or provided the wrong type of data to a function. Whenever an error occurs, R will stop any ongoing calculation and give you a message explaining what went wrong. Having errors is completely normal and happens to all programmers, novice and expert. Much like a natural language, R is something you will get better at the more you practice and work through your mistakes."
  },
  {
    "objectID": "sessions_core/01_introduction.html#rstudio-and-r",
    "href": "sessions_core/01_introduction.html#rstudio-and-r",
    "title": "Introduction to R",
    "section": "RStudio and R",
    "text": "RStudio and R\nR is a functional programming language that can be used to clean and manipulate data, run analyses (especially statistical ones), visualize results, and much more.\nRStudio is a piece of software that provides a user-friendly interface for R (also called an IDE, for Integrated Development Environment). While using a graphical interface isn’t required, it is strongly recommended for beginners.\n\nGetting Started with RStudio\nLet’s get started!\n\nOpen RStudio using the start menu or desktop shortcut; if RStudio is already open, please close it and open it again.\n\nYou should see an interface that looks something like this:\n\n\n\nView of the Rstudio IDE interface at opening\n\n\n\nInspect the RStudio interface.\n\nYou will have either three or four panels, including:\n\nUpper Right Corner\nTo the upper right there will be a panel with several tabs. Many of these are beyond the scope of this course, but we will use the following two tabs during the course:\n\nEnvironment. A list of the objects saved by the user in the current session. Because you’ve just started a new session, your environment should be empty.\nHistory. A record of all the commands you have executed during the current session.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can think of an R session like you would think of starting up a computer. Whenever a session starts, everything is blank and ready for computation in the same way that there aren’t any programs open when you first turn on your computer. In general, we encourage you to stop and start your R sessions regularly, you may just find that turning it off an on again will fix some of your bugs.\n\n\n\n\nBottom Right Corner\nTo the bottom right there will be another multi tab panel, including:\n\nFiles. A file explorer for the working directory, which is the folder location where R is currently working.\nPlots. A location where RStudio will display static visualizations; this tab should be empty for the moment.\nPackages. A list of all the R packages installed on your computer. Packages are collections of functions that help extend the functionality of R, and we will discuss them in greater detail in the next lesson.\nHelp. A place to read help pages and documentation for functions and packages.\nViewer. A location where RStudio will display html outputs such as tables, interactive widgets, or even full on dashboards.\n\n\n\nLeft Side\n\nTo the left (or bottom left if you have four panels) you should see the console, where R itself is run.\nTo the top left (if you have four panels) will be any open scripts.\n\nIn the next two sections, let’s talk about the console and scripts in more detail.\n\n\n\nThe Console\nThe console is where R itself is run.\nWhenever you open a new session, R will start by printing a bit of information about your set up, such as your R version number. Below this there should be a line containing the &gt; symbol and a blinking cursor. To run a command in R, you simply need to type it in after this &gt; and press Enter. R will then process your code and print the result (if there is one). A new &gt; line will then appear ready for the next command.\n\n\n\n\n\n\nImportant\n\n\n\nIf the last line shown in the console starts with a + instead of a &gt; that means the console is not ready for a new command either because it is still processing a previous one or because it received a bit of incomplete code. If at any point you would like to cancel an ongoing or incomplete command, press Esc.\n\n\n\nRun the following commands in the console one line at a time and observe the output.\n\n5 + 90\n\n6 * 171\n\n189 / 36.6\n\n92^3\n\n(12 + 9)^4 / 1000\n\nNow, run the following command. Note that the final ) is missing, making the command incomplete. What happens when you do this?\n\n3 / (2 + 97\n\n\nYou may have noticed in the above examples that our code includes a lot of spaces between characters. This is not by accident. It is considered best practice to include spaces around most operators, such as +, -, *, /, &lt;, &gt;, =, and &lt;-. Not only do these spaces make your code easier for other people to read and understand, in some (rare) cases they may even be necessary to avoid errors. That said, do be aware that there are a small number of operators that should not be surrounded by spaces, such as ^, . and :.\n\n1+29+4.8/3*3           # BAD\n1 + 29 + 4.8 / 3 * 3   # GOOD\n\n1 ^ 2 # BAD\n1^2   # GOOD\n\nWe can also run functions in the console. We will discuss functions in more depth later in this lesson, but meanwhile know that the idea of functions in R is very similar to the one in Excel, where you no doubt are familiar with functions such as SUM or MEAN.\n\nRun the following commands in the console (one line at a time).\n\n# Find the minimum value\nmin(5, 10)\nmin(1, 8, 56, 0.3)\n\n# Find the maximum value\nmax(568, 258, 314)\n\n\n\n\nScripts\nScripts are text files that contain a series of commands for a particular programming language. The extension of the file indicates which language the commands were written in, and we will be using .R. Scripts allow us to create code that can be reused, shared, and even automated.\n\nWriting Your First Script\n\n\n\nSteps to create a new script in the RStudio IDE\n\n\nTo create a new script, follow the menu File &gt; New File &gt; R Script. Alternatively, you can click on the small green + sign just below the File menu or use the keyboard shortcut CTRL+SHIFT+N. This new and unsaved script will appear as a blank document in the top left panel.\nTo save your script, either use the menu File &gt; Save As or the keyboard shortcut CTRL+S.\n\nCreate and save a new script called discovery.R. Don’t forget to include the .R extension. For now, you can save it on your Desktop or any convenient location, but we will talk more about organizing your scripts in the next session.\n\n\n\nExecuting Code from a Script\nTo run code from a script simply place your cursor on the line you wish to run (or select multiple lines) and do one of the following:\n\nClick the Run icon at the top right of the script panel\nUse the shortcut CTRL+Enter (cursor will move to the next line afterwards)\nUse the shortcut ALT+Enter (cursor will stay on the current line afterwards)\n\n\nCopy the code you ran in the previous exercises into your script and run it using each of the above methods.\nFrom now on, you will write your code in your script and execute it from there, unless told otherwise in the instructions.\n\n\n\nComments\nIn R, any text prefaced by a # (until the end of a line) is called a comment. R does not consider comments to be code and will ignore them whenever you run your scripts. This makes comments an excellent way to document your code.\n\n# This is a comment\n\n2 + 3  # This is also a comment\n\nIt is helpful to future you and others to start your scripts with a few commented lines providing some information about the file.\n\n#### IMPORT & PREPARE DATA ####\n# Author :  Mathilde Mousset\n# Creation Date : 23/11/2024\n# Last Update : 30/11/2024\n# Description : Import and clean measles surveillance data from Moissala\n\n\nAdd some comments to the top of your script describing it.\n\nComments are also a handy way to split longer scripts into thematic sections, such as “Data Importation”, “Analysis”, “Visualization”, etc. For example:\n\n# NAME OF SECTION 1 -----------------------------------------------             \n\n# NAME OF SECTION 2 -----------------------------------------------             \n\n\nUse comments to create sections in your script that correspond to the main sections in this tutorial.\n\nFinally, comments allow us write helpful notes for our colleagues (and our future selves) that can help them understand the code and why we wrote it the way we did. The general guidance is to focus on comments that explain the “why” rather than the “what”. This is because the “what” of well written code should be relatively self explanatory.\nThis comment, for example, is completely superfluous:\n\n1 + 3  # Code to add one to three\n\nBy comparison, here are a few use cases that would warrant comments:\n\nYou define a constant, say a seroprevalence threshold value. You may want to add a comment providing the reference for where the value comes from.\nYour code contains a value or file name that needs to be updated every week. You should indicate this with a comment to ensure that anyone else using the code is aware.\nYou use a rare command or package that your colleague may not know or may find counter intuitive. You can use a comment to explain the rational behind that decision.\n\nThat being said, you are learning, and the scripts you are writing during this course are your notes, so feel free to us as many comments (of the “what” and “why” sort) as you need. You will naturally write less comments in the future, when some of the things that seem alien now become natural.\n\n\n\n\n\n\nTip\n\n\n\nYou can comment a selected line with the shortcut CTRL+SHIFT+C.\nYou can add a first level section with CTRL+SHIFT+R.\n\n\n\nAdd some comments to describe the code that you’ve written thus far in your script."
  },
  {
    "objectID": "sessions_core/01_introduction.html#data-types",
    "href": "sessions_core/01_introduction.html#data-types",
    "title": "Introduction to R",
    "section": "Data Types",
    "text": "Data Types\nR has several different data types. The ones we will see most often in this course include:\n\nnumeric\nstring (text)\nboolean (TRUE / FALSE)\ndate\nfactor\n\n\nNumerics\nThe numeric type includes both integers and doubles (numbers that include a decimal) and can be created by simply writing the “naked” value into your script or console.\n\n\nStrings\nStrings are the R version of text and can be created by surrounding text with single or double quotation marks, for example \"district\" or 'cases' (double quotations are typically considered best practice).\n\nCompare the output in the console for the following commands:\n\n28         # numeric\n\"28\"       # text\n28 + \"28\"  # produces an error\n\n\nThe last command above will give an error because we cannot perform arithmetic operations that combine text and numbers.\n\n\n\n\n\n\nImportant\n\n\n\nR is case sensitive, meaning that the string \"ABC\" is not the same as \"abc\".\n\n\n\nIf you would like to create a string that contains a quotation mark the best practice is to escape the character by putting a \\ in front of it, ie: \"She said \\\"Hello\\\" then left\" or 'it\\'s a beautiful day'. Equivalently, if you used a double quotation to create the string you can use single quotes inside of it freely (ie: \"it's a beautiful day\") and vice versa (i.e.: 'She said \"Hello\" then left').\n\n\n\nBoolean (Logical)\nThe boolean (or “logical”) type stores true/false values and is created by writing either TRUE or FALSE without quotation marks.\nInternally, R thinks of TRUE and FALSE as being a special version of 1 and 0 respectively, and boolean values can be easily translated to these numeric equivalents for arithmetic operations.\n\n\n\n\n\n\nNote\n\n\n\nYou may find people using T or F but this is discouraged as T and F can also be used as object or variable names. TRUE and FALSE, however, are protected in R, meaning they cannot be reassigned to another value.\n\n\n\n\nDetermining the Type of an Object\nThere are several functions to determine the type of an object (often called the class of the object in R).\n\nType the following commands in your script and run them:\n\n# Get the Type of an Object\nclass(28)  \nclass(\"Mandoul\")\n\n# Test the Type of an Object\nis.numeric(28)\nis.numeric(\"Mandoul\")\nis.character(\"Mandoul\")\n\nis.numeric(TRUE)\nis.character(TRUE)\nis.logical(FALSE)"
  },
  {
    "objectID": "sessions_core/01_introduction.html#sec-assignement-operator",
    "href": "sessions_core/01_introduction.html#sec-assignement-operator",
    "title": "Introduction to R",
    "section": "Creating an Object",
    "text": "Creating an Object\nIn R, pretty much everything is an object, including functions, scalar values, and other more complex data structures. Before introducing these structures, let’s take an important detour to discuss how objects are saved into your environment.\nOften, we will want to reuse the same values or data throughout a script and it is therefore very useful to store them as objects in our environment. To do this we use the assignment operator, &lt;-.\n\nLook at the environment panel on the top right, verifying that it is empty, then type the following command in your script and run it to save a variable called cases into your environment.\n\ncases &lt;- 28\n\nLook at the environment again. Is it still empty?\n\nIf you’d like to access the value of your new object, cases, you simply need to execute its name.\n\ncases\n\n[1] 28\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe reason we need to wrap strings in quotation marks is actually to allow R to differentiate between strings (\"cases\" and object names cases).\n\n\nOnce created, objects can be used in other commands:\n\ncases + 5\n\n[1] 33\n\n\n\nFrom your script, create an object called region that contains the value \"Mandoul\". Do you see it in your environment?\n\n\n\n\n\n\n\nTip\n\n\n\nDon’t forget that we should always surround &lt;- with spaces to improve readability and avoid errors.\n\nx&lt;-3     # BAD\nx &lt;- 3   # GOOD\n\n\n\n\nUpdating an Object\nWe often want to update the value stored in an object. To do this, we simply assign a new value with the same syntax we used to create it in the first place:\n\ncases &lt;- 32\n\n\nUpdate the value of region to \"Moyen Chari\".\n\n\n\nObject Names\nWhen naming your objects, there are a few (relatively) hard rules:\n\nDon’t start with a number\nDon’t use spaces (use a _ instead)\nDon’t use protected values (like TRUE and FALSE) or function names (like mean)\nDon’t use capital letters\n\nBeyond these hard rules, there are also more subjective best practices and personal styles. In general aim for names that are short and descriptive:\n\na &lt;- 19                             # BAD (not informative)\nage_du_patient_a_l_admission &lt;- 19  # BAD (too long)\nage &lt;- 19                           # GOOD\n\nGiving your objects clear and informative names helps to make your code readable, making it easy for others to understand without the need for checking the data dictionary every two seconds."
  },
  {
    "objectID": "sessions_core/01_introduction.html#data-structures",
    "href": "sessions_core/01_introduction.html#data-structures",
    "title": "Introduction to R",
    "section": "Data Structures",
    "text": "Data Structures\nUp until now we have looked only at simple objects that store single values, let’s now turn our focus to more complex structures that can store entire datasets.\n\nVectors\nWe can collect multiple values (such as numerics or strings) into a single object, called a vector.\nTechnically, there are several types of vectors, for example:\n\nSimple vectors (or atomic vectors) can only contain one type of values. For example, a numeric vector 2, 4, 6 or a string vector \"Mandoul\", \"Moyen Chari\".\nRecursive vectors (usually called lists) are far more complex and can contain multiple dimensions and types of data. We will not learn about them in this lesson.\n\nThis course will not go into detail on the more abstract concepts behind these structures and instead focus only on those you will encounter most often in your daily work.\n\nSimple Vectors\nSimple vectors can contain one or more values of a single data type, they thus have two key properties: length and type. For the purpose of this class, we will use the terms “simple vector” and “vector” interchangeably (as is typical in the R community).\nYou’ve technically already created your first simple vector when you built cases and region. These were simply vectors with a length of one. To create a vector with more than one value, we will use the function c() (mnemonic):\n\ncases &lt;- c(2, 5, 8, 0, 4)\n\n\nUpdate cases with the above values and update region to create a string vector containing the values: Mandoul, Moyen-Chari, Logone Oriental, Tibesti, and Logone Occidental.\n\nWe can now use functions on the objects we have created:\n\nmean(cases)      # calculate the mean value of the cases vector\n\n[1] 3.8\n\ntoupper(region)  # convert all the values in region to upper case\n\n[1] \"MANDOUL\"           \"MOYEN-CHARI\"       \"LOGONE ORIENTAL\"  \n[4] \"TIBESTI\"           \"LOGONE OCCIDENTAL\"\n\n\n\nLet’s use some functions! Try to write code that does the following:\n\nCalculate the sum of cases using the function sum()\nConvert the text in region to lowercase using the function tolower()\n\n\n\n\n\nAccessing the Values of a Vector\nIt is possible to access the value of a vector using square brackets containing the index (position) of the desired value, ie: [3] or [189].\n\ncases[2]   # 2nd value of cases\n\n[1] 5\n\ncases[10]  # 10th value of cases\n\n[1] NA\n\n\nOoops it does not exist! We will come back to what this NA means in the Missing Values section.\nWe can also access a range of values, just as we might do in Excel. To create a range we use the : operator to separate the desired minimum and maximum index:\n\ncases[2:4]  # 2nd to 4th values of cases\n\n[1] 5 8 0\n\n\n\nGet the 3rd value of region.\nWrite code to access the values “Mandoul” and “Moyen-Chari” in the vector region.\n\n\n\nData frames\nData frames are tabular structures / 2D tables with rows and columns. It is very similar to a “table” structure in Excel. As epidemiologists, this type of data structure is perhaps the most useful and you will likely use them on a daily basis, to store linelist data for example.\n\nCreating a data frame\nWe can create a data frame using the function data.frame():\n\ndata.frame(col1 = c(1, 4, 2, 9),\n           col2 = c(\"a bit of text\", \"some more text\", \"hello\", \"epidemiologists!\"))\n\n  col1             col2\n1    1    a bit of text\n2    4   some more text\n3    2            hello\n4    9 epidemiologists!\n\n\nSee how col1 was created from a numeric vector, and col2 from a vector of strings. Here we chose the names of the columns (col1 and col2), which is the normal way, but you can run the code without to see how R handles names by default.\n\nIn your script, create a data frame called data_cases that contains cases in one column and region in the other.\n\n\n\nExploring a data frame\ndata_cases should now appear in your environment. You can click on the blue circle with a white triangle in to see some additional information, or click on its name to open the object in the same pane as the scripts to view it.\n\n\n\nThe data_case data frame now appears in the Environment pane\n\n\nThere are several handy functions we can use to explore a data frame:\n\nRun the following commands and try to determine what type of information they are returning.\n\nstr(data_cases)     # STRucture of the object\ndim(data_cases)     # DIMension of the object\nnrow(data_cases)    # Number of ROWs\nncol(data_cases)    # Number of COLumns\nnames(data_cases)   # column NAMES\n\n\nLet’s practice a bit more! R comes with several built in data sets that can be accessed directly, including one called iris. It is convenient today as we have not learned to import data in R yet (don’t worry, we will work on linelist data from the second session then onwards).\nWe can see the first few lines of this data frame using the function head():\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\nHow many rows and columns are there in iris? What are the names of its columns?\n\n\n\nAccessing Data in a data frame\nIn R, there are several methods for accessing the rows and/or columns of a data frame. In this introductory session we will focus on the [ ] syntax.\nWe use square brackets to access single values or ranges within our data frame. To do this we must give R both a row number (or range of rows) and column number/name (or range of columns), using the syntax [row, column].\n\ndata_cases[1, 2]          # the value of row one, column 2\n\n[1] \"Mandoul\"\n\ndata_cases[1, \"region\"]   # first value in the region column\n\n[1] \"Mandoul\"\n\n\nIf we want to access all of the rows (or columns) we can simple leave a space in the place of the number/name:\n\ndata_cases[1, ]           # values of all columns in row one\n\n  cases  region\n1     2 Mandoul\n\ndata_cases[2:4, ]         # values of all columns for rows 2 through 4\n\n  cases         region\n2     5       Sud Kivu\n3     8 Kasai oriental\n4     0          Kasai\n\ndata_cases[ , \"region\"]   # values of all rows for the region column\n\n[1] \"Mandoul\"        \"Sud Kivu\"       \"Kasai oriental\" \"Kasai\"         \n[5] \"Haut Katanga\"  \n\n\nWe can even select multiple non-consecutive indices by using a numeric vector:\n\ndata_cases[c(1, 3), ]  # lines 1 and 3 (all columns)\n\n  cases         region\n1     2        Mandoul\n3     8 Kasai oriental\n\n\nDo be careful, as the type of output returned when extracting data from a data frame can sometimes depend on the style of indexing used:\n\nstr(data_cases[1 , ])   # returns a data frame\n\n'data.frame':   1 obs. of  2 variables:\n $ cases : num 2\n $ region: chr \"Mandoul\"\n\nstr(data_cases[ , 1])   # returns a simple vector\n\n num [1:5] 2 5 8 0 4\n\n\nAnother syntaxt to extract the various columns of a data frame:\n\ndata_cases[2]           # returns the second column (as a data frame)\n\n          region\n1        Mandoul\n2       Sud Kivu\n3 Kasai oriental\n4          Kasai\n5   Haut Katanga\n\ndata_cases[\"region\"]    # returns the region column (as a data frame)\n\n          region\n1        Mandoul\n2       Sud Kivu\n3 Kasai oriental\n4          Kasai\n5   Haut Katanga\n\n\nNotice that these commands returned single-column data frames.\n\nWrite some code to:\n\nextract the third value in the region column of your data frame\n\nextract the second and third values of the cases column\n\ncalculate the sum of the cases column of your data frame"
  },
  {
    "objectID": "sessions_core/01_introduction.html#sec-missing-values",
    "href": "sessions_core/01_introduction.html#sec-missing-values",
    "title": "Introduction to R",
    "section": "Missing Values",
    "text": "Missing Values\nAs epidemiologists, we work with missing data all the time. In R, missing values are coded using a special value: NA (meaning Not Available). NA is somewhat unique in R as it doesn’t per se have a fixed type, rather, it will take on the type of the values around it. For example, an NA in a numeric column will then take on the numeric type. We will discuss the idea of missing data in more depth in later sessions of the course."
  },
  {
    "objectID": "sessions_core/01_introduction.html#sec-functions",
    "href": "sessions_core/01_introduction.html#sec-functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\nFunctions are objects that contain commands (instead of values) that are run whenever the function is called. You are without doubt familiar with functions in Excel such as SUM or MEAN and the idea of functions in R is exactly the same.\nMost functions require some sort of input, such as a dataset or parameter. These inputs are called arguments and are normally named. For example, when we ran sum(cases), we provided the vector cases as the first (and only) argument to the function sum().\nOften, a function will have a combination of both required and optional arguments. The first argument of a function is almost always required and is typically a dataset. As an obligatory and rather obvious argument, most people omit its name when calling a function; ie: i.e. people will write mean(cases) instead of mean(x = cases). Optional arguments on the other hand are usually added using their name, i.e.: mean(x = cases, na.rm = TRUE).\nOptional arguments typically have default values and we only include them when we want to change their defaults (and thus change the default behavior of the function). For example, the na.rm argument of mean() determines whether R will ignore missing values when calculating a mean. The default state of the na.rm argument is FALSE, so by default, the mean performed on data with missing values will always return NA as the result:\n\nmean(c(1, 3, NA))\n\n[1] NA\n\n\nThis is true for many arithmetic operations in R. If we want R to calculate the mean on whatever data is available (and ignore the missing values) we need to explicitly set na.rm = TRUE:\n\nmean(c(1, 3, NA), na.rm = TRUE)\n\n[1] 2\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that arguments are separated by commas. These commas should always be followed by a space and whenever a named argument is used the = should be surrounded by spaces:\n\nmean(cases,na.rm=TRUE)     # BAD\nmean(cases, na.rm = TRUE)  # GOOD\n\nAs you work with increasingly complex functions, you may start to have a lot of arguments. For readability, it is typically recommended to split each argument onto its own line:\n\nmean(cases, \n     na.rm = TRUE) \n\n\n\nWhat happens if we put the arguments in the wrong order? If you provided the name of the arguments in you command, the function will still work exactly as expected. That being said, doing this would make your code harder to read and we encourage you to stick with a standard order of putting obligatory arguments like data first.\n\n# technically functional but hard to read:\nmean(na.rm = TRUE,  \n     x = cases) \n\n# better:\nmean(cases,         \n     na.rm = TRUE)\n\nOf course, if you mess up the ordering of arguments and didn’t include their names your code will not work as expected, or even throw an error:\n\nmean(TRUE, cases)   # not what you expect"
  },
  {
    "objectID": "sessions_core/01_introduction.html#done",
    "href": "sessions_core/01_introduction.html#done",
    "title": "Introduction to R",
    "section": "Done!",
    "text": "Done!\nThat’s all for this session, congratulations on taking your first steps with R and RStudio!\n\n\n\n Solutions file"
  },
  {
    "objectID": "sessions_core/03_data_verbs.html",
    "href": "sessions_core/03_data_verbs.html",
    "title": "Data Manipulation, Basics",
    "section": "",
    "text": "Learn basic data verbs from {dplyr} to:\n\nSelect specific columns (select())\nRename columns (rename())\nAdd new columns and change existing ones (mutate())\nRemove duplicate observations\n\nUnderstand the pipe operator |&gt;"
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#objectives",
    "href": "sessions_core/03_data_verbs.html#objectives",
    "title": "Data Manipulation, Basics",
    "section": "",
    "text": "Learn basic data verbs from {dplyr} to:\n\nSelect specific columns (select())\nRename columns (rename())\nAdd new columns and change existing ones (mutate())\nRemove duplicate observations\n\nUnderstand the pipe operator |&gt;"
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#setup",
    "href": "sessions_core/03_data_verbs.html#setup",
    "title": "Data Manipulation, Basics",
    "section": "Setup",
    "text": "Setup\nDependencies. This session assumes that you know how to use RStudio and that you are able to import data. If you need a refresher on either of these topics, we encourage you to review the first two sessions in the learning pathway.\n\nThis session will work with the raw Moissala linelist data, which can be downloaded here:\n\n\n\n  Download Data\n\n\n\n Make sure this dataset is saved into the appropriate subdirectory of your R project and create a new script called data_verbs_practice.R in your R directory. Add an appropriate header and load the following packages: {here}, {rio}, and {tidyverse}.  Finally, add an import section where you use {here} and {rio} to load your data into an object called df_raw."
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#manipulating-data-with-dplyr",
    "href": "sessions_core/03_data_verbs.html#manipulating-data-with-dplyr",
    "title": "Data Manipulation, Basics",
    "section": "Manipulating Data with {dplyr}",
    "text": "Manipulating Data with {dplyr}\nNow that we know how to set up a project and import data, we can finally start to play around with it. Going forward we will be using several packages from the “tidyverse” to help us manipulate, summarize, and visualize our data. Today’s session will focus on data manipulation using a package called {dplyr}.\n\nWhat is {dplyr}\nData manipulation is the foundation of working with data in R and as such is foundational to the work we do as epidemiologists. In particular, data manipulation skills will be critical when trying to clean our data.\nIn R, the package {dplyr} provides a large number of functions to help us manipulate data frames and perform many of the tasks that we will need to use on a daily basis, for example:\n\nSubsetting our data to remove certain variables\nRenaming certain variables\nAdding or modifying a variable\nRemoving duplicate entries\n\nIn {dplyr} each of these actions can be done with a particular function, which typically have an intuitive verb for a name. For example, renaming columns will use the function rename().\nIn today’s session we will look at the “data manipulation verb”, ie the function, needed for each of the above tasks as well as how to chain them all together into an efficient data pipeline.\n\n\n\n\n\n\nNote\n\n\n\nYou may have noticed that we asked you to load a package called {tidyverse} rather than {dplyr} in the setup. Loading {tidyverse} will load several of the most useful packages from the broader tidyverse, including {dplyr} and a couple other packages that we will see later in the session."
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#basic-data-verbs",
    "href": "sessions_core/03_data_verbs.html#basic-data-verbs",
    "title": "Data Manipulation, Basics",
    "section": "Basic Data Verbs",
    "text": "Basic Data Verbs\n\nSelecting Specific Columns\nA lot of the time when we receive a dataset it will have extra columns that we don’t need, either because those columns contain sensitive data or because our analysis will only focus on a subset of the data. This is where a function like select() comes in handy.\nHere is the basic syntax, note that this is pseudo-code and isn’t something you are intended to run yourself.\n\n# DO NOT RUN (PSEUDO-CODE)\nselect(df_raw, first_column_to_keep, second_column_to_keep)\n\nHere we see that the first argument is our dataset and each subsequent argument is the name of a column that we would like to keep. In the tidyverse, variables (ie column names) don’t need to be put into quotation marks. So for example, if we want to select the columns id, sex, and age we can use the following:\n\nselect(df_raw, id, sex, age)\n\n\nUse select() to select the following variables in your dataset: id, sex, age, sub_prefecture, date_onset, and outcome. The head of your output should look something like this:\n\n\n  id   sex age date_onset   outcome\n1  1 femme  36 2022-08-13 recovered\n2  2     f   5 2022-08-18      &lt;NA&gt;\n3  3     f 156 2022-08-17 recovered\n4  6 homme   8 2022-08-22 recovered\n5  7     m   7 2022-08-30 recovered\n6 10     m   4 2022-08-30 recovered\n\n\n Take a look at this output and then at df_raw. We can see that df_raw still contains all of the columns, which is what we want. But can you tell why it didn’t change?\n\nOften, we want to keep most of the variables in our dataset and only remove one or two. We can use the above syntax to do this, but it can become pretty tedious to write out every column name. In these cases, instead of telling select what to **keep**, we can use a subtraction sign (-) to tell it what to **remove**. For example, if we wanted to remove thevillage_commune` column from our dataframe we can use the following:\n\nselect(df_raw, -village_commune)\n\nWay easier!\n\nUse the - syntax in select() to select all of the columns in df_raw except: full_name and age_unit from your dataset.\n\n\n\nRenaming Columns\nAnother common issue when we get new datasets is that the variable names are inconvenient. In those cases, rename() can work wonders. Here’s the basic syntax:\n\n# DO NOT RUN (PSEUDO CODE)\nrename(df,\n       new_column_name = old_column_name,\n       another_new_name = another_old_name)\n\nAs in the case of select(), and indeed in essentially all {dplyr} verbs, the first argument is our daframe. Then each subsequent argument is a statement of new_column_name = old_column_name telling R which columns to rename and the new names that we want to use, with each pair given its own line to improve readability. If we wanted to change village_commune to simply be village, for example, we can write:\n\nrename(df_raw,\n       village = village_commune)\n\n\nUse rename() on df_raw to change the columns sub_prefecture, village_commune, and health_facility_name to be prefecture, village, and facility respectively.\n\nIn the above exercise it may have been difficult to check if the output looked correct because R would have printed out the full data frame. In these cases it can be helpful to create a temporary object just to check if everything looks alright. You can call this object whatever you want, but a common name is tmp.\n\nRepeat the last exercise but this time assign the output to an object called tmp and use names() to check that the column names changed as you expected. The output of names() should give you something like this:\n\n\n [1] \"id\"                \"full_name\"         \"sex\"              \n [4] \"age\"               \"age_unit\"          \"region\"           \n [7] \"prefecture\"        \"village\"           \"date_onset\"       \n[10] \"date_consultation\" \"hospitalisation\"   \"date_admission\"   \n[13] \"facility\"          \"malaria_rdt\"       \"fever\"            \n[16] \"rash\"              \"cough\"             \"red_eye\"          \n[19] \"pneumonia\"         \"encephalitis\"      \"muac\"             \n[22] \"vacc_status\"       \"vacc_doses\"        \"outcome\"          \n[25] \"date_outcome\"     \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTemporary objects, like the tmp data frame you just created are just that: temporary. They are usually used to test if something has worked and designed to be overwritten each time you need to test something else. As such, you should not use these temporary objects as the input for other parts of your code. If you want to make a data frame that will be reused, such as a clean version of df_raw, this should be done using an object with a proper name like df or df_clean.\n\n\n\n\nChanging and Adding Columns\nSo now we know how to select and rename columns, but how do we modify them? This is where mutate() comes into play. This function can be used both to add new columns and to change existing ones.\nLet’s start with the basic mutate() syntax needed to add a new column:\n\n# DO NOT RUN (PSEUDO-CODE)\nmutate(df,\n       new_column = action(existing_column),\n       another_new_column = another_action(another_existing_column))\n\nIn the above code, we are creating a new column (new_column) by performing some sort of action (action()) on an existing column in the dataframe (existing_column). This action could be anything, it could use a function or be a simple arithmetic operation and can use one or more columns. For example, if we wanted to create a new column expressing MUAC in cm we could use the following:\n\nmutate(df_raw,\n       muac_cm = muac / 100)\n\n\nUse mutate() to create a new column called age_years that expresses age in years rather than months. The head of your new age_years column should look like this:\n\n\n   age_years\n1  3.0000000\n2  0.4166667\n3 13.0000000\n4  0.6666667\n5  0.5833333\n6  0.3333333\n\n\n\nGreat! But what if instead of creating a new column we instead wanted to change an existing one? You just need to use the existing column name on the left side of the = instead of giving a new column name. For example, in the above MUAC code we would write:\n\nmutate(df_raw,\n       muac = muac / 100)\n\nWe might want to keep age in months as well as years, so we won’t reassign that column. But there are some other columns that could stand to be changed. There are a lot of reasons we might want to change a column, two of the most common ones are:\n\nThe format of a string needs changing\nThe data type of a column is incorrect\n\nOur dataset has both of these problems. For example, while it isn’t per se a problem that region and sub_prefecture are in all capitals, it also isn’t particularly nice. To fix this, we can use another function from the {tidyverse}, this time from a package called {stringr} to make these columns title case:\n\nmutate(df_raw,\n       region = str_to_title(region),\n       sub_prefecture = str_to_title(sub_prefecture))\n\n\nUse mutate() to update the format of malaria_rdt and outcome to use title case. The head of these two columns should now look something like this:\n\n\n  malaria_rdt   outcome\n1    Negative Recovered\n2    Negative      &lt;NA&gt;\n3    Negative Recovered\n4    Negative Recovered\n5    Negative Recovered\n6    Negative Recovered\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that we didn’t need to load {stringr} to do the above exercise. That’s because, like {dplyr} this package is loaded when we load the {tidyverse}.\n\n\nThat’s nicer. Now let’s consider the second issue, having variables with the wrong type.\n\nTake a look at the data type of your columns, do any of them look strange?  Hint. str() may be useful here.\n\nMost of the columns look ok, but it seems theres something strange with the dates. Some of them are character type and others are something called POSIXct. We would much rather have all of these columns use the simple Date type.\nTo convert to dates, we are going to call on yet another package from the the tidyverse, {lubridate}. In particular, we are going to use the function ymd(). For example:\n\nmutate(df_raw,\n       date_outcome = ymd(date_outcome))\n\n\nUse mutate() and ymd() to modify date_onset and date_admission to be Date type. Use a temporary data frame tmp to check that your code is doing what you want it to.\n\n\n\nRemoving Duplicates\nOk great! We know how to select, rename, and modify our data. Another task we will often need to do is removing duplicate entries. Fortunately this one is easily done using the function distinct(), which has the following basic syntax:\n\n# DO NOT RUN (PSEUDO-CODE)\ndistinct(df)\n\nNotice that distinct only needs one argument by default, the dataset itself. This will look for and remove any duplicate observations in the data frame. There are some fancier ways of using distinct() that will look for duplication on certain variables only, but that’s outside of the scope of today’s session.\n\nUse distinct() to create a temporary data frame, tmp, that contains all the unique observations in df_raw. Compare the number of rows in tmp to that of df_raw. Did we have any duplicates?"
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#the-pipe-operator",
    "href": "sessions_core/03_data_verbs.html#the-pipe-operator",
    "title": "Data Manipulation, Basics",
    "section": "The Pipe Operator",
    "text": "The Pipe Operator\nSo it looks like we have actually done quite a bit of cleaning while learning the core {dplyr} verbs. We should probably try to put some of the above steps together to start building a basic data cleaning pipeline. So far we haven’t been saving any of our changes, except perhaps to a temporary data frame. It would be nice to keep them in a new clean df object.\nFor example, if we want to effect the column renaming we did above to a reusable object we might write something like this:\n\ndf &lt;- rename(df_raw, \n             prefecture = sub_prefecture,\n             village = village_commune,\n             facility = health_facility_name)\n\n\n\n\n\n\n\nTip\n\n\n\nIn general, it’s good practice to keep a raw version of your dataset, here df_raw, that remains unmodified in your code. This is so that you always have it available in your environment as a reference and is always available at the start of your cleaning pipeline to improve reproducibility.\n\n\nNow we have a new object, df that we can do more operations on. Brilliant. For example, if we now wanted to select everything except for full_name we could update the above code like this:\n\n# Step 1: Rename Variables\ndf &lt;- rename(df_raw, \n             prefecture = sub_prefecture,\n             village = village_commune,\n             facility = health_facility_name)\n\n# Step 2: Select Variables to Keep\ndf &lt;- select(df,\n             -full_name)\n\nNotice that in this second step we are using df as the input of select() rather than df_raw because we want to continue working on our modified version of the data. Let’s say now we want to add a column of age in years:\n\n# Step 1: Rename Variables\ndf &lt;- rename(df_raw, \n             prefecture = sub_prefecture,\n             village = village_commune,\n             facility = health_facility_name)\n\n# Step 2: Select Variables to Keep\ndf &lt;- select(df,\n             -full_name)\n\n# Step 3: Add Age in Years\ndf &lt;- mutate(df,\n             age_years = age / 12)\n\nHm, ok well this is working but it is starting to get repetitive. With each step we are reusing the output of the last step and then updating the same data frame, df. It would be better if these actions could be chained together in a simpler way.\nThis is exactly what the pipe operator, |&gt; is for! The pipe has the following basic syntax:\n\n# DO NOT RUN (PSEUDO-CODE)\ninput |&gt; action()\n\nHere the input on the left side (input) is “piped into” the action on the right side (action()). So for example instead of writing:\n\nselect(df_raw, id, sex)\n\nWe could instead write:\n\ndf_raw |&gt;\n  select(id, sex)\n\n\nTry out the above code to see if it works.\n\nThis can be used to chain multiple actions together and you will often see tidyverse style code that uses pipes in the following way:\n\n# DO NOT RUN (PSEUDO-CODE)\ndf &lt;- df_raw |&gt;\n  first_action() |&gt;\n  second_action() |&gt;\n  third_action()\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that here each action in the pipe is given its own line. This is considered good practice as it makes your code easier to read and understand.\n\n\nSo, if we wanted to chain the example actions we saw above into a single pipe, we might write something like this:\n\ndf &lt;- df_raw |&gt;\n  rename(prefecture = sub_prefecture,\n         village = village_commune,\n         facility = health_facility_name) |&gt;\n  select(-full_name) |&gt;\n  mutate(age_years = age / 12)\n\nThat’s a lot easier than reassigning df after each step!\n\nLet’s see if we can put together what we learned above into a single pipeline! Use the pipe operator |&gt;, select(), rename(), mutate(), str_to_title(), ymd(), and distinct() to perform the following actions on df_raw and assign the output to a new data frame called df:\n\nRemove the variables full_name and age_unit\nRename the following variables:\n\nage becomes age_months\nsub_prefecture becomes prefecture\nvillage_commune becomes village\nhealth_facility_name becomes facility\n\nAdd a variable age_years with patient age in years\nUpdate region and prefecture to use title case\nUpdate all date columns to use Date type\nRemove any duplicate observations\n\nThe head of your final data should look something like this:\n\n\n  id   sex age_months  region prefecture        village date_onset\n1  1 femme         36 Mandoul   Moissala Sangana Koïtan 2022-08-13\n2  2     f          5 Mandoul   Moissala      Mousdan 1 2022-08-18\n3  3     f        156 Mandoul   Moissala     Djaroua Ii 2022-08-17\n4  6 homme          8 Mandoul   Moissala     Monakoumba 2022-08-22\n5  7     m          7 Mandoul   Moissala      Tétindaya 2022-08-30\n6 10     m          4 Mandoul   Moissala      Danamadja 2022-08-30\n  date_consultation hospitalisation date_admission\n1        2022-08-14             yes     2022-08-14\n2        2022-08-25             yes     2022-08-25\n3        2022-08-20            &lt;NA&gt;           &lt;NA&gt;\n4        2022-08-25              no           &lt;NA&gt;\n5        2022-09-02              no           &lt;NA&gt;\n6        2022-09-02             yes     2022-09-02\n                         facility malaria_rdt fever rash cough red_eye\n1 Hôpital du District de Moissala    negative    No &lt;NA&gt;   Yes      No\n2 Hôpital du District de Moissala    negative    No   No   Yes      No\n3                      CS Silambi    negative   Yes &lt;NA&gt;    No      No\n4 Hôpital du District de Moissala    negative    No   No    No    &lt;NA&gt;\n5                      CS Silambi    negative  &lt;NA&gt;   No   Yes     Yes\n6                    Moissala Est    negative   Yes   No    No    &lt;NA&gt;\n  pneumonia encephalitis muac vacc_status vacc_doses   outcome date_outcome\n1        No           No  244        &lt;NA&gt;       &lt;NA&gt; recovered   2022-08-18\n2      &lt;NA&gt;           No  232          No       &lt;NA&gt;      &lt;NA&gt;   2022-08-28\n3        No         &lt;NA&gt;  123  Yes - oral       &lt;NA&gt; recovered         &lt;NA&gt;\n4        No           No  210          No       &lt;NA&gt; recovered         &lt;NA&gt;\n5        No           No   80          No       &lt;NA&gt; recovered         &lt;NA&gt;\n6        No           No  220          No       &lt;NA&gt; recovered   2022-09-03\n   age_years\n1  3.0000000\n2  0.4166667\n3 13.0000000\n4  0.6666667\n5  0.5833333\n6  0.3333333\n\n\nHint. Be careful with your column names here! If you renamed something you will need to use the new names for any subsequent parts of the pipe.\n\nAmazing! That looks like a great start at a data cleaning pipeline. Keep this code handy, you will use it in the next session where we will look at another common part of data cleaning: recoding."
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#done",
    "href": "sessions_core/03_data_verbs.html#done",
    "title": "Data Manipulation, Basics",
    "section": "Done!",
    "text": "Done!\nWell done, you’ve learned the fundamentals of data manipulation and how to string multiple commands together into a data manipulation pipe. Moving forward, solution files will focus less on being “exercise by exercise” and rather provide an example of what a real script might look like in a real world context. In this case, the solutions will then focus only on the final pipe that is created at the end of the session.\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#going-further",
    "href": "sessions_core/03_data_verbs.html#going-further",
    "title": "Data Manipulation, Basics",
    "section": "Going Further",
    "text": "Going Further\n\nExtra Exercises\n\nA a line to your mutate() to update the hospitalisation variable so that its text would be in title case as well.\nPerhaps you would prefer to use lower case for the region column rather than the title case, update your code to do this instead. Hint: you might want to check out str_to_lower() from {stringr}.\nCreate a delay_consultation column, that contains the number of days between the onset of symptoms and the consultation."
  },
  {
    "objectID": "explore.html",
    "href": "explore.html",
    "title": "Explore",
    "section": "",
    "text": "Choose your own adventure by browsing all available sessions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Data Visualization\n\n\n\nCore\n\n\nVisualization\n\n\n\nLearn the basics of buidling plots with ggplot2, and create your first epicurve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourbes épidémiques hebdomadaires\n\n\n\nSatellite\n\n\nVisualization\n\n\n\nApprenez à tracer des courbes épidémiques hebdomadaires et à améliorer les étiquettes des axes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Exploration\n\n\n\nSatellite\n\n\nData Exploration\n\n\n\nExplore your data after importation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Importation\n\n\n\nCore\n\n\nRStudio\n\n\nData Import\n\n\n\nCreate an Rstudio project, install useful packages and start importing data to work in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Basics\n\n\n\nCore\n\n\nData Manipulation\n\n\nData Cleaning\n\n\n\nAn introduction to data manipulation and cleaning using {dplyr}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Filtering and Recoding\n\n\n\nCore\n\n\nData Manipulation\n\n\nData Cleaning\n\n\nLogic\n\n\n\nUsing {dplyr} and conditional logic to filter and recode data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnquête standardisée sur la mortalité\n\n\n\nCompagnon\n\n\nAnalyse\n\n\n\nSession complémentaire au module FETCH d’enquête\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploration des données\n\n\n\nSatellite\n\n\nData Exploration\n\n\n\nExplorez vos données après l’importation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFaceting\n\n\n\nSatellite\n\n\nVisualization\n\n\n\nCreate a plot with multiple subplots (facets)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraphiques multiples (facetting)\n\n\n\nSatellite\n\n\nVisualization\n\n\n\nApprenez à créer plusieurs mini graphiques “par catégorie” en une seule commande\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportation des données\n\n\n\nCore\n\n\nRStudio\n\n\nData Import\n\n\n\nCréez un projet Rstudio, installez les paquets utiles et importez des données pour travailler dans R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R\n\n\n\nCore\n\n\nR Basics\n\n\nData Types\n\n\n\nYour first steps in R. Learn your way around Rstudio, and meet some common R objects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to data visualization with ggplot2\n\n\n\nCore\n\n\nVisualization\n\n\n\nApprenez les bases de la visualisation avec ggplot2, et créez votre première épicurve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction à R\n\n\n\nCore\n\n\nR Basics\n\n\nData Types\n\n\n\nVos premiers pas dans R. Familiarisez-vous avec Rstudio et avec les objets courants de R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Mortality Survey\n\n\n\nCompanion\n\n\nAnalysis\n\n\n\nCompanion session to the survey FETCH module\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Tables\n\n\n\nCore\n\n\nSummary Tables\n\n\n\nCreate summary tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurveillance\n\n\n\nCompagnon\n\n\nAnalyse\n\n\n\nTutoriel d’accompagnement au module Surveillance du FETCH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurveillance\n\n\n\nCompanion\n\n\nAnalysis\n\n\n\nCompanion session to the surveillance Fetch-R module\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTableaux récapitulatifs\n\n\n\nCore\n\n\nTableaux de resumé\n\n\n\nCréer des tableaux récapitulatifs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraitement de données, les bases\n\n\n\nCore\n\n\nManipulation des données\n\n\nNettoyage des données\n\n\n\nUne introduction à la manipulation et au nettoyage des données à l’aide du paquet {dplyr}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraitement de données, recoder et filtrer\n\n\n\nCore\n\n\nManipulation des données\n\n\nNettoyage des données\n\n\nLogique\n\n\n\nApprenez à recoder vos variables avec {dplyr} et comment sélectionner les lignes d’un data frame suivant certains critères\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Epicurves\n\n\n\nSatellite\n\n\nVisualization\n\n\n\nPlot weekly epicurves and improve date labels on the x-axis\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pathway.html",
    "href": "pathway.html",
    "title": "Pathway",
    "section": "",
    "text": "These sessions can be followed in order to get a baseline level in R. The series assumes no prior experience in R and is suitable for beginners.\nLooking for more? Want more flexibility? Consider browsing the full session catalog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R\n\n\n\n\n\nYour first steps in R. Learn your way around Rstudio, and meet some common R objects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Importation\n\n\n\n\n\nCreate an Rstudio project, install useful packages and start importing data to work in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Basics\n\n\n\n\n\nAn introduction to data manipulation and cleaning using {dplyr}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Filtering and Recoding\n\n\n\n\n\nUsing {dplyr} and conditional logic to filter and recode data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Tables\n\n\n\n\n\nCreate summary tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Data Visualization\n\n\n\n\n\nLearn the basics of buidling plots with ggplot2, and create your first epicurve\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sessions_extra/faceting.html",
    "href": "sessions_extra/faceting.html",
    "title": "Faceting",
    "section": "",
    "text": "Learn the syntax to make subplots real quick in {ggplot2}\nLearn arguments to modify the appearance of the subplots"
  },
  {
    "objectID": "sessions_extra/faceting.html#objectives",
    "href": "sessions_extra/faceting.html#objectives",
    "title": "Faceting",
    "section": "",
    "text": "Learn the syntax to make subplots real quick in {ggplot2}\nLearn arguments to modify the appearance of the subplots"
  },
  {
    "objectID": "sessions_extra/faceting.html#introduction",
    "href": "sessions_extra/faceting.html#introduction",
    "title": "Faceting",
    "section": "Introduction",
    "text": "Introduction\nThis satellite builds on the core epicurve session, which is a prerequisite. In that session, we learned how to create an epicurve of measles cases across time:\n\n\n\n\n\n\n\n\n\nNow, this plot is cool, but in your sitrep you would like to show the data by age group. There are several ways to do that:\n\nYou could, for each age group, filter your data frame and copy and paste the plotting command to create specific plots\nYou could learn to use for loops or apply() or map() family functions, which are very useful ways to automatize actions, and involve less copy and pasting\nOr you could trust {ggplot2} to have a solution…\n\nThe first option is tedious and it is error prone, and we advise against it; learning the tools of the second option will be a good investment of you time at some point as they are really powerful, but they are way out of the scope of this tutorial because a much simpler option already exist in {ggplot2}."
  },
  {
    "objectID": "sessions_extra/faceting.html#setup",
    "href": "sessions_extra/faceting.html#setup",
    "title": "Faceting",
    "section": "Setup",
    "text": "Setup\n\nWe will use the same clean linelist that we used in the past sessions, which you can download here:\n\n\n\n Download clean data\n\n\n\n Make sure this dataset is saved into the appropriate subdirectory of your R project and create a new script called faceting.R in your R directory. Add an appropriate header and load the following packages: {here}, {rio}, and {tidyverse}.  Finally, add an import section where you use {here} and {rio} to load your data into an object called df_linelist."
  },
  {
    "objectID": "sessions_extra/faceting.html#faceting",
    "href": "sessions_extra/faceting.html#faceting",
    "title": "Faceting",
    "section": "Faceting",
    "text": "Faceting\nThe function facet_wrap() allows you to replicate a graph based on the categories of a variable. For example, you could make the epicurve graph by sex, or by site. As other layers of a ggplot graph, you add it to your existing graph with a +. It creates a a figure with multiple small graphs, that {ggplot2} calls facets or small multiples.\n\nGet the Data Ready\nIn the following session, we will explain the code by creating subplots by sub-prefecture, and you will be plotting the epicurve by age group.\nIf we want to to plot anything by sub-prefecture, the sub_prefecture variable must be present in the aggregated data frame that we use to plot.\nLet’s create a new summarized dataset that has the number of patients by day and by sub-prefecture!\n\ndf_pref &lt;- df_linelist %&gt;%\n  count(date_onset, sub_prefecture,\n        name = \"patients\")\n\n\n\n  date_onset sub_prefecture patients\n1 2022-08-13       Moissala        1\n2 2022-08-17       Moissala        1\n3 2022-08-18       Moissala        1\n4 2022-08-22       Moissala        1\n5 2022-08-30       Moissala        2\n6 2022-09-01       Moissala        1\n\n\n\nYou will draw a plot of the number of admissions by age group, so you need a new data frame summarized by day and age group. Create this data frame, and call it df_age. It should have this format:\n\n\n  date_onset    age_group n\n1 2022-08-13  1 - 4 years 1\n2 2022-08-17 5 - 14 years 1\n3 2022-08-18   &lt; 6 months 1\n4 2022-08-22 6 - 8 months 1\n5 2022-08-30   &lt; 6 months 1\n6 2022-08-30 6 - 8 months 1\n\n\n\n\n\nAdd the Facet Layer to the Plot\nNow, let’s plot this data. Look at the code bellow: it is exactly the same as before but for the last line, which creates the facets:\n\ndf_pref %&gt;%\n  ggplot(aes(x = date_onset,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of onset\",\n       y = \"Measles cases\",\n       title = \"Measles cases in Mandoul (Chad)\") +\n  theme_classic(base_size = 15) +\n  facet_wrap(vars(sub_prefecture))   # Make the plot by sub-prefecture!\n\n\n\n\n\n\n\n\nIsn’t that incredible? As you can see, the function facer_wrap() takes as argument a variable name wrapped in the vars() function.\n\nNow is your turn, draw the epicurve by age group (still keeping all the plots improvement: labels, themes etc.)\nIt should look like this:"
  },
  {
    "objectID": "sessions_extra/faceting.html#customize-facets",
    "href": "sessions_extra/faceting.html#customize-facets",
    "title": "Faceting",
    "section": "Customize Facets",
    "text": "Customize Facets\nCheck out the function help page to learn about the arguments that facet_wrap() accepts. We will cover a couple here.\n\nNumber of Rows or Columns\nThe arguments nrow and ncol allow you to decide how many facets there should be on one row, respectively one column.\nFor exemple, we could have all plots on two rows, for a wide figure:\n\ndf_pref %&gt;%\n  ggplot(aes(x = date_onset,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of onset\",\n       y = \"Measles cases\",\n        title = \"Measles cases in Mandoul (Chad)\") +\n  theme_classic(base_size = 15) +\n  \n  facet_wrap(vars(sub_prefecture),\n             nrow = 2)  \n\n\n\n\n\n\n\n\nOr force the number of rows to four, which forces a taller figure:\n\ndf_pref %&gt;%\n  ggplot(aes(x = date_onset,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of onset\",\n       y = \"Measles cases\",\n       title = \"Measles cases in Mandoul (Chad)\") +\n  theme_classic(base_size = 15) +\n  \n  facet_wrap(vars(sub_prefecture),\n             nrow = 4)  \n\n\n\n\n\n\n\n\n\nUsing one of the mentioned argument, create a graph with three columns.\n\n\n\nAxis Ranges\nDid you notice that in the graph we produced, all facets share the same axis in x and y? This is often a desired feature, as playing with axes is one of the best ways to mislead readers.\nThat being said, if you are more interesting in seeing the shape of the epicurve by category and less by comparing categories to each other, zooming on the available data can be appropriate (alert your reader to the scale variation though!)\nThe scales argument accepts the following strings:\n\n\"fixed\": the default, same limits on x and y for all facets\n\"free_x\": the x axis may have different limits in different facets\n\"free_y\": the y axis may have different limits in different facets\n\"free\": both axis may vary in different facets\n\nLook at this graph:\n\n\n\n\n\n\n\n\n\nWe kept time window on the x axis fixed but allowed the y axis to vary to better read the number of cases by sub-prefecture.\n\nYour turn! Draw you graph with age group as facets with a free y axis, and a fixed x axis."
  },
  {
    "objectID": "sessions_extra/faceting.html#done",
    "href": "sessions_extra/faceting.html#done",
    "title": "Faceting",
    "section": "Done!",
    "text": "Done!\nVery well done team! You have learned how to facet plots! This will work not only on bar plots such as epicurves, but also on other types of plots made by {ggplot2}.\nDepending on the size of your graph, the date labels on the x-axis may be a bit messed up, the ones in my examples definitely are. Fear not, this can be controlled and is the object of another satellite!\n\n\n\n Solutions file"
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html",
    "href": "sessions_extra/weekly_epicurves.html",
    "title": "Weekly Epicurves",
    "section": "",
    "text": "In the epicurve session you learned how to plot an epicurve of the number of cases per day:\n\n\n\n\n\n\n\n\n\nThis graph is aggregated by day, which is a reasonable level of aggregation if the outbreak is short or if you wish to zoom on a short period. As epidemiologists we however often want to plot data by week.\nIn this tutorial we will learn two ways of aggregating data by week, plot the data and tweak the date labels."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#objectives",
    "href": "sessions_extra/weekly_epicurves.html#objectives",
    "title": "Weekly Epicurves",
    "section": "",
    "text": "In the epicurve session you learned how to plot an epicurve of the number of cases per day:\n\n\n\n\n\n\n\n\n\nThis graph is aggregated by day, which is a reasonable level of aggregation if the outbreak is short or if you wish to zoom on a short period. As epidemiologists we however often want to plot data by week.\nIn this tutorial we will learn two ways of aggregating data by week, plot the data and tweak the date labels."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#setup",
    "href": "sessions_extra/weekly_epicurves.html#setup",
    "title": "Weekly Epicurves",
    "section": "Setup",
    "text": "Setup\nWe will build on code from the epicurve session so you may either write your code in the script associated with that session or create a new script.\n\nCreate a new script for this tutorial or open the script from the epicurve lesson.\n Make sure the following packages are installed and loaded:\n\n{here} to write robust absolute paths,\n{rio} to import the data,\n{dplyr} to manipulate data,\n{ggplot2} to create the graphs,\n{lubridate} to manage dates and times\n{scales} to create prettier labels\n\nIf it is not already done, import the clean data (moissala_linelist_clean_EN.rds) into a df_linelist data frame and create a new section in your script called PREPARE DATA.\n\nAs we did in the core session, the examples in this lesson will be shown for outcomes and you will code the classic epicurve for date of onset in the exercises."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#aggregate-data-by-week",
    "href": "sessions_extra/weekly_epicurves.html#aggregate-data-by-week",
    "title": "Weekly Epicurves",
    "section": "Aggregate Data by Week",
    "text": "Aggregate Data by Week\nWe will to discuss two ways of aggregating data by weeks. You may be more familiar with the first one (using week numbers to identify weeks), but we will to focus more heavily on a more robust way (using the firs day of the week to identify weeks).\n\nUsing Week Numbers\nProbably the most intuitive way of thinking of weekly aggregated data is to think in terms of week numbers, as aggregated data from MoH are often in this format, and you probably created a lot of epicurves with week numbers yourselves.\nTheisoweek() from the {lubridate} packages takes a date (or a vector of dates) and returns the associated ISO week.\n\nexample_date &lt;- as.Date('2025-02-24')\n\nexample_date\n\n[1] \"2025-02-24\"\n\nisoweek(example_date)\n\n[1] 9\n\n\nWe can use this function to create a week_outcome_number in our data frame:\n\ndf_linelist &lt;- df_linelist |&gt; \n  mutate(week_outcome_number = isoweek(date_outcome))\n\nThe head of the date_outcome and week_outcome_number columns looks like this:\n\n\n  date_outcome week_outcome_number\n1   2022-08-18                  33\n2   2022-08-28                  34\n3   2022-09-03                  35\n4   2022-09-12                  37\n5   2022-09-10                  36\n6   2022-09-18                  37\n\n\n\nYour turn. Use the mutate() and isoweek() functions to create a new column in your data frame called week_onset_number that contains the ISO week associated with every onset date. The head of date_onset and week_onset_number columns should look like this:\n\n\n  date_onset week_onset_number\n1 2022-08-13                32\n2 2022-08-18                33\n3 2022-08-17                33\n4 2022-08-22                34\n5 2022-08-30                35\n6 2022-08-30                35\n\n\n\nNow, you could use this column to aggregate data by week using count() and then plot the weekly aggregated data using {ggplot2} with a code very similar to what we saw in the core epicurve session.\nThere is a problem, though. With isoweek() there is a first week in 2022, but also in 2023, 2024 and so on. With a short outbreak that would be only in 2022, this would be fine. However, our data frame gathers data at the whole region scale, and the dates range from 2022 to 2023. So if we were to just count the number of patients by week number, this table would be wrong:\n\n# WRONG\ndf_linelist |&gt; \n  count(week_onset_number) |&gt; \n  head(10)\n\n   week_onset_number  n\n1                  1 36\n2                  2 35\n3                  3 42\n4                  4 56\n5                  5 70\n6                  6 78\n7                  7 85\n8                  8 49\n9                  9 62\n10                10 81\n\n\nInstead, we could count by week stratified by years:\n\ndf_linelist |&gt; \n  mutate(year_onset = isoyear(date_onset)) |&gt; \n  count(year_onset, week_onset_number) |&gt; \n  head(10)\n\n   year_onset week_onset_number  n\n1        2022                32  1\n2        2022                33  2\n3        2022                34  1\n4        2022                35  8\n5        2022                36  8\n6        2022                37 10\n7        2022                38 17\n8        2022                39 17\n9        2022                40 19\n10       2022                41 16\n\n\nThese counts are perfectly correct. You could plot them using faceting by year, or just filter a given year and plot the weekly numbers with the ISO week number on the x-axis. For example:\n\ndf_linelist |&gt; \n  mutate(year_onset = isoyear(date_onset)) |&gt; \n  count(year_onset, week_onset_number) |&gt; \n  ggplot(aes(x = week_onset_number,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  theme_classic(base_size = 16) +\n  facet_wrap(vars(year_onset),  # Magic to make subplots very easily\n             ncol = 1)\n\n\n\n\n\n\n\n\nIf you have not read about facetting yet, do no worry, think of this plot as a teaser of how easily you can make subplots by a variable! But this is out of the scope of this tutorial. Instead, we will show you another way of aggregating data by week which is robust to multi-year data.\n\n\nUsing the First Day of the Week\nAn alternative way of aggregating by week is to use the function floor_date() (also from the {lubridate} package), which returns the first date of a given period. You can think of it as a sort of rounding to the smallest value, but for dates.\nThe function has a unit argument that allows you to choose the period of interest (week, month…) and a week_start argument where you can pass the first day of the week (Mondays are 1).\n\ndf_linelist &lt;- df_linelist |&gt; \n  mutate(\n    week_outcome_monday = floor_date(date_outcome,\n                                     unit = \"week\",\n                                     week_start = 1)\n  )\n\nLet’s look at all these different time variables to figure out what’s happening:\n\ndf_linelist |&gt; \n  select(id, date_outcome, week_outcome_number, week_outcome_monday) |&gt;\n  arrange(date_outcome) |&gt;     # Sort the data by date\n  head(n = 10)\n\n   id date_outcome week_outcome_number week_outcome_monday\n1   1   2022-08-18                  33          2022-08-15\n2   2   2022-08-28                  34          2022-08-22\n3  10   2022-09-03                  35          2022-08-29\n4  16   2022-09-10                  36          2022-09-05\n5  22   2022-09-12                  37          2022-09-12\n6  14   2022-09-12                  37          2022-09-12\n7  41   2022-09-16                  37          2022-09-12\n8  20   2022-09-17                  37          2022-09-12\n9  17   2022-09-18                  37          2022-09-12\n10 23   2022-09-19                  38          2022-09-19\n\n\nIt might be easier to visualize if we calculate the day of the week associated with each date using the function wday() (which also belong to the {lubridate} package, are you maybe seeing a pattern here 😉):\n\ndf_linelist |&gt; \n  # Get the name of the day for several date variables, to understand a bit better\n  mutate(\n    day_outcome = wday(date_outcome, \n                       label = TRUE, \n                       abbr = FALSE),\n    they_are_mondays   = wday(week_outcome_monday, \n                           label = TRUE, \n                           abbr = FALSE)) |&gt; \n  arrange(date_outcome) |&gt;     # Sort the data by date\n  select(date_outcome,\n         day_outcome,\n         week_outcome_number,\n         week_outcome_monday,\n         they_are_mondays) |&gt; \n  head(n = 10)\n\n   date_outcome day_outcome week_outcome_number week_outcome_monday\n1    2022-08-18    Thursday                  33          2022-08-15\n2    2022-08-28      Sunday                  34          2022-08-22\n3    2022-09-03    Saturday                  35          2022-08-29\n4    2022-09-10    Saturday                  36          2022-09-05\n5    2022-09-12      Monday                  37          2022-09-12\n6    2022-09-12      Monday                  37          2022-09-12\n7    2022-09-16      Friday                  37          2022-09-12\n8    2022-09-17    Saturday                  37          2022-09-12\n9    2022-09-18      Sunday                  37          2022-09-12\n10   2022-09-19      Monday                  38          2022-09-19\n   they_are_mondays\n1            Monday\n2            Monday\n3            Monday\n4            Monday\n5            Monday\n6            Monday\n7            Monday\n8            Monday\n9            Monday\n10           Monday\n\n\nThis illustrates how week_outcome_number and week_outcome_monday are two ways to have only one value representing a week. While week numbers are not unique as discussed before, dates are!\n\nAdd a new command to your mutate() call and create the variable week_onset_monday that contains the first day of the week for patient date of onset. Choose your argument as if the first day of the week is a Monday.\n\n\n\n\n\n\n\nTip\n\n\n\nGo read the help page for floor_date() to check out the list of possible units.\n\n\n\n\nActually Count Things\nNow that we have variables that represent week, it’s time to do the actual aggregation, ie count things!\n\nCount the number of patients per week of of onset, using the week start (week_onset_monday).\nHere are the first ten lines of what it should look like:\n\n\n   week_onset_monday  n\n1         2022-08-08  1\n2         2022-08-15  2\n3         2022-08-22  1\n4         2022-08-29  8\n5         2022-09-05  8\n6         2022-09-12 10\n7         2022-09-19 17\n8         2022-09-26 17\n9         2022-10-03 19\n10        2022-10-10 16"
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#draw-the-epicurve",
    "href": "sessions_extra/weekly_epicurves.html#draw-the-epicurve",
    "title": "Weekly Epicurves",
    "section": "Draw the Epicurve",
    "text": "Draw the Epicurve\nSo far so good, now we can pipe that aggregated data frame into our plot commands, making a couple adjustments to make it work.\n\nCreate a ggplot with the same look at the epicurve from the epicurve core session, but with the first day of the week on the x-axis. Don’t forget to update axes names.\n\nIt should look like that:\n\n\n\n\n\n\n\n\n\nWe see dates on the x-axis, but a bar represent data for a week starting on Monday."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#improve-the-axis",
    "href": "sessions_extra/weekly_epicurves.html#improve-the-axis",
    "title": "Weekly Epicurves",
    "section": "Improve the Axis",
    "text": "Improve the Axis\nNow, let’s learn how to tweak the appearance of that date axis!\n{ggplot2} automatically provided labels for the x-axis, trying to adjust for the range of data. That default may not always please us so we may want to manually force the labels to be more or less frequent, or change their format.\nTo modify the appearance of the axis, we will use another {ggplot2} function, from the scale family: scale_x_date().\n\nModify Breaks\nThe breaks controls the frequency of ticks on the axis.\nThe scale_x_date() function has a date_breaks argument that accepts the interval between two labels in a string. The string can have the following format: \"1 week\", \"2 weeks\", \"4 months\", \"2 years\" etc.\n\ndf_linelist |&gt; \n  count(week_outcome_monday) |&gt; \n  ggplot(aes(x = week_outcome_monday,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of outcome\",\n       y = \"Measles cases\",\n       title = \"Measles outcomes in Mandoul region (Chad)\") +\n  scale_x_date(date_breaks = \"4 months\") +  # Define breaks\n  theme_classic(base_size = 16)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\nYour turn! Modify your code so that the x-axis displays labels at reasonable intervals on your screen.\n\n\n\nImprove Labels\nNow that we changed the interval between ticks, let’s improve the labels themselves (the way dates are displayed on the axis). By default the labels are in the form year-month-day. We are going to show you two ways to change that.\n\nWith the {scales} Package.With the strptime Syntax\n\n\nThe scale_x_date() function has a label argument, that accepts several entries, among which a vector containing the dates, but also a function that generates labels from the breaks. The {scales} package provides such a function, label_date_short(), that attempts to create efficient and short labels for dates.\n\ndf_linelist |&gt; \n  count(week_outcome_monday) |&gt; \n  ggplot(aes(x = week_outcome_monday,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of outcome\",\n       y = \"Measles outcomes\",\n       title = \"Measles outcomes in Mandoul region (Chad)\") +\n  scale_x_date(date_breaks = \"2 months\",\n               labels = scales::label_date_short()) + # Short labels\n  theme_classic(base_size = 16)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\nModify your code and use label_date_short() to generate labels.\n\n\n\nIf you prefer to have full control on how to format dates, R has a syntax to describe date and time formats. There is a long help page with all the syntax items accessible with the help(strptime) command, but here are a few of the most useful elements to format a date label:\nDay:\n\n%d: from 01 to 31\n%e: from 1 to 31\n\nMonth:\n\n%b: abbreviated month name (current locale on your computer)\n%B: full month name (current locale on your computer)\n%m: month as a decimal number\n\nYear:\n\n%y: Year without the century (two digits)\n%Y: year in four digits\n\nSpecial separators:\n\n%n: newline\n%t: tab\n\nYou can assemble these items in a string, that you pass to different functions that accept a format as argument. Here we will pass it to the format() function to quickly see what display it creates, but after that we will use them in our graph command.\n\n# Create a date vector to explore different formats\nsome_dates &lt;- as.Date(c(\"2024-10-06\", \"2024-12-15\", \"2025-01-20\"))\n\n# Let's try out different syntax\nformat(some_dates, \"%Y-%b-%d\")\n\n[1] \"2024-Oct-06\" \"2024-Dec-15\" \"2025-Jan-20\"\n\nformat(some_dates, \"%Y-%b\")\n\n[1] \"2024-Oct\" \"2024-Dec\" \"2025-Jan\"\n\nformat(some_dates, \"%Y %B %d\")\n\n[1] \"2024 October 06\"  \"2024 December 15\" \"2025 January 20\" \n\nformat(some_dates, \"%y/%m/%d\")\n\n[1] \"24/10/06\" \"24/12/15\" \"25/01/20\"\n\nformat(some_dates, \"%d/%m/%Y\")\n\n[1] \"06/10/2024\" \"15/12/2024\" \"20/01/2025\"\n\n\nBack to the graph! The scale_x_date() function has an argument date_labels that accepts a string of text in the above format for the date labels.\n\ndf_linelist |&gt; \n  count(week_outcome_monday) |&gt; \n  ggplot(aes(x = week_outcome_monday,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of outcome\",\n       y = \"Measles cases\",\n       title = \"Measles outcomes in Mandoul region (Chad)\") +\n  scale_x_date(\n    date_breaks = \"2 months\",      # Define intervals betw. labels\n    date_labels = \"%Y%n%b%n%d\") +  # Define format of labels\n  theme_classic(base_size = 16)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\nModify the code of your graph to have labels look like this:"
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#done",
    "href": "sessions_extra/weekly_epicurves.html#done",
    "title": "Weekly Epicurves",
    "section": "Done!",
    "text": "Done!\nCongratulations! Dates are complicated, and their formatting is often scary, but we hope this little introduction showed you some nice tricks for your epicurves!\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#going-further",
    "href": "sessions_extra/weekly_epicurves.html#going-further",
    "title": "Weekly Epicurves",
    "section": "Going Further",
    "text": "Going Further\n\nExtra execices\n\nUse date format like this: “2024-oct.”, “2024-dec.”\nCreate an epicurve with the date of consultation, using the first day of the week on the x-axis (format dates the way you prefer)\nCreate an epicurve for 2023 data using the date of hospital admission and the ISO week number on the x-axis.\n\n\n\nChallenge\n\nDo the epicurve for the date of onset, but instead of aggregating by week, aggregate it by month. Find an appropriate format for the labels."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#resources",
    "href": "sessions_extra/weekly_epicurves.html#resources",
    "title": "Weekly Epicurves",
    "section": "Resources",
    "text": "Resources\n\nChapter of the Elegant graphics for data analyses book on date scales\nGet started with lubridate from the package homepage."
  },
  {
    "objectID": "sessions_core/02_import_data.fr.html",
    "href": "sessions_core/02_import_data.fr.html",
    "title": "Importation des données",
    "section": "",
    "text": "Créer un projet RStudio\nMettre en place un code organisé et bien documenté\nInstaller et charger des paquets dans la session\nEcrire des chemins d’accès aux fichiers robustes\nImporter et inspecter des données dans R\n\n\n\n\n\n\n\nImportant\n\n\n\nLes principes vus dans le module FETCH sur la gestion des données s’appliquent aussi à votre code : on souhaite écrire un script qui fonctionne maintenant, mais également dans le futur, et qui soit partageable. Il existe quelques bonnes pratiques qui peuvent nous aider à aller dans cette direction, et la première est d’avoir un code source propre et bien organisé."
  },
  {
    "objectID": "sessions_core/02_import_data.fr.html#objectifs",
    "href": "sessions_core/02_import_data.fr.html#objectifs",
    "title": "Importation des données",
    "section": "",
    "text": "Créer un projet RStudio\nMettre en place un code organisé et bien documenté\nInstaller et charger des paquets dans la session\nEcrire des chemins d’accès aux fichiers robustes\nImporter et inspecter des données dans R\n\n\n\n\n\n\n\nImportant\n\n\n\nLes principes vus dans le module FETCH sur la gestion des données s’appliquent aussi à votre code : on souhaite écrire un script qui fonctionne maintenant, mais également dans le futur, et qui soit partageable. Il existe quelques bonnes pratiques qui peuvent nous aider à aller dans cette direction, et la première est d’avoir un code source propre et bien organisé."
  },
  {
    "objectID": "sessions_core/02_import_data.fr.html#mise-en-place-du-projet",
    "href": "sessions_core/02_import_data.fr.html#mise-en-place-du-projet",
    "title": "Importation des données",
    "section": "Mise en place du projet",
    "text": "Mise en place du projet\n\nStructure des dossiers\n\nSi ce n’est pas déjà fait, téléchargez le dossier du cours décompressez-le. Sauvegardez le dossier non compressé à un endroit non connecté à OneDrive et ouvrez-le.\n\n\n\n  Dossier du cours\n\n\n\n\nCe dossier illustre une structure typique et recommandée pour vos projets de code :\n\n📁 data\n\n📁 raw\n📁 clean\n\n📁 R\n📁 outputs\n\nCe dossier sera votre répertoire de travail pour toutes les sessions de ce cours. Vous y créerez un projet RStudio (explications ci-dessous), et y enregistrerez tous vos scripts (sous dossier R). Les données brutes se trouvent déjà dans data/raw.\n\n\nDéfinitions\nVoici deux concepts importants que nous allons rencontrer dans cette session :\nRépertoire de travail. Le répertoire de travail est l’emplacement (dossier) où votre session R en cours travaille. Si vous enregistrez un fichier, par exemple, il sera enregistré dans ce dossier par défaut. De même, Si vous ouvrez un fichier, ce dossier sera affiché par défaut. Tous les chemins relatifs auront ce dossier pour origine. Par défaut, R choisit généralement votre dossier “Documents” comme répertoire de travail sur les machines Windows.\nRacine. La racine fait référence au niveau de dossier le plus élevé du répertoire de travail. Si le dossier de votre cours s’appelle FETCHR la racine se trouverait directement à l’intérieur de celui-ci (et non dans l’un de ses sous-dossiers comme R ou data).\n\n\nProjets RStudio\nUn projet RStudio est outil qui va faciliter votre vie et aider RStudio à trouver les différents fichiers.\nPour rappel, votre interface doit ressembler à ceci :\n\n\n\n\n\n\nFigure 1: Capture d’écran d’une interface RStudio typique\n\n\n\n\nOuvrez RStudio et suivez ces étapes pour créer un nouveau projet :\n\ncliquez sur File &gt; New Project &gt; Existing Directory &gt; Browse,\nnaviguez jusqu’au dossier du cours (en l’ouvrant)\ncliquez sur Create Project.\n\n\n\nDans l’explorateur Windows, examinez le dossier du cours. Vous devriez maintenant voir un nouveau fichier avec l’extension .Rproj qui a une petite icône bleue avec un R au milieu\n\n\n\n\nIcône associée aux projets RStudio\n\n\n\n\n\n\n\n\nNote\n\n\n\nSi vous ne voyez pas ce fichier, c’est probablement parce qu’il est caché par défaut sur votre ordinateur. Pour modifier ce paramètre dans l’explorateur Windows, allez dans le menu Afficher et sélectionnez Extensions de noms de fichier.\n\n\nLorsque vous ouvrez un projet RStudio, RStudio démarre une nouvelle session R spécifique à ce projet, ouvre les fichiers associés et définit la racine de votre dossier comme répertoire de travail. Une conséquence immédiate est que le panneau Files en bas à droite de l’interface montre les sous dossiers présents dans le répertoire de travail, i.e. votre dossier de cours.\n\n\n\n\n\n\nTip\n\n\n\nIl est fortement recommandé de mettre en place un projet RStudio distinct pour chacune de vos analyses afin de garantir que les fichiers de vos projets restent organisés.\n\n\nIl existe plusieurs façons d’ouvrir un projet RStudio :\n\nUtilisez le menu RStudio File &gt; Open Project puis sélectionnez le fichier .Rproj approprié\nCliquez sur le bouton Project: (none) en haut à droite de l’interface RStudio\nNaviguez dans l’explorateur de fichiers Windows jusqu’à votre dossier de cours et double-cliquez sur le fichier avec l’extension .Rproj\n\n\n\nLes options de RStudio\nAvant de poursuivre, allons modifier certaines des options de RStudio qui peuvent causer des problèmes.\n\nOuvrez les options globales (Tools &gt; Global Options) et ouvrez l’onglet General (menu de gauche). Déselectionnez toutes les cases des sections R Sessions, Workspace et History.\n\n\n\n\nCapture d’écran des options de RStudio\n\n\nLorsque ces options sont activées, RStudio enregistre les objets de votre environnement et les charge à chaque fois que vous ouvrez une nouvelle session R. Ca semble être une bonne idée, mais il est en fait préférable de toujours commencer votre travail à partir d’une session R vide afin d’éviter les erreurs.\n\n\n\n\n\n\nImportant\n\n\n\nN’oubliez pas que toutes les commandes nécessaires au nettoyage et à l’analyse de vos données doivent être enregistrées explicitement dans un script, dans le bon ordre. Faire retourner le script devrait arriver aux mêmes résultats que précédement.\n\n\n\n\nCréation d’un nouveau script\n\nOuvrez un nouveau script et enregistrez-le dans le sous-dossier R de votre projet sous le nom import_data.R.\nAjoutez des métadonnées au début du script, comme recommandé lors première session, en utilisant des commentaires. Veillez à inclure :\n\nLe titre\nL’auteur du script\nLa date de création\nUne description rapide de ce que fait le script\n\n\nNous sommes prêts à commencer à coder"
  },
  {
    "objectID": "sessions_core/02_import_data.fr.html#sec-packages",
    "href": "sessions_core/02_import_data.fr.html#sec-packages",
    "title": "Importation des données",
    "section": "Paquets",
    "text": "Paquets\nLes paquets [packages] sont des collections de fonctions qui étendent les fonctionalités de R. Vous en utiliserez un grand nombre pendant ce cours et dans votre travail quotidien. R étant open-souce, les packages sont téléchargeable et utilisable gratuitement.\n\n\n\n\n\n\nNote\n\n\n\nDans ce cours, nous utiliserons une convention commune qui est de référencer les paquets entre {}. Par exemple {ggplot2} est le nom du paquet ggplot2 qui contient des fonctions pour créer des graphes, telles que ggplot(), geom_point() etc…\n\n\n\nInstallation\nLa fonction install.packages() télécharge et installe un nouveau paquet sur votre ordinateur, dans la bibliothèque de paquets associée à R. Vous n’avez à faire cette opération qu’une seule fois par paquet et ordinateur.\n\ninstall.packages(\"here\") # installe le paquet {here} \n\nN’oubliez pas de mettre le nom du paquet entre guillemets lorsque vous utilisez la commande install.packages(). Que se passe-t-il si vous ne le faites pas ?\n\n\n\n\n\n\nNote\n\n\n\nSi vous suivez cette session dans le cadre d’un cours, pour éviter tout problème potentiel de connectivité internet pendant la formation, nous vous avons déjà fait installer la plupart des paquets du cours.\nSi vous suivez ce tutoriel seul ou si vous n’avez pas encore installé les paquets, vous devrez installer manuellement chaque nouveau paquet que nous rencontrerons avec la fonction install.packages().\n\n\n\n\nUtilisation\nUne fois qu’un paquet est installé, il faut indiquer à R que nous souhaitons l’utiliser pour une session donnée en le chargeant dans la session avec la fonction library().\n\nlibrary(here) # charge le paquet {here} dans la session\n\n\nUtilisez la fonction library() pour charger les paquets here et rio qui seront utilisés aujourd’hui.\n\nIl se peut que vous obteniez parfois un message d’avertissement signalant que certaines fonctions ont été masquées ou que la version actuelle du paquet a été construite pour une version différente de R. Ces messages ne doivent pas vous inquiéter, mais il faut les lire et essayer de comprendre ce qui se passe.\n\nExécutez le code suivant. Comprenez-vous le message d’erreur ?\n\nlibrary(ggplot)\n\n\nLe code ci-dessus génère une erreur car il y a une faute de frappe dans le nom du paquet, et vous avez donc essayé de charger un paquet qui n’existe pas. Rappelez-vous que R est pénible, et en particulier est sensible à la casse : beaucoup de vos erreurs viendront de petites fautes dans les noms de fonctions ou d’objets. Ici, par exemple, nous voulions charger le paquet ggplot2 mais nous avons écrit ggplot à la place.\n\n\n\n\n\n\nTip\n\n\n\nIl est recommandé d’avoir une section au début de votre script qui charge tous les paquets dont vous aurez besoin dans votre script en un seul endroit :\n\n# Packages ----------------------------\nlibrary(tidyverse)   # manipulation de données\nlibrary(lubridate)   # manipulation des dates\n\nCelà permet de savoir rapidement quels paquets doivent être installés pour exécuter un script.\n\n\n\nCréez une section “Paquets” dans votre script à l’aide de commentaires\n\n\n\nMettre à jour les paquets\nR dispose d’une communauté de développeurs très active et il est assez courant que les paquets soient mis à jour, avec de nouvelles fonctionalités ou des corrections de bugs. Pour mettre à jour les paquets de votre bibliothèque, rendez-vous dans l’onglet Packages du panneau inférieur droit et cliquez sur Update. N’oubliez pas que vous devez être connecté à internet pendant ce processus.\n\n\n\n\n\n\nImportant\n\n\n\nLa mise à jour de certains paquets peut parfois changer le comportement de certaines fonctions, ce qui peut casser votre code. Pas de panique. La meilleure pratique consiste à adapter votre code mais, dans le pire des cas, vous pouvez installer une ancienne version du paquet incriminé."
  },
  {
    "objectID": "sessions_core/02_import_data.fr.html#importation-de-données",
    "href": "sessions_core/02_import_data.fr.html#importation-de-données",
    "title": "Importation des données",
    "section": "Importation de données",
    "text": "Importation de données\n\nTrouver son chemin…\nPour ouvrir un fichier dans R, vous devez fournir un chemin d’accès au fichier. Un chemin d’accès est simplement un (long) nom pour un fichier qui inclut son emplacement sur votre ordinateur. Les chemins d’accès peuvent être absolus ou relatifs.\n\nChemins d’accès absolus\nLes chemins d’accès absolus sont spécifiques à votre ordinateur et vont jusqu’au niveau de votre disque dur. Par exemple : D:/OneDrive - MSF/Documents/monitoring/cholera/fancy_project/data/raw/example_linelist.xlsx. Il est clair que ce chemin ne fonctionne que sur un ordinateur particulier.\nL’utilisation de chemins absolus encodés en dur est fortement déconseillé car cela rend votre code fragile et augmente la maintenance : en effet, les chemins devront tous être mis à jour chaque fois quelqu’un d’autre exécute votre code, ou que le dossier du projet est déplacé sur votre ordinateur.\n\n\nChemins d’accès relatifs\nLes chemins relatifs sont définis par rapport à votre répertoire de travail. Comme l’emplacement du fichier .Rproj définit le répertoire de travail, les chemins sont relatifs à cette racine. Pour vous, un chemin relatif ressemblera à ça : data/raw/example_linelist.xlsx.\nCela signifie que tant que la structure interne du dossier contenant votre projet est préservée, le chemin d’accès relatif sera valable quelque soit l’ordinateur.\n\n\nChemins d’accès robustes avec la fonction here()\nLe paquet {here} dispose d’une fonction here() qui aide à créer des chemins d’accès. Elle présente deux avantages :\n\nElle détecte la présence d’un fichier .Rproj et est capable de construire un chemin absolu à partir d’un chemin relatif dans votre projet RStudio.\nElle choisit automatiquement le séparateur adapté à votre système d’exploitation : /, \\ ou //.\n\n\nlibrary(here)\nhere(\"data\", \"raw\", \"example_linelist.xlsx\")\n\n[1] \"/Users/hugzsoubrier/GitHub/repicentre/data/raw/example_linelist.xlsx\"\n\n\n\nlibrary(here)\nhere(\"data\", \"raw\", \"example_linelist.xlsx\")\n\n[1] \"/Users/hugzsoubrier/GitHub/repicentre/data/raw/example_linelist.xlsx\"\n\n\nVoyez comme nous n’avons défini que le chemin relatif et la fonction a reconstitué le chemin absolu. Celà marchera donc sur l’ordinateur d’un collègue, y compris sur un autre système d’exploitation, du moment que la structure du répertoire de travail est intacte.\nNous vous encourageons fortement à utiliser here() chaque fois que vous devez créer un chemin d’accès à un fichier.\n\nExécutez le code ci-dessus dans la console. Quel chemin d’accès here(\"data\", \"raw\") vous donne-t-il ?\n\n\nUtilisez here() pour créer le chemin vers le fichier Moissalla-rougeole-liste-lineaire-FR.xlsx.\n\n\n\n\n\n\n\nImportant\n\n\n\nhere() crée une chaîne de caractères contenant l’adresse d’un fichier, mais ne vérifie pas si ce fichier existe réellement sur votre ordinateur. Si le fichier est absent ou s’il y a une faute de frappe dans votre code, vous obtiendrez une erreur lors de l’utilisation du chemin ainsi créé. Vous pouvez tester si un fichier existe à cette adresse avec la fonction file.exists().\n\n\n\n\n\n\n\n\nTip\n\n\n\nOn veut souvent définir plusieurs chemins dans un projet (données brutes, données propres, où sauver les graphes etc.). C’est une bonne pratique que de créer une nouvelle section au début de votre script, après le chargement des paquets, pour définir et stocker les chemins d’accès dans des objets.\n\n\n\n\n\nImporter les données\nDans R, différents formats de fichiers sont importés par différentes fonctions spécialisées, ce qui est fastidieux à mémoriser et à charger. La fonction import() du paquet {rio} nous fait gagner du temps en reconnaissant l’extension des fichier et en appelent automatiquement une fonction spécialisée pour charger les données.\nComme import() ne fait qu’appeler d’autres fonctions en arrière-plan, il est possible qu’elle ait besoin d’arguments optionnels spécifiques pour certains types de fichier.\n\n\n\n\n\n\nTip\n\n\n\nLa (longue) liste des types de fichiers pris en charge par {rio} est sur le site du paquet. Dans la suite de la leçon, nous nous concentrerons sur l’importation de données à partir de fichiers Excel .xlsx.\n\n\n\nImport de la première feuille\nAu minimum la fonction import() a besoin qu’on lui donne le chemin du fichier avec l’argument file :\n\nimport(file = here(\"data\", \"raw\", \"example_linelist.xlsx\"))\n\nNotez que nous avons imbriqué la commande here() à l’intérieur de la commande import(). L’imbrication de fonctions est autorisée et même courrante en R. R évalue les fonctions imbriquées de l’intérieur (here()) à l’extérieur (import()). La valeur renvoyée par here() est donc utilisée comme valeur d’entrée d’import().\n\nImportez le fichier Moissalla-rougeole-liste-lineaire-FR.xlsx en utilisant here() et import().\n\nSi votre importation a fonctionné correctement, R affichera les données dans la console mais ne les enregistrera pas dans l’environnement car nous ne les avons pas assignées à un objet.\n\nRéimportez vos données, mais cette fois-ci, sauvegardez-les dans un objet appelé df_linelist.\n\n\n\n\n\n\n\nTip\n\n\n\nSi votre jeu de données est très gros, il vaut mieux éviter de l’afficher dans la console…\n\n\n\n\nImport d’une autre feuille\nComme vous venez de le voir, la fonction import() importe la première feuille d’un fichier Excel par défaut. Il est cependant possible de passer le numéro de la feuille ou son nom (en chaîne de caractères) à l’argument which :\n\nimport(file = here(\"data\", \"raw\", \"example_linelist.xlsx\"),  # chemin\n       which = 2)                                            # spécifie la deuxième feuille\n\nNotez que l’argument which est spécifique aux types de fichiers comportant plusieurs feuilles, tels que les fichiers Excel ou .Rdata. Si vous essayez de l’utiliser sur un fichier .csv l’argument sera ignoré."
  },
  {
    "objectID": "sessions_core/02_import_data.fr.html#aperçu-des-données",
    "href": "sessions_core/02_import_data.fr.html#aperçu-des-données",
    "title": "Importation des données",
    "section": "Aperçu des données",
    "text": "Aperçu des données\nNous avons importé un jeu de données dans R et l’avons assigné à un objet (df_linelist). Nous pouvons maintenant inspecter le data frame créé pour vérifier que l’export s’est bien passé, et commencer à évaluer le nettoyage à faire.\nNous pouvons commencer par jeter un coup d’œil rapide aux premières lignes du data frame à l’aide de la fonction head(). Son premier argument est le data frame à inspecter et le second, n, accepte un nombre de lignes à afficher (optionnel).\n\nhead(df_linelist, n = 10) # Affiche les 10 premières lignes\n\n\nUtilisez head() pour examiner les 12 premières lignes de df_linelist.\n\nNous pouvons inspecter la structure du data frame à partir de l’onglet Environnement dans le panneau supérieur droit. Nous pouvons également visualiser le data frame dans le le visualiseur de données de RStudio (en haut à gauche).\n\nCliquez sur le bouton rond bleu à côté de df_linelist dans votre environnement pour examiner sa structure. Cliquez ensuite sur le nom du data frame pour le visualiser.\n\nLe visualiseur permet d’afficher le data frame comme dans un tableur et est un moyen pratique d’examiner rapidement vos données. Vous pouvez trier et filtrer vos données dans cet onglet mais ces actions ne modifieront pas l’objet df_linelist. Le visualiseur peut également être ouvert en utilisant directement la fonction View() sur le data frame."
  },
  {
    "objectID": "sessions_core/02_import_data.fr.html#cest-fini",
    "href": "sessions_core/02_import_data.fr.html#cest-fini",
    "title": "Importation des données",
    "section": "C’est fini !",
    "text": "C’est fini !\nBravo et n’oubliez pas de sauvegarder votre code !\n\n\n\n Solutions exercices"
  },
  {
    "objectID": "sessions_core/02_import_data.fr.html#pour-aller-plus-loin",
    "href": "sessions_core/02_import_data.fr.html#pour-aller-plus-loin",
    "title": "Importation des données",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\n\nExercices supplémentaires\n\nUtilisez dim() pour examiner les dimensions de votre data frame.\nUtilisez str() pour vérifier le type de données de chaque colonne. Voyez-vous quelque chose d’étrange ? N’oubliez pas que vous pouvez également utiliser des fonctions telles que is.character() et is.numeric() si vous souhaitez tester le type d’une colonne particulière.\nEn utilisant une fonction apprise lors de la première session, pouvez-vous extraire les noms des colonnes du data frame ? Ces résultats correspondent-ils à ce que vous voyez lorsque vous ouvrez les données dans Excel ?\nEssayez d’exécuter la fonction summary() sur votre data frame. Qu’est ce que le résultat vous apprend sur les variables ?\n\n\n\nRessources complémentaires\n\nLe site web de {rio}\nPlus d’exemples sur l’importation de données de différents types de fichiers"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.fr.html",
    "href": "sessions_core/04_data_verbs_conditional.fr.html",
    "title": "Traitement de données, recoder et filtrer",
    "section": "",
    "text": "Dans la session précédente vous avez appris les bases du traitement de données en R avec les fonctions du {tidyverse}, en particulier comment sélectionner et modifier les colonnes d’un data frame. Dans cette session nous allons allez plus loin sur la modification des data frame et apprendre à :\n\nÉcrire des conditions logiques basiques, ce qui va nous permettre de :\nSélectionner des lignes d’un data frame avec filter()\nRecoder des variables avec case_when()"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.fr.html#objectifs",
    "href": "sessions_core/04_data_verbs_conditional.fr.html#objectifs",
    "title": "Traitement de données, recoder et filtrer",
    "section": "",
    "text": "Dans la session précédente vous avez appris les bases du traitement de données en R avec les fonctions du {tidyverse}, en particulier comment sélectionner et modifier les colonnes d’un data frame. Dans cette session nous allons allez plus loin sur la modification des data frame et apprendre à :\n\nÉcrire des conditions logiques basiques, ce qui va nous permettre de :\nSélectionner des lignes d’un data frame avec filter()\nRecoder des variables avec case_when()"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.fr.html#mise-en-place",
    "href": "sessions_core/04_data_verbs_conditional.fr.html#mise-en-place",
    "title": "Traitement de données, recoder et filtrer",
    "section": "Mise en place",
    "text": "Mise en place\nPrérequis : cette leçon part du principe que vous connaissez les bases de la manipulation de données avec {dplyr}, et en particulier la fonction mutate(). Aller vous rafraîchir si besoin.\n\nNous utiliserons la liste linéaire avec les données brutes qui peut être téléchargée ici :\n\n\n\n Télécharger les données\n\n\n\n Si ce n’est pas déjà fait, enregistrez la dans le sous-dossier approprié de votre projet RStudio puis créez un nouveau script appelé filtrer_recoder.R dans votre sous-dossier R. Ajoutez un en-tête approprié et chargez les paquets suivants : {here}, {rio} et {tidyverse}.  Enfin, ajoutez une section dédiée à l’import des données, utilisez {here} et {rio} pour importer vos données dans R, et assignez-les à un objet que nous appellerons df_brut"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.fr.html#filtrer-des-données-avec-des-conditions-logiques",
    "href": "sessions_core/04_data_verbs_conditional.fr.html#filtrer-des-données-avec-des-conditions-logiques",
    "title": "Traitement de données, recoder et filtrer",
    "section": "Filtrer des données avec des conditions logiques",
    "text": "Filtrer des données avec des conditions logiques\nNous avons appris précédemment comment comment sélectionner les colonnes d’un data frame. Nous allons à présent apprendre la tâche complémentaire, qui est la sélection des lignes d’un data frame. C’est une tâche particulièrement courante du travail d’épidémiologiste qui permet de sélectionner des observations qui satisfont à certains critères. Le paquet {dplyr} possède bien évidement une fonction pour ça, la fonction filter().\nAvant de pouvoir l’utiliser nous allons néanmoins devoir apprendre à écrire des conditions logique, qui sont également un prérequis pour recoder des variables. Les conditions logiques sont des questions (ou tests) auxquelles R va répondre par TRUE ou FALSE (ou NA).\n\nEgalité\nLa syntaxe de filter() est assez simple :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\ndf_brut |&gt;\n  filter([condition_logique])\n\nCette syntaxe permet de conserver les lignes où condition_logique est vraie. Ici, la condition logique va demander si quelque chose est égale à autre chose. Par exemple, si telle variable est égale à telle valeur (est ce que patient a été hospitalisé ?). En R, nous testons l’égalité avec l’opérateur ==.\nEn pratique, pour créer un filtre qui ne garde que les patients hospitalisés nous écrivons :\n\ndf_brut |&gt;\n  filter(hospitalisation == \"oui\")\n\nIci, filter() parcourt chaque ligne de notre data frame et teste si la valeur d’hospitalisation dans cette ligne est égale à \"oui\". La fonction ne renvoie alors que les lignes où la réponse à la question est TRUE [vrai].\n\nFiltrez vos données pour ne conserver que les patients qui avaient de la fièvre (c’est à dire les patients contenant la valeur \"Yes\" dans la colonne fievre. Le début de la colonne fievre dans la sortie filtrée est :\n\n\n  fievre\n1    Yes\n2    Yes\n3    Yes\n4    Yes\n5    Yes\n6    Yes\n\n\nInspectez la sortie et df_brut. Pourquoi df_brut contient-il encore les patients qui n’avaient pas de fièvre ?\n\n\n\nInégalité\nParfois, nous préférons tester l’inégalité plutôt que l’égalité ; pour examiner les patients qui ne se sont pas rétablis, par exemple, qu’ils soient décédés ou sorti contre avis médical. Dans ce cas nous utiliserons l’opérateur !=, ce qui donne ce code :\n\ndf_brut |&gt;\n  filter(issue != 'gueri') # Garde les lignes avec patients NON guéris\n\n\nFiltrez votre data frame pour ne montrer que les patients qui n’ont pas de carte confirmant leur statut vaccinal. Le début de la colonne filtrée ressemble à :\n\n\n  statut_vaccinal\n1             Non\n2             Non\n3             Non\n4             Non\n5             Non\n6             Non\n\n\nAstuce : Rappelez-vous que vous pouvez utiliser count() pour vérifier les modalités de statut_vaccinal.\n\n\n\nSupérieur à / Inférieur à\nDans le cas des variables numériques, on sera souvent intéressé par savoir si une valeur est supérieure ou inférieure à un seuil. Par exemple, quels sont les patients de moins de 5 ans. Ici, nous utiliserons les opérateurs &lt; et &gt; pour évaluer si une variable est inférieure à ou supérieure à une valeur donnée, respectivement.\nNous pouvons par exemple filtrer les patients de moins de 60 mois :\n\ndf_brut |&gt;\n  filter(age &lt; 60)\n\n\nAffichez un data frame ne contenant que les patients souffrant de malnutrition aiguë sévère. Le début de la colonne concernée est :\n\n\n    pb\n1  244\n2  232\n3  123\n4  210\n5   80\n6  220\n7  152\n8  155\n9  232\n10 135\n\n\nEcrivez un autre filtre qui sélectionne les patients âgés de plus de 15 ans. L’en-tête de votre colonne d’âge doit ressembler à ceci :\n\n\n  age\n1 348\n2 348\n3 312\n4 432\n5 444\n6 324\n\n\n\nSi nous ne voulons pas l’égalité stricte nous pouvons ajouter un signe égal aux opérateurs précédents, ce qui donne &lt;= pour “inférieur ou égal à” et &gt;= pour “supérieur ou égal à”. Attention, le = doit venir après les opérateurs &lt; et &gt;, pas avant.\nPour filtrer les patients avec 10 ans ou moins :\n\ndf_brut |&gt;\n  filter(age &lt;= 120)\n\n\nSélectionnez tous les patients avec un état nutritionnel normal, c’est-à-dire les patients dont le PB est supérieur ou égal à 125mm. L’en-tête du pb devrait ressembler à ceci :\n\n\n    pb\n1  244\n2  232\n3  210\n4  220\n5  152\n6  155\n7  232\n8  135\n9  146\n10 202\n\n\n\n\n\nConditions multiples\nIl est possible de combiner plusieurs conditions logiques dans un même filtre ! Il suffit de séparer plusieurs conditions logiques par une virgule.\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\ndf_brut |&gt;\n  filter([condition 1],\n         [condition 2],\n         [condition 3])\n\nPar exemple, nous pourrions sélectionner tous les patients hospitalisés de moins de cinq ans :\n\ndf_brut |&gt;\n  filter(age &lt; 5,\n         hospitalisation == \"oui\")\n\n\nCréez un filtre qui sélectionne tous les patients de la sous-préfecture de Koumra hospitalisés et sévèrement malnutris Cela donne :\n\n\n    id sous_prefecture hospitalisation  pb\n1 8624          KOUMRA             oui 103\n2 8939          KOUMRA             oui  67\n3 9957          KOUMRA             oui  71\n\n\nIndice :  if faut une condition sur le statut d’hospitalisation, une sur la sous-préfecture et une sur le PB.\n\n\n\nRésumé des conditions logiques\nNous avons fait le tour des conditions logiques les plus basiques en R. Les voici rassemblées dans un tableau pour références futures :\n\n\n\nCondition\nR\n\n\n\n\nA identique à B ?\nA == B\n\n\nA pas identique à B ?\nA != B\n\n\nA supérieur à B ?\nA &gt; B\n\n\nA supérieur ou égal à B ?\nA &gt;= B\n\n\nA inférieur à B ?\nA &lt; B\n\n\nA inférieur ou égal à B ?\nA &lt;= B"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.fr.html#recoder-des-variables-avec-case_when",
    "href": "sessions_core/04_data_verbs_conditional.fr.html#recoder-des-variables-avec-case_when",
    "title": "Traitement de données, recoder et filtrer",
    "section": "Recoder des variables avec case_when()",
    "text": "Recoder des variables avec case_when()\nL’utilité des conditions logiques dans le traitement de données va bien plus loin que la sélection de lignes ! Elles sont par exemple très utiles quand nous voulons recoder des variables. Nous utiliserons les conditions logiques à l’intérieur de la fonction case_when() (également du paquet {dplyr}) pour recoder les variables.\nLa fonction case_when() est un peu plus complexe que ce que l’on a vu jusqu’à présent, mais très puissante (et va donc vous être très utile). Nous allons décomposer sa syntaxe pas à pas.\nVous utiliserez presque toujours case_when() dans un mutate() pour recoder une variable existante ou en créer une nouvelle, avec cette syntaxe :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\ndf_brut |&gt;\n  mutate(nouvelle_colonne = case_when(\n    [condition_1] ~ [valeur_si_condition_1_est TRUE],\n    [condition_2] ~ [valeur_si_condition_2_est TRUE],\n    .défaut = [valeur_par_défaut]))\n\nDécomposons-la commande.\nA l’exception de la dernière ligne, chaque ligne à l’intérieur de la fonction case_when() a le format suivant :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\n[condition] ~ [valeur si condition est VRAIE]  # Les crochets sont là pour la lisibilité\n\nAinsi, pour recoder les patients avec un PB inférieur à 110mm comme \"MAS\", nous écrivons la commande suivante dans notre case_when() :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\n# [condition] ~ [valeur si VRAIE]\n   pb &lt; 110   ~ \"MAS\"\n\nIl y a en général plus d’une condition ! Dans notre exemple, une autre condition logique testerait si le patient est modérément malnutri avec l’instruction pb &lt; 125 ~ \"MAM\".\nLa dernière ligne de notre pseudo code contient l’argument .default et sert à fournir la valeur à utiliser lorsqu’aucune des conditions n’est remplie. Dans notre exemple, ça pourrait être \"Normal\".\nPour résumer, pour résumer, pour créer une variable contenant le statut nutritionnel à partir du PB :\n\ndf_brut |&gt;\n  mutate(malnut = case_when(\n    pb &lt; 110 ~ \"MAS\",\n    pb &lt; 125 ~ \"MAM\",\n    .default = \"Normal\"))\n\n\nExécutez le code ci-dessus pour créer une variable malnut contenant le statut nutritionnel des patients. Le haut des deux colonnes concernées renvoie :\n\n\n   pb malnut\n1 244 Normal\n2 232 Normal\n3 123    MAM\n4 210 Normal\n5  80    MAS\n6 220 Normal\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nL’ordre des conditions logiques est important ! case_when() teste les conditions dans l’ordre que vous lui donnez et attribue une valeur dès qu’une condition est TRUE.\nAinsi, dans l’exemple ci-dessus, case_when() pose ces questions suivantes dans l’ordre :\n\nEst-ce que pb &lt; 110 pour ce patient ? Si oui, attribuer la valeur \"MAS\"\nSi le patient n’est pas MAS, est-ce que pb &lt; 125 ? Si oui, attribuer la valeur \"MAM\"\nSi aucune des conditions précédentes n’est vraie, attribuer la valeur \"Normal\"\n\n\n\n\nIntervertissez l’ordre des deux premières conditions dans le case_when()précédent (pb &lt; 125 testé en premier). Le haut des deux colonnes concernées est maintenant :\n\n\n   pb malnut\n1 244 Normal\n2 232 Normal\n3 123    MAM\n4 210 Normal\n5  80    MAM\n6 220 Normal\n\n\nVous pouvez enregistrer le data frame crée dans un objet temporaire temp pour l’inspecter plus facilement. Où sont les patients MAS ? Comprenez-vous ce qui s’est passé ?\n\n\n\n\n\n\n\nNote\n\n\n\nL’argument .default dans case_when() n’est pas obligatoire. Si vous ne l’incluez pas, case_when() utilisera la valeur NA par défaut.\n\n\nDans notre exemple, nous avons utilisé case_when() pour créer une variable catégorique (le statut nutritionnel) à partir d’une variable continue (le PB). Un autre exemple typique et similaire est de créer une colonne contenant les classes d’âge.\n\nUtilisez case_when() pour créer une variable groupe_age avec les catégories suivantes :\n\n\"&lt; 5 Ans\"\n\"5 - 15 Ans\"\n\"&gt; 15 Ans\".\nsi l’âge est manquant, attribuer la valeur \"Inconnu\".\n\nFaites attention à l’ordre ! L’en-tête des colonnes concernées doit ressembler à ceci :\n\n\n   age  age_group\n1   36    &lt; 5 Ans\n2    5    &lt; 5 Ans\n3  156 5 - 15 Ans\n4    8    &lt; 5 Ans\n5    7    &lt; 5 Ans\n6    4    &lt; 5 Ans\n7    2    &lt; 5 Ans\n8   48    &lt; 5 Ans\n9  156 5 - 15 Ans\n10 348   &gt; 15 Ans\n\n\n\n\nL’opérateur %in%\nNous savons maintenant recoder les variables en catégories, ce qui vous arrivera très souvent en épidémiologie. Un autre cas d’usage majeur est d’utiliser case_when() pour standardiser les valeurs d’une variable.\n\nUtilisez count() pour inspecter les variables catégorielles de votre jeu de données. Lesquelles devraient être standardisées ?\n\nVous avez dû voir que la variable sexe présente quelques problèmes d’encodage. Par exemple, les patientes sont codées comme f, female et femme. Utilisons case_when() pour recoder cette variable. Ici, nous ne créerons pas une nouvelle variable, mais remplacerons la variable existante :\n\ndf_brut |&gt;\n  mutate(sexe = case_when(sexe == \"f\"      ~ \"Femme\",\n                          sexe == \"female\" ~ \"Femme\",\n                          sexe == \"femme\"  ~ \"Femme\",\n                          sexe == \"h\"      ~ \"Homme\",\n                          sexe == \"male\"   ~ \"Homme\",\n                          sexe == \"homme\"  ~ \"Homme\",\n                          .default = \"Inconnu\"))\n\nCe code fonctionne correctement mais est terriblement répétitif et verbeux. Heureusement il y a un raccourci pour lister toutes les options à réaffecter à “Femme” (et celles à “Homme”), l’opérateur %in% ! L’opérateur %in% permet de tester la condition “est ce que la valeur existe dans ce vecteur ?”.\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\n[valeur] %in% [vector_des_options]\n\nAinsi, par exemple, nous pourrions vérifier si la valeur “f” est dans les options “f” et “femme” :\n\n\"f\" %in% c(\"f\", \"femme\")\n\n\nExécutez l’instruction ci-dessus. Quel est le type de données de votre résultat ?\n\nLa commande renvoie un bolléen, c’est-à-dire un résultat logique. C’est donc une condition logique valide à utiliser dans un case_when() (ou un filter()) ! On peut donc simplifier notre code :\n\ndf_brut |&gt;\n  mutate(sexe = case_when(\n    sexe %in% c(\"f\", \"female\", \"femme\") ~ \"Femme\",\n    sexe %in% c(\"h\", \"male\", \"homme\") ~ \"Homme\",\n    .default = \"Inconnu\"))\n\nC’est plus court comme ça…\n\nUtilisez case_when() et l’opérateur %in% pour créer une nouvelle colonne vacc_status_strict qui a la valeur :\n\n\"Oui\" si le statut vaccinal est confirmé\n\"Non\" pour les cas non vaccinés\n\n\"Non vérifié\" sinon.\n\nLa tête de la nouvelle colonne ressemble à ceci :\n\n\n  statut_vaccinal statut_vaccinal_strict\n1            &lt;NA&gt;            Non vérifié\n2             Non                    Non\n3      Oui - oral            Non vérifié\n4             Non                    Non\n5             Non                    Non\n6             Non                    Non"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.fr.html#pipeline-de-nettoyage-des-données",
    "href": "sessions_core/04_data_verbs_conditional.fr.html#pipeline-de-nettoyage-des-données",
    "title": "Traitement de données, recoder et filtrer",
    "section": "Pipeline de nettoyage des données",
    "text": "Pipeline de nettoyage des données\nMaintenant que vous savez utiliser les conditions logiques pour recoder colonnes avec case_when(), nous pouvons reprendre le pipeline de nettoyage que nous avions commencé dans la session précédente.\n\nReprenez, le code de la session précédente, amendez-le et complétez le pour créer un gros pipeline de nettoyage des données, qui crée un data frame df_linelist en effectuant les opérations suivantes :\n\nSupprimer les variables nom_complet et unite_age\nRenommer les variables suivantes :\n\nage devient age_ans\nsous_prefecture devient prefecture\nvillage_commune devient village\nnom_structure_sante devient structure\n\nAjouter une variable age_ans avec l’âge du patient en années\nMettre à jour region et prefecture pour utiliser la casse de titre\nMettre à jour toutes les colonnes contenant des dates pour utiliser le type Date\nCréer une nouvelle variable groupe_age avec les groupes &lt; 6 mois, 6 - 11 mois, 12 - 59 mois, 5 - 15 ans et &gt; 15 ans (les patients dont l’âge est inconnu sont Inconnu)\nRecoder le sexe pour n’avoir que les valeurs : Femme, Homme et Inconnu\n\nSupprimer toutes les lignes en double\n\nLe début de vos données finales devrait ressembler à ceci :\n\n\n  id  sexe age_mois  region prefecture        village date_debut\n1  1 Femme       36 Mandoul   Moissala Sangana Koïtan 2022-08-13\n2  2 Femme        5 Mandoul   Moissala      Mousdan 1 2022-08-18\n3  3 Femme      156 Mandoul   Moissala     Djaroua Ii 2022-08-17\n4  6 Homme        8 Mandoul   Moissala     Monakoumba 2022-08-22\n5  7 Homme        7 Mandoul   Moissala      Tétindaya 2022-08-30\n6 10 Homme        4 Mandoul   Moissala      Danamadja 2022-08-30\n  date_consultation hospitalisation date_admission\n1        2022-08-14             oui     2022-08-14\n2        2022-08-25             oui     2022-08-25\n3        2022-08-20            &lt;NA&gt;           &lt;NA&gt;\n4        2022-08-25             non           &lt;NA&gt;\n5        2022-09-02             non           &lt;NA&gt;\n6        2022-09-02             oui     2022-09-02\n                        structure tdr_paludisme fievre eruption toux\n1 Hôpital du District de Moissala       negatif     No     &lt;NA&gt;  Yes\n2 Hôpital du District de Moissala       negatif     No       No  Yes\n3                      CS Silambi       negatif    Yes     &lt;NA&gt;   No\n4 Hôpital du District de Moissala       negatif     No       No   No\n5                      CS Silambi       negatif   &lt;NA&gt;       No  Yes\n6                    Moissala Est       negatif    Yes       No   No\n  yeux_rouges pneumonie encephalite  pb statut_vaccinal doses_vaccin issue\n1          No        No          No 244            &lt;NA&gt;         &lt;NA&gt; gueri\n2          No      &lt;NA&gt;          No 232             Non         &lt;NA&gt;  &lt;NA&gt;\n3          No        No        &lt;NA&gt; 123      Oui - oral         &lt;NA&gt; gueri\n4        &lt;NA&gt;        No          No 210             Non         &lt;NA&gt; gueri\n5         Yes        No          No  80             Non         &lt;NA&gt; gueri\n6        &lt;NA&gt;        No          No 220             Non         &lt;NA&gt; gueri\n  date_issue    age_ans   groupe_age\n1 2022-08-18  3.0000000 12 - 59 mois\n2 2022-08-28  0.4166667     &lt; 6 mois\n3       &lt;NA&gt; 13.0000000   5 - 15 ans\n4       &lt;NA&gt;  0.6666667  6 - 11 mois\n5       &lt;NA&gt;  0.5833333  6 - 11 mois\n6 2022-09-03  0.3333333     &lt; 6 mois\n\n\n\nTop ! Nous pouvons maintenant exporter ce data frame (presque) propre hors de R. Pour cela nous utiliserons la fonction export() de {rio} (et notre fidèle compagnon, la fonction here() de {here} pour gérer les chemins d’accès) :\n\ndf |&gt;\n  export(here('data', 'clean', 'measles_linelist_clean.xlsx'))\n\nNotez ici que nous plaçons nos données dans le sous-dossier clean dans data.\n\n\n\n\n\n\nTip\n\n\n\nEnregistrer les données au format .xlsx est utile pour pouvoir les ouvrir dans Excel pour les inspecter ou les partager. Cependant, nous préférerons souvent utiliser un fichier avec l’extension .rds. Ce type de fichier est spécifique à R et est plus robuste aux problèmes liés à l’encodage ou au formatage des dates que les fichiers de type .xlsx ou .csv.\nPour exporter votre data frame vers un fichier .rds, il suffit de modifier l’extension :\n\ndf |&gt;\n  export(here('data', 'clean', 'measles_linelist_clean.rds')) # TADAM !"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.fr.html#cest-fini",
    "href": "sessions_core/04_data_verbs_conditional.fr.html#cest-fini",
    "title": "Traitement de données, recoder et filtrer",
    "section": "C’est fini !",
    "text": "C’est fini !\nBravo. Lors des deux dernières sessions vous avez appris à utiliser les fonctions qui forment le socle du traitement de données, mais aussi les conditions logiques et comment organiser votre code en un pipeline de nettoyage !\n\n\n\n Solutions exercices"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.fr.html#aller-plus-loin",
    "href": "sessions_core/04_data_verbs_conditional.fr.html#aller-plus-loin",
    "title": "Traitement de données, recoder et filtrer",
    "section": "Aller plus loin",
    "text": "Aller plus loin\n\nExercices supplémentaires"
  },
  {
    "objectID": "sessions_extra/data_exploration.fr.html",
    "href": "sessions_extra/data_exploration.fr.html",
    "title": "Exploration des données",
    "section": "",
    "text": "Effectuer une exploration rapide d’un ensemble de données importé\nProduire des tableaux de fréquence pour les variables"
  },
  {
    "objectID": "sessions_extra/data_exploration.fr.html#objectifs",
    "href": "sessions_extra/data_exploration.fr.html#objectifs",
    "title": "Exploration des données",
    "section": "",
    "text": "Effectuer une exploration rapide d’un ensemble de données importé\nProduire des tableaux de fréquence pour les variables"
  },
  {
    "objectID": "sessions_extra/data_exploration.fr.html#setup",
    "href": "sessions_extra/data_exploration.fr.html#setup",
    "title": "Exploration des données",
    "section": "Setup",
    "text": "Setup\nDependances. Cette session supplémentaire suppose que vous avez suivi les sessions introduction à R et R studio, et importation de données.\n\nPour cette session, nous travaillerons avec notre liste brute de rougeole de Moissala qui peut être téléchargée ici :\n\n\n\n  Course Folder\n\n\n\n Assurez-vous qu’il est correctement stocké dans data/raw de votre projet. Ensuite, ouvrez un nouveau script appelé data-exploration.R, et assurez-vous que les paquets {here}, {rio} et {dplyr} sont chargés. Enfin, importez les données dans R sous la forme d’un objet appelé df_linelist."
  },
  {
    "objectID": "sessions_extra/data_exploration.fr.html#exploration-des-données",
    "href": "sessions_extra/data_exploration.fr.html#exploration-des-données",
    "title": "Exploration des données",
    "section": "Exploration des données",
    "text": "Exploration des données\nJuste après avoir importé des données dans R, nous pouvons avoir envie d’y jeter un coup d’œil. Lorsque l’on parle d’exploration de données, on veut généralement faire plusieurs choses :\n\nExaminer les dimensions des données (c’est-à-dire le nombre de lignes et de colonnes).\nExaminer les noms des colonnes\nVisualiser les premières ou les dernières lignes\nDéterminer le type des variables\nDéterminer la plage de valeurs des variables continues\nObserver les valeurs possibles de chaque variable catégorielle\n\nCe processus est crucial et nous permettra de nous familiariser avec nos données et d’identifier les problèmes qui seront traités lors de l’étape de nettoyage des données."
  },
  {
    "objectID": "sessions_extra/data_exploration.fr.html#exploration-de-base",
    "href": "sessions_extra/data_exploration.fr.html#exploration-de-base",
    "title": "Exploration des données",
    "section": "Exploration de base",
    "text": "Exploration de base\nLa toute première chose que vous voulez savoir sur vos données, ce sont les dimensions, c’est-à-dire le nombre de lignes et le nombre de colonnes qui composent vos données. Il existe plusieurs façons d’obtenir ces informations dans R :\n\nRegardez votre volet environnement dans RStudio et vérifiez vos données - le nombre à côté (5230x25) nous indique qu’il s’agit d’un data frame avec 5230 lignes et 25 colonnes.\nUtilisez dim() sur vos données pour renvoyer un vecteur avec le nombre de lignes et le nombre de colonnes.\nVous pouvez aussi utiliser ncol() pour obtenir le nombre de colonnes et nrow() pour le nombre de lignes.\n\nIl est bon de se souvenir de ces nombres afin de pouvoir repérer rapidement tout changement inattendu dans vos données au cours de votre analyse (c’est-à-dire plus ou moins de lignes ou de colonnes que prévu).\n\nEn utilisant la méthode de votre choix, obtenez les dimensions de votre data frame df_linelist."
  },
  {
    "objectID": "sessions_extra/data_exploration.fr.html#noms-de-variables",
    "href": "sessions_extra/data_exploration.fr.html#noms-de-variables",
    "title": "Exploration des données",
    "section": "Noms de variables",
    "text": "Noms de variables\nComme nous allons utiliser les noms des variables très souvent au cours de notre analyse, nous voulons nous familiariser avec eux dès le début. De plus, nous devons identifier celles qui devront être renommées lors du nettoyage des données. La fonction names() renvoie un vecteur de tous les noms de variables dans notre cadre de données :\n\nnames(df_linelist)\n\n [1] \"id\"                   \"full_name\"            \"sex\"                 \n [4] \"age\"                  \"age_unit\"             \"region\"              \n [7] \"sub_prefecture\"       \"village_commune\"      \"date_onset\"          \n[10] \"date_consultation\"    \"hospitalisation\"      \"date_admission\"      \n[13] \"health_facility_name\" \"malaria_rdt\"          \"fever\"               \n[16] \"rash\"                 \"cough\"                \"red_eye\"             \n[19] \"pneumonia\"            \"encephalitis\"         \"muac\"                \n[22] \"vacc_status\"          \"vacc_doses\"           \"outcome\"             \n[25] \"date_outcome\"        \n\n\n\nQue pensez-vous des noms de votre ensemble de données ? Pouvez-vous déjà repérer des noms de variables que vous aimeriez renommer ?"
  },
  {
    "objectID": "sessions_extra/data_exploration.fr.html#inspecter-vos-données",
    "href": "sessions_extra/data_exploration.fr.html#inspecter-vos-données",
    "title": "Exploration des données",
    "section": "Inspecter vos données",
    "text": "Inspecter vos données\nIl est également intéressant d’inspecter vos données, cela peut vous permettre de repérer plus facilement certaines incohérences, des variables avec beaucoup de valeurs manquantes, et cela vous permettra de voir à quelles valeurs s’attendre pour chacune d’entre elles. Vous pouvez “print” vos données dans la console en :\n\nExécutant l’objet df_linelist seul (attention, vous ne voudrez peut-être pas faire cela si vous avez un grand ensemble de données).\nUtilisant la fonction head() pour voir les 6 premières lignes (vous pouvez augmenter ce nombre en utilisant l’argument n)\nUtilisant la fonction tail() pour voir les 6 dernières lignes (encore une fois, vous pouvez augmenter ce nombre en utilisant l’argument n)\n\nCes méthodes n’afficheront que les 40 premières lignes de vos données au maximum, car c’est la limite de votre console. Alternativement, vous pouvez utiliser View() pour voir vos données sous forme de tableau. Cela ouvrira une nouvelle fenêtre avec vos données affichées comme dans une feuille de calcul Excel. Note : cette commande ne fait qu’afficher les données, elle ne vous permet pas de les modifier.\n\n\n\n\n\n\nTip\n\n\n\nSoyez très prudent avec View() sur un grand jeu de données car cela peut faire planter votre session RStudio. Pour éviter cela, vous pouvez imprimer la sortie dans la console.\n\n\n\nPouvez-vous afficher les 15 premières lignes de vos données ? Que se passe-t-il lorsque vous modifiez la largeur de votre fenêtre de console et que vous exécutez à nouveau la commande ?"
  },
  {
    "objectID": "sessions_extra/data_exploration.fr.html#types-des-variables",
    "href": "sessions_extra/data_exploration.fr.html#types-des-variables",
    "title": "Exploration des données",
    "section": "Types des variables",
    "text": "Types des variables\nNous voulons maintenant vérifier le type des différentes variables. C’est important car une partie du nettoyage des données consiste à s’assurer que les variables numériques sont de type numeric, dates Date, et que les variables catégorielles sont de type factor ou character. Vous avez déjà vu la fonction class(), qui permet de vérifier le type d’un vecteur. Dans R, chaque variable d’un dataframe est un vecteur. Nous pouvons extraire toutes les valeurs de ce vecteur en utilisant le sign $, et les passer à la fonction class() :\n\nclass(df_linelist$age)\n\n\nEssayez d’extraire toutes les valeurs de la variable sex. Quelle est le type de cette variable ?\n\nVous pouvez également utiliser str() sur votre dataframe pour vérifier le type de toutes les variables à la fois :\n\nstr(df_linelist)\n\n\nUtilisez str() pour vérifier le type de données de chaque colonne. Y a-t-il quelque chose d’étrange ? Rappelez-vous que vous pouvez aussi utiliser des fonctions comme is.character() et is.numeric() si vous voulez tester le type d’une colonne en particulier."
  },
  {
    "objectID": "sessions_extra/data_exploration.fr.html#exploration-des-variables-continues",
    "href": "sessions_extra/data_exploration.fr.html#exploration-des-variables-continues",
    "title": "Exploration des données",
    "section": "Exploration des variables continues",
    "text": "Exploration des variables continues\nMaintenant que vous savez comment extraire les valeurs d’une variable, vous pouvez vouloir explorer certaines des valeurs des variables numériques pour vérifier les incohérences. Calculer des statistiques récapitulatives pour ces variables, et Base R fournit de nombreuses fonctions pratiques :\n\n\n\n\n\n\n\n\n\nFonction\nDescription\nExemple\nRetours\n\n\n\n\nmin()\nValeur minimale\nmin(x)\nValeur minimale unique\n\n\nmax()\nValeur maximale\nmax(x)\nValeur maximale unique\n\n\nmean()\nMoyenne arithmétique\nmean(x)\nValeur moyenne\n\n\nmedian()\nValeur moyenne\nmedian(x)\nValeur moyenne\n\n\nrange()\nMin et max\nrange(x)\nVecteur de (min, max)\n\n\nIQR(x)\nQ3 - Q1\nIQR(x)\nQ3 - Q1\n\n\nquantile()\nQuantiles spécifiés\nquantile(x, probs = c(0.25, 0.75))\nQuantiles demandés\n\n\nsd()\nEcart-type\nsd()\nEcart-type\n\n\nvar()\nVariance\nvar(x)\nVariance\n\n\nsum()\nSomme des valeurs\nsum(x)\nSomme\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCes fonctions exigent que vous supprimiez explicitement les valeurs manquantes (NA) en utilisant l’argument na.rm = TRUE\n\n\nVous pouvez extraire les valeurs d’une variable en utilisant $, et les passer à n’importe laquelle de ces fonctions.\n\nUtilisez la syntaxe $ pour obtenir :\n\nLa valeur minimale de age\nLe maximum de muac\n\nDes problèmes ?"
  },
  {
    "objectID": "sessions_extra/data_exploration.fr.html#exploration-des-variables-catégorielles",
    "href": "sessions_extra/data_exploration.fr.html#exploration-des-variables-catégorielles",
    "title": "Exploration des données",
    "section": "Exploration des variables catégorielles",
    "text": "Exploration des variables catégorielles\nEnfin, examinons les valeurs de nos variables catégorielles. Pour ce faire, nous pouvons utiliser des tableaux de fréquence. C’est pratique car :\n\nIls nous permettent de voir rapidement les valeurs uniques d’une variable catégorielle\nLe nombre d’observations pour chacune de ces catégories\n\nPour ce faire, on utilise la fonction count() du package {dplyr}, qui accepte un dataframe et le nom d’une (ou plusieurs !) colonne(s) en tant qu’arguments. Il compte alors le nombre d’observations de chaque élément unique dans cette colonne. Par exemple, voyons les valeurs possibles de la variable sex :\n\ncount(df_linelist, sex)\n\nLe résultat est un nouveau dataframe, plus petit, contenant le nombre de patients observés, stratifié par sex. Il semble que cette variable nécessite un recodage… Nous le ferons dans une prochaine session.\n\nEn utilisant les données de votre liste, examinez les valeurs de la variable outcome. A quoi cela ressemble-t-il ?\nMaintenant, essayez d’ajouter l’argument sort = TRUE à la fonction count(). Que fait cet argument ?"
  },
  {
    "objectID": "sessions_extra/data_exploration.fr.html#cest-fait",
    "href": "sessions_extra/data_exploration.fr.html#cest-fait",
    "title": "Exploration des données",
    "section": "C’est fait !",
    "text": "C’est fait !\nBravo pour ce premier coup d’œil sur vos données !\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_extra/faceting.fr.html",
    "href": "sessions_extra/faceting.fr.html",
    "title": "Graphiques multiples (facetting)",
    "section": "",
    "text": "Dans cette session, le but est d’apprendre à :\n\ncréer des graphiques multiples très rapidement avec {ggplot2}\nmodifier les paramètres les plus courants pour améliorer l’apparence de ces graphiques"
  },
  {
    "objectID": "sessions_extra/faceting.fr.html#objectifs",
    "href": "sessions_extra/faceting.fr.html#objectifs",
    "title": "Graphiques multiples (facetting)",
    "section": "",
    "text": "Dans cette session, le but est d’apprendre à :\n\ncréer des graphiques multiples très rapidement avec {ggplot2}\nmodifier les paramètres les plus courants pour améliorer l’apparence de ces graphiques"
  },
  {
    "objectID": "sessions_extra/faceting.fr.html#introduction",
    "href": "sessions_extra/faceting.fr.html#introduction",
    "title": "Graphiques multiples (facetting)",
    "section": "Introduction",
    "text": "Introduction\nPrérequis : ce satellite s’appuie sur la session sur les courbes épidémiques, durant laquelle nous avons appris à visualiser la distribution quotidienne des cas de rougeoles au cours du temps à l’aide de {ggplot2} :\n\n\n\n\n\n\n\n\n\nCe graphique est très utile, mais ce qui serait encore plus utile serait de pouvoir le décliner rapidement selon les modalités d’une autre variable. Par exemple, nous pourrions vouloir insérer un graphique similaire mais par groupe d’âge dans un rapport de situation [sitrep en anglais]. Il y a plusieurs manières d’arriver à ce résultat. Vous pourriez :\n\nfiltrer un jeu de données pour chacune des classes d’âge, copier-coller le code du graphe et l’adapter pour créer un graphique par classe d’âge\napprendre à utiliser les boucles for ou les fonctions des familles apply() ou map() qui servent répéter des actions sans copier-coller\nfaire confiance à {ggplot2} pour avoir une solution rapide…\n\nLa première option est fastidieuse et source d’erreurs, et nous la déconseillons. La seconde option n’est pas mauvaise en soi : les outils mentionnés sont extrêmement puissants et de bonnes cibles d’apprentissage pour quand vous serez plus à l’aise avec le langage. Mais ils sont trop avancés pour ce petit tutoriel, et une options simple existe déjà dans {ggplot2}."
  },
  {
    "objectID": "sessions_extra/faceting.fr.html#mise-en-place",
    "href": "sessions_extra/faceting.fr.html#mise-en-place",
    "title": "Graphiques multiples (facetting)",
    "section": "Mise en place",
    "text": "Mise en place\n\nNous utiliserons la même liste linéaire nettoyée que précédemment et qui peut être téléchargée ici :\n\n\n\n Télécharger les données\n\n\n\n Si ce n’est pas déjà fait, enregistrez le jeu de données dans data/clean puis créez un nouveau script appelé faceting.R dans votre sous-dossier R (alternativement, vous pouvez rajouter une section au script sur les courbes épis).\n Si vous créez un nouveau script, ajoutez un en-tête approprié et chargez les paquets suivants : {here}, {rio} et {tidyverse}. Importez ensuite les données propres dans R et enregistrez-les dans un objet df_linelist."
  },
  {
    "objectID": "sessions_extra/faceting.fr.html#un-graphe-par-modalité-faceting",
    "href": "sessions_extra/faceting.fr.html#un-graphe-par-modalité-faceting",
    "title": "Graphiques multiples (facetting)",
    "section": "Un graphe par modalité (faceting)",
    "text": "Un graphe par modalité (faceting)\nDans ggplot, “faceting” est est l’action de créer des graphiques en plusieurs parties. La fonction facet_wrap() trace automatiquement un graphique pour chacune des modalités d’une variable. Par exemple, vous pouvez créer une courbe épi par sexe, ou par site. Comme les autres couches d’un ggplot, on l’ajoute à un graphique avec un +. Cela crée une figure avec plusieurs petits graphiques, les fameuses facettes.\n\nPréparer les données\nDans cette leçon, nous expliquerons le code en traçant la courbe par sous-préfecture, et vous tracerez la courbe par groupe d’âge.\nSi l’on veut tracer la courbe épidémique par sous-préfecture, il faut que cette variable soit dans le data frame que nous passons à ggplot(). Nous allons donc créer un nouveau jeu de données agrégées qui contient le nombre de patients par jour et par sous-préfecture.\n\ndf_pref &lt;- df_linelist %&gt;%\n  count(date_debut, sous_prefecture,\n        name = 'patients')\n\nhead(df_pref)\n\n  date_debut sous_prefecture patients\n1 2022-08-13        Moissala        1\n2 2022-08-17        Moissala        1\n3 2022-08-18        Moissala        1\n4 2022-08-22        Moissala        1\n5 2022-08-30        Moissala        2\n6 2022-09-01        Moissala        1\n\n\n\nVous devez tracer le nombre de patients par groupe d’âge, donc il vous faut un data frame agrégé par jour et groupe d’âge. Créez-le et enregistrez-le comme df_age. Il a le format suivant :\n\n\n  date_debut age_groupe n\n1 2022-08-13  1 - 4 ans 1\n2 2022-08-17 5 - 14 ans 1\n3 2022-08-18   &lt; 6 mois 1\n4 2022-08-22 6 - 8 mois 1\n5 2022-08-30   &lt; 6 mois 1\n6 2022-08-30 6 - 8 mois 1\n\n\n\n\n\nTracer le graphique\nMaintenant que les données sont prêtes, il ne nous reste plus qu’à tracer le graphique. Examinez le code ci-dessous, il est presque identique à ce que nous avons fait précédemment, à part la dernière ligue qui crée les facettes :\n\ndf_pref %&gt;%\n  ggplot(aes(x = date_debut,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date d'apparition des symptomes\",\n       y = \"Cas de rougeole\",\n       title = \"Measles cases in Mandoul (Chad)\") +\n  theme_classic(base_size = 15) +\n  facet_wrap(vars(sous_prefecture))   # Graphique par sous-pref !\n\n\n\n\n\n\n\n\nJ’espère que vous êtes soufflés ! D’un point de vue syntaxe, la fonction facer_wrap() prend en argument le nom de la variable catégorique qui nous intéresse, enrobé dans la fonction vars().\n\nA votre tour. Tracez le graphe par classe d’âge. Il devrait ressembler à ça :"
  },
  {
    "objectID": "sessions_extra/faceting.fr.html#modifier-laspect-des-facettes",
    "href": "sessions_extra/faceting.fr.html#modifier-laspect-des-facettes",
    "title": "Graphiques multiples (facetting)",
    "section": "Modifier l’aspect des facettes",
    "text": "Modifier l’aspect des facettes\nOuvrez la page d’aide de la fonction sur le site du paquet pour avoir la liste des arguments acceptés. Nous allons aborder certains d’entre eux à présent.\n\nNombre de lignes ou de colonnes\nLes arguments nrow et ncol vous permettent de décider combien de facettes il doit y avoir sur une ligne, ou sur une colonne.\nSi nous voulions avoir toutes les facettes sur deux lignes :\n\ndf_pref %&gt;%\n  ggplot(aes(x = date_debut,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date d'apparition des symptomes\",\n       y = \"Cas de rougeole\",\n       title = \"Cas de rougeole dans la région de Mandoul (Tchad)\") + \n  theme_classic(base_size = 15) +\n  \n  facet_wrap(vars(sous_prefecture),\n             nrow = 2)  \n\n\n\n\n\n\n\n\nOu au contraire nous pouvons forcer le nombre de lignes à 4 pour avoir une figure tout en hauteur :\n\ndf_pref %&gt;%\n  ggplot(aes(x = date_debut,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date d'apparition des symptomes\",\n       y = \"Cas de rougeole\",\n       title = \"Cas de rougeole dans la région de Mandoul (Tchad)\") + \n  theme_classic(base_size = 15) +\n  \n  facet_wrap(vars(sous_prefecture),\n             nrow = 4)  \n\n\n\n\n\n\n\n\n\nUtilisez un des deux arguments présentés ci-dessus pour créer un graphe avec trois colonnes.\n\n\n\nPlages des axes\nAvez-vous remarqué que les valeurs minimales et maximales en x et en y étaient les mêmes pour toutes les facettes ? C’est que par défaut facet_wrap() fixe les plages pour les deux axes. Ce comportement est raisonnable pour pouvoir comparer les facettes et éviter d’induire le lecteur en erreur.\nCeci étant dit, si vous êtes plus intéressé par la forme de la courbe à l’intérieur de chaque facette que par la comparaison des catégories entre elles, il peut être approprié de zoomer sur les données disponibles en autorisant des axes indépendants (“libres” de varier). Prévenez alors le lecteur que les facettes ne sont pas toutes à la même échelle.\nL’argument scales [échelles] accepte les valeurs suivantes :\n\n\"fixed\" : la valeur par défaut, x et y à la même échelle pour toutes les facettes\n\"free_x\" : l’échelle de x peut varier entre facettes\n\"free_y\" : l’échelle de y peut varier entre facettes\n\"free\" : les deux axes peuvent varier entre facettes\n\nContrastez le graphe précédent avec celui-ci :\n\n\n\n\n\n\n\n\n\nNous avons autorisé à avoir des échelles indépendantes sur toutes les facettes en x et en y, pour zoomer sur les cas dans chaque sous-préfecture.\n\nTracez la courbe par groupe d’âge, avec l’axe des abscisses fixe et l’axe des ordonnées libre."
  },
  {
    "objectID": "sessions_extra/faceting.fr.html#cest-fini",
    "href": "sessions_extra/faceting.fr.html#cest-fini",
    "title": "Graphiques multiples (facetting)",
    "section": "C’est fini !",
    "text": "C’est fini !\nBravo, vous avez créé vos premiers graphiques en fonctions d’une variable catégorique. Cela devrait vous être très utile. Sachez que la fonction fonctionne aussi avec d’autres types de graphes créés par {ggplot2}.\nSi le graphique est très large, il est possible que les étiquettes des dates ne soient pas très lisibles en x, et c’est le cas pour certains des exemples. Cela peut être contrôlé, et le sujet est abordé dans un autre satellite !\n\n\n\n Solutions des exercices"
  },
  {
    "objectID": "pathway.fr.html",
    "href": "pathway.fr.html",
    "title": "Cours",
    "section": "",
    "text": "Ces sessions peuvent être suivies afin d’obtenir un niveau de base dans R. La série suppose aucune expérience préalable dans R et convient bien aux débutants.\nVous en voulez plus ? Vous voulez plus de flexibilité ? Consultez le catalogue complet des sessions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R\n\n\n\n\n\nYour first steps in R. Learn your way around Rstudio, and meet some common R objects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Importation\n\n\n\n\n\nCreate an Rstudio project, install useful packages and start importing data to work in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Basics\n\n\n\n\n\nAn introduction to data manipulation and cleaning using {dplyr}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Filtering and Recoding\n\n\n\n\n\nUsing {dplyr} and conditional logic to filter and recode data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Tables\n\n\n\n\n\nCreate summary tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Data Visualization\n\n\n\n\n\nLearn the basics of buidling plots with ggplot2, and create your first epicurve\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resources.fr.html",
    "href": "resources.fr.html",
    "title": "Ressources",
    "section": "",
    "text": "Cette page contiendra (éventuellement) des ressources externes pour poursuivre votre parcours d’apprentissage du R."
  },
  {
    "objectID": "explore.fr.html",
    "href": "explore.fr.html",
    "title": "Explorer",
    "section": "",
    "text": "Choisissez votre propre aventure en parcourant toutes les sessions disponibles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Data Visualization\n\n\n\nCore\n\n\nVisualization\n\n\n\nLearn the basics of buidling plots with ggplot2, and create your first epicurve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourbes épidémiques hebdomadaires\n\n\n\nSatellite\n\n\nVisualization\n\n\n\nApprenez à tracer des courbes épidémiques hebdomadaires et à améliorer les étiquettes des axes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Exploration\n\n\n\nSatellite\n\n\nData Exploration\n\n\n\nExplore your data after importation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Importation\n\n\n\nCore\n\n\nRStudio\n\n\nData Import\n\n\n\nCreate an Rstudio project, install useful packages and start importing data to work in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Basics\n\n\n\nCore\n\n\nData Manipulation\n\n\nData Cleaning\n\n\n\nAn introduction to data manipulation and cleaning using {dplyr}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Filtering and Recoding\n\n\n\nCore\n\n\nData Manipulation\n\n\nData Cleaning\n\n\nLogic\n\n\n\nUsing {dplyr} and conditional logic to filter and recode data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnquête standardisée sur la mortalité\n\n\n\nCompagnon\n\n\nAnalyse\n\n\n\nSession complémentaire au module FETCH d’enquête\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploration des données\n\n\n\nSatellite\n\n\nData Exploration\n\n\n\nExplorez vos données après l’importation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFaceting\n\n\n\nSatellite\n\n\nVisualization\n\n\n\nCreate a plot with multiple subplots (facets)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraphiques multiples (facetting)\n\n\n\nSatellite\n\n\nVisualization\n\n\n\nApprenez à créer plusieurs mini graphiques “par catégorie” en une seule commande\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportation des données\n\n\n\nCore\n\n\nRStudio\n\n\nData Import\n\n\n\nCréez un projet Rstudio, installez les paquets utiles et importez des données pour travailler dans R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R\n\n\n\nCore\n\n\nR Basics\n\n\nData Types\n\n\n\nYour first steps in R. Learn your way around Rstudio, and meet some common R objects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to data visualization with ggplot2\n\n\n\nCore\n\n\nVisualization\n\n\n\nApprenez les bases de la visualisation avec ggplot2, et créez votre première épicurve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction à R\n\n\n\nCore\n\n\nR Basics\n\n\nData Types\n\n\n\nVos premiers pas dans R. Familiarisez-vous avec Rstudio et avec les objets courants de R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Mortality Survey\n\n\n\nCompanion\n\n\nAnalysis\n\n\n\nCompanion session to the survey FETCH module\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Tables\n\n\n\nCore\n\n\nSummary Tables\n\n\n\nCreate summary tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurveillance\n\n\n\nCompagnon\n\n\nAnalyse\n\n\n\nTutoriel d’accompagnement au module Surveillance du FETCH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurveillance\n\n\n\nCompanion\n\n\nAnalysis\n\n\n\nCompanion session to the surveillance Fetch-R module\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTableaux récapitulatifs\n\n\n\nCore\n\n\nTableaux de resumé\n\n\n\nCréer des tableaux récapitulatifs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraitement de données, les bases\n\n\n\nCore\n\n\nManipulation des données\n\n\nNettoyage des données\n\n\n\nUne introduction à la manipulation et au nettoyage des données à l’aide du paquet {dplyr}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraitement de données, recoder et filtrer\n\n\n\nCore\n\n\nManipulation des données\n\n\nNettoyage des données\n\n\nLogique\n\n\n\nApprenez à recoder vos variables avec {dplyr} et comment sélectionner les lignes d’un data frame suivant certains critères\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Epicurves\n\n\n\nSatellite\n\n\nVisualization\n\n\n\nPlot weekly epicurves and improve date labels on the x-axis\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sessions_companion/survey_basic.fr.html",
    "href": "sessions_companion/survey_basic.fr.html",
    "title": "Enquête standardisée sur la mortalité",
    "section": "",
    "text": "Calculer personne-temps à risque\nUtiliser {srvyr} pour estimer les taux de mortalité"
  },
  {
    "objectID": "sessions_companion/survey_basic.fr.html#objectifs",
    "href": "sessions_companion/survey_basic.fr.html#objectifs",
    "title": "Enquête standardisée sur la mortalité",
    "section": "",
    "text": "Calculer personne-temps à risque\nUtiliser {srvyr} pour estimer les taux de mortalité"
  },
  {
    "objectID": "sessions_companion/survey_basic.fr.html#introduction",
    "href": "sessions_companion/survey_basic.fr.html#introduction",
    "title": "Enquête standardisée sur la mortalité",
    "section": "Introduction",
    "text": "Introduction\nCette session se concentre sur les analyses de base pour une enquête rétrospective sur la mortalité à l’aide du protocole standard de MSF. Nous utiliserons une étude de cas dans laquelle une enquête a été menée à la suite d’une épidémie de choléra en Haïti en 2010.\nCette session suppose que vous avez suivi le parcours d’apprentissage de base pour R et que vous êtes capable de :\n\nImporter des données\nEffectuer un nettoyage de base à l’aide de case_when()\nAgrégation des données à l’aide de count() et summarize()\nProduction de tableaux à l’aide de gt()\n\nSi vous avez besoin de revoir ou d’apprendre l’un de ces sujets, veuillez vous reporter aux sessions de base du parcours d’apprentissage."
  },
  {
    "objectID": "sessions_companion/survey_basic.fr.html#configuration",
    "href": "sessions_companion/survey_basic.fr.html#configuration",
    "title": "Enquête standardisée sur la mortalité",
    "section": "Configuration",
    "text": "Configuration\n\nCette session utilise une étude de cas spécifique. Téléchargez et décompressez le dossier associé, puis ouvrez le script main.R à partir du dossier R :\n\n\n\n Télécharger\n\n\n\n\nLe dossier que vous avez téléchargé contient un script R (presque) vide ainsi que des fichiers Excel pour le formulaire Kobo utilisé dans l’enquête et les données collectées avec celui-ci.\n\nPrenez une minute pour ouvrir et examiner le formulaire Kobo et les données brutes. Que contiennent les différents onglets des données brutes ?"
  },
  {
    "objectID": "sessions_companion/survey_basic.fr.html#import",
    "href": "sessions_companion/survey_basic.fr.html#import",
    "title": "Enquête standardisée sur la mortalité",
    "section": "Import",
    "text": "Import\nNotre ensemble de données comporte deux onglets, le premier contenant les données au niveau des ménages et le second les données individuelles. Pour l’instant, nous nous intéressons principalement aux données individuelles, mais nous aurons finalement besoin des deux. Chargeons tout cela dans R (ainsi que les paquets que nous utiliserons dans la session d’aujourd’hui).\n\nDans votre script (main.R), ajoutez un en-tête approprié pour le fichier et créez une section qui charge les paquets suivants :\n\nhere\nrio\ngt\nsrvyr\ntidyverse\n\nCréez ensuite une nouvelle section appelée Import et utilisez rio pour importer la deuxième feuille de votre ensemble de données dans un objet appelé df_raw. Nous n’avons pas besoin de toutes les colonnes de ces données, utilisez select() pour sélectionner uniquement les suivantes :\n\nsex\nage\nborn\nborn_date\njoined\njoined_date\nleft\ndied\ndied_date\ndied_cause\n_parent_index renommé hh\n\nCréez ensuite un deuxième objet appelé df_hh contenant la première feuille de votre ensemble de données en conservant uniquement les colonnes suivantes :\n\ninterview_date\nclst_id\n_index renommé hh\npresent\nconsent\n\nAstuce. N’oubliez pas que lorsque vous utilisez select(), vous pouvez rapidement renommer un élément à l’aide d’un =, par exemple : hh = \"_parent_index\"."
  },
  {
    "objectID": "sessions_companion/survey_basic.fr.html#premier-aperçu-et-recodage",
    "href": "sessions_companion/survey_basic.fr.html#premier-aperçu-et-recodage",
    "title": "Enquête standardisée sur la mortalité",
    "section": "Premier aperçu (et recodage)",
    "text": "Premier aperçu (et recodage)\nSuper ! Maintenant que nous avons chargé nos données, jetons-y un premier coup d’œil. L’une des premières choses que nous pouvons faire est de vérifier la structure de nos données :\n\ndf_raw |&gt;\n  str()\n\nNous pouvons également vérifier rapidement combien de personnes dans l’ensemble de données sont décédées, car notre enquête porte sur la mortalité :\n\ndf_raw |&gt;\n  count(died)\n\n\nUtilisez count() pour déterminer le nombre de participants par sexe.\n\nHum, 1 et 2 pour le sexe sont un peu ambigus. Il pourrait être utile de recoder nos données catégorielles afin d’utiliser des étiquettes plus significatives. Par exemple :\n\ndf &lt;- df_raw |&gt;\n  # recodage\n  mutate(\n    sexe = case_when(\n      sexe == 1 ~ \"Homme\",\n      sexe == 2 ~ 'Femme`,\n      .default = NA\n    )\n  )\n\n\nCréez une nouvelle section dans votre script intitulée Nettoyage. Cette section comprendra un ‘pipeline de nettoyage’ qui prendra df_raw, effectuera plusieurs étapes de nettoyage et stockera le cadre de données résultant dans un objet appelé ‘df’.  À l’aide de case_when(), créez une nouvelle étape dans votre pipeline de nettoyage qui recode les variables catégorielles de votre ensemble de données. Vous pouvez utiliser le recodage ci-dessus pour sex. Pour les variables born, joined, left et died, utilisez le recodage suivant :\n\n0 = Non\n1 = Oui\n99 = Inconnu\n\nPour ‘died_cause’, utilisez le recodage suivant :\n\n1 = Diarrhée\n2 = Fièvre\n3 = Maladie respiratoire\n4 = Accident\n5 = Pendant l’accouchement\n6 = Autre\n99 = Inconnu\nNA = N’est pas décédé\n\nLa tête de ‘df’ devrait ressembler à ceci :\n\n\n    sex age born_date joined_date left_date died_date born joined left died\n1 Femme  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n2  Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n3 Femme  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n4 Femme   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n5  Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n6 Femme  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n        died_cause hh\n1 N'est pas décédé  1\n2 N'est pas décédé  1\n3 N'est pas décédé  1\n4 N'est pas décédé  1\n5 N'est pas décédé  1\n6 N'est pas décédé  1\n\n\nMaintenant que nous avons des étiquettes plus claires, explorons un peu plus nos données. Par exemple :\n\nCombien de personnes sont décédées pour chaque cause potentielle ?\nObservez les combinaisons de died, left, joined et born. Quelles sont les combinaisons les plus courantes ? Cela vous semble-t-il logique ?\nQui est le plus touché par le décès, les hommes ou les femmes ? Qui était le plus à risque ?\n\n Astuce. N’oubliez pas que vous pouvez attribuer plusieurs noms de colonnes à count() afin de créer des tableaux de contingence."
  },
  {
    "objectID": "sessions_companion/survey_basic.fr.html#nettoyage",
    "href": "sessions_companion/survey_basic.fr.html#nettoyage",
    "title": "Enquête standardisée sur la mortalité",
    "section": "Nettoyage",
    "text": "Nettoyage\n\nDates en tant que dates (simples)\nTerminons le nettoyage de nos données pour l’analyse. Nous devons notamment nous assurer que toutes nos données sont du bon type. Nous avons déjà recodé toutes les variables catégorielles, mais nous n’avons pas encore examiné les dates.\n\nclass(df_raw$born_date)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nIl semble que nos dates soient de type POSITct. Convertissons-les en un format de date plus simple à l’aide de ymd() de {lubridate}.\n\nAjoutez une étape à votre pipeline de nettoyage qui utilise ymd() de {lubridate} pour convertir les dates de born_date, joined_date, left_date et died_date en dates simples. \nLa classe () de df$born_date devrait désormais être Date :\n\nclass(df$born_date)\n\n[1] \"Date\"\n\n\n\n\n\nCorrection des problèmes logiques\nUn formulaire Kobo bien conçu peut grandement contribuer à garantir la collecte de données de bonne qualité dès le départ. Par exemple, nous pouvons rendre certaines questions obligatoires afin d’éviter les données manquantes. Nous pouvons également créer des ‘contraintes’ qui généreront des messages d’erreur lorsque les données saisies enfreignent les règles prédéfinies ; par exemple, nous pouvons créer une contrainte qui empêche l’enquêteur de saisir une date de décès qui ne correspond pas à la période de rappel.\n\nOuvrez le fichier Excel de l’enquête Kobo ‘retrospective-mortality_kobo.xlsx’ et examinez la colonne ‘contrainte’. Quelles sont les contraintes créées dans ce fichier ? Pouvez-vous penser à quelque chose qui n’a pas été pris en compte ?\n\nMalgré toutes les protections que nous avons mises en place dans le processus de collecte, un peu de nettoyage sera toujours nécessaire. Par exemple, bien que nous ayons veillé à ce que toutes les dates se situent dans la période de rappel, nous n’avons pas créé de contrôles pour d’autres relations illogiques entre les dates. Voyons, par exemple, si quelqu’un est né après son décès :\n\ndf_raw |&gt;\nfilter (died_date &lt; born_date)\n\n\nUtilisez filter() pour vérifier s’il y a des cas de personnes qui ont rejoint la famille après leur décès. Combien de fois cela s’est-il produit ?\nLe début de votre sortie devrait ressembler à ceci :\n\nCe n’est pas terrible. Comment pouvons-nous corriger cela ? La meilleure pratique à adopter ici fait l’objet d’un débat, mais nous vous recommandons de conserver la date de décès et de supprimer la date de naissance/d’adhésion (c’est-à-dire la remplacer par ‘NA’). Sinon, si vous constatez cette erreur pendant la collecte des données, vous pouvez interroger l’enquêteur à ce sujet. Il se peut qu’il s’agisse d’une faute de frappe et qu’il se souvienne des dates exactes. Dans la mesure du possible, essayez d’effectuer ce type de contrôle quotidiennement afin de pouvoir corriger les problèmes en temps réel.\nSi nous ne pouvons conserver qu’une seule date, pourquoi privilégier la date de décès ? La date de décès est la variable la plus importante dans le cadre de cette enquête particulière. Il s’agit également d’un événement rare, ce qui signifie que toute date manquante pourrait avoir un impact disproportionné sur les résultats. De plus, on peut supposer que la date de décès est plus fiable que d’autres dates (telles que la date exacte de naissance ou la date d’arrivée/de départ d’une personne dans le foyer).\n\n\n\n\n\n\nImportant\n\n\n\nDans ce cas précis, il n’y avait que quelques cas de ‘naissance ou entrée dans le foyer après le décès’ sur un ensemble de données de plus de 18 000 personnes, donc supprimer leurs dates de naissance/d’entrée dans le foyer n’est pas très grave. Toutefois, si des erreurs de ce type sont plus fréquentes, cela peut indiquer un problème important dans la conception du formulaire et/ou la formation des enquêteurs. Rechercher rapidement ce type de problèmes (même dans Excel) après la phase pilote et pendant la phase de collecte des données peut vous aider à repérer les problèmes tant que vous avez encore le temps de les corriger.\n\n\n\nÀ l’aide de mutate() et case_when(), ajoutez une étape à votre pipeline de nettoyage pour remplacer les dates de naissance/d’adhésion problématiques par NA. Comment pouvez-vous vérifier si cela a fonctionné correctement ?\n\nVoyez-vous d’autres problèmes dans les données ? J’en vois deux :\n\nQuelques personnes sont nées pendant la période de rappel, mais ont un âge supérieur à 0\nUne personne est décédée après avoir quitté le foyer\n\n\nComment traiteriez-vous ces deux problèmes ? Réfléchissez aux types de problèmes qui ont pu les causer et aux conséquences des différentes stratégies de nettoyage sur vos résultats finaux.  Bonus. Pensez-vous que l’un ou l’autre de ces problèmes aurait pu être évité grâce à une meilleure conception du Kobo ?\n\nExaminons d’abord le problème des personnes nées pendant la période de rappel et dont l’âge est supérieur à 0. La manière de traiter les âges inférieurs à 1 peut être délicate et les enquêteurs doivent être formés de manière explicite pour savoir s’ils doivent ‘arrondir’ ou ‘arrondir à la baisse’. Sinon, les enquêtes modernes ont tendance à demander l’âge en mois pour les personnes en dessous d’une certaine limite (généralement 12, 23 ou 59 mois). Il est particulièrement important d’enregistrer l’âge en mois des jeunes enfants dans les enquêtes qui se concentrent sur des questions telles que la vaccination, la malnutrition ou la mortalité, où les problèmes de santé concernés sont (potentiellement) associés aux nourrissons ou aux enfants de moins de 5 ans. Une contrainte aurait également pu être ajoutée au formulaire Kobo pour éviter ce problème.\nDans le cadre de cette enquête, nous ne disposons d’aucune information sur les mois, donc la meilleure chose à faire est d’imposer une règle cohérente selon laquelle tout âge inférieur à 12 mois doit être enregistré comme 0. Cela signifie que si un enfant est né pendant la période de rappel (qui est une période inférieure à 12 mois), son âge doit être 0.\n\nAjoutez une étape à votre pipeline qui garantit que toute personne née pendant la période de rappel a un âge de 0. Si vous avez effectué cette opération correctement, vous devriez pouvoir filtrer df pour ne voir que les personnes nées pendant la période de rappel et vérifier que leur âge est 0 :\n\ndf |&gt;\n  filter(born == 'Oui') |&gt;\n  pull(age) |&gt;\n  unique()\n\n[1] 0\n\n\n\nLe deuxième problème est un peu plus complexe. Examinons les individus concernés :\n\ndf |&gt;\nfilter(left_date &lt; died_date)\n\n     sex age born_date joined_date  left_date  died_date born joined left died\n1   Male  25      &lt;NA&gt;        &lt;NA&gt; 2010-11-02 2011-03-08  Non    Non  Oui  Oui\n2 Female   3      &lt;NA&gt;  2011-01-05 2010-11-20 2011-03-28  Non    Oui  Oui  Oui\n3 Female  60      &lt;NA&gt;  2011-03-08 2011-02-15 2011-03-15  Non    Oui  Oui  Oui\n  died_cause   hh\n1   Diarrhée   88\n2   Diarrhée  236\n3   Diarrhée 2861\n\n\n\nExaminez les trois personnes dans le résultat ci-dessus. Sont-elles toutes problématiques ? Pourquoi ou pourquoi pas ?\n\nLes personnes âgées de 3 et 60 ans ne posent pas de problème, elles ont simplement quitté le foyer puis l’ont réintégré. En revanche, la personne âgée de 25 ans semble avoir quitté le foyer puis être décédée sans jamais y revenir entre-temps. Que devons-nous faire ? Réfléchissons aux raisons pour lesquelles une telle situation pourrait apparaître dans nos données. Il existe deux options principales :\n\nPeut-être que la personne a réintégré le foyer, mais que le participant a oublié de le mentionner\nPeut-être que le participant n’a pas bien compris que l’enquête ne prendrait en compte que les décès survenus alors que la personne était encore membre du foyer au moment de son décès\n\nSi possible, nous pourrions discuter avec l’enquêteur qui a mené cet entretien afin de déterminer quelle option est la plus probable. En l’absence d’informations supplémentaires, nous devrons probablement opter pour la deuxième option. Si nous faisons cela, nous devrons recoder cette personne comme étant vivante plutôt que décédée, car elle était encore en vie au moment où elle a quitté le ménage.\n\nAjoutez une étape supplémentaire à votre pipeline qui recode cette personne comme étant vivante, c’est-à-dire : sa valeur ‘décédé’ doit être réinitialisée à ‘Non’ et sa date de décès doit être supprimée. Si vous vérifiez à nouveau les personnes qui ont quitté le foyer avant leur décès, vous ne devriez voir que deux personnes :\n\n\n    sex age born_date joined_date  left_date  died_date born joined left died\n1 Femme   3      &lt;NA&gt;  2011-01-05 2010-11-20 2011-03-28  Non    Oui  Oui  Oui\n2 Femme  60      &lt;NA&gt;  2011-03-08 2011-02-15 2011-03-15  Non    Oui  Oui  Oui\n  died_cause   hh\n1   Diarrhée  236\n2   Diarrhée 2861\n\n\nRemarque. La décision de supprimer un décès de l’ensemble de données est discutable. N’oubliez pas que les décès étant des événements rares, l’ajout ou la suppression d’un décès peut avoir un impact disproportionné sur les calculs de mortalité. Pour minimiser ce type de problème, veillez à consacrer suffisamment de temps à la formation des enquêteurs afin qu’ils comprennent parfaitement les concepts fondamentaux tels que la période de rappel et la notion de ‘ménage continu’. Donner des exemples spécifiques comme celui-ci pendant la formation peut aider les enquêteurs à gérer ces problèmes de manière appropriée lorsqu’ils se présentent pendant la collecte des données."
  },
  {
    "objectID": "sessions_companion/survey_basic.fr.html#joindre-les-données-des-ménages",
    "href": "sessions_companion/survey_basic.fr.html#joindre-les-données-des-ménages",
    "title": "Enquête standardisée sur la mortalité",
    "section": "Joindre les données des ménages",
    "text": "Joindre les données des ménages\nNos données individuelles semblent correctes, mais elles sont complètement déconnectées de nos données au niveau des ménages (vous vous souvenez de df_hh au début du tutoriel ?). Par exemple, nous aimerions connaître la date de l’entretien associée à chaque individu ainsi que le groupe auquel il appartenait. Pour ce faire, nous devons effectuer une jointure.\nUne analyse approfondie des jointures dépasse le cadre de cette session, mais en substance, les jointures sont utilisées pour prendre les données d’un dataframe et les ajouter (ligne par ligne) à un autre dataframe sur la base d’une variable commune aux deux ensembles de données (telle qu’un identifiant). Par exemple, nous voulons ici parcourir ligne par ligne nos données individuelles (df) et ajouter des colonnes contenant les informations relatives au niveau des ménages pour chaque personne (à partir de df_hh). Pour ce faire, nous utiliserons la fonction left_join() de {dplyr} :\n\ndf |&gt;\nleft_join(df_hh) |&gt;\nhead()\n\nJoining with `by = join_by(hh)`\n\n\n    sex age born_date joined_date left_date died_date born joined left died\n1 Femme  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n2  Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n3 Femme  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n4 Femme   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n5  Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n6 Femme  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n        died_cause hh interview_date clst_id present consent\n1 N'est pas décédé  1     2011-03-29       1       1       1\n2 N'est pas décédé  1     2011-03-29       1       1       1\n3 N'est pas décédé  1     2011-03-29       1       1       1\n4 N'est pas décédé  1     2011-03-29       1       1       1\n5 N'est pas décédé  1     2011-03-29       1       1       1\n6 N'est pas décédé  1     2011-03-29       1       1       1\n\n\nNotez qu’ici, R a utilisé la colonne hh (identifiant du ménage) comme variable commune entre les ensembles de données ; vous pouvez voir un message indiquant cela juste au-dessus de la sortie de head().\n\n\n\n\n\n\nNote\n\n\n\nQue signifie ‘left’ dans left_join() ? En termes simples, les jointures gauches impliquent un ensemble de données auquel des données sont ajoutées (ensemble de données A) et un autre dont des données sont extraites (ensemble de données B). L’ensemble de données A est l’‘ensemble de données principal’ et le résultat inclura toujours toutes ses lignes. Les lignes de l’ensemble de données B seront conservées si et seulement si left_join() trouve une ligne appropriée dans l’ensemble de données A à laquelle elles peuvent être ajoutées. Dans nos données, par exemple, les lignes de données sur les ménages qui n’avaient aucun membre (et qui n’apparaissent donc pas dans df) ne seront pas incluses dans le résultat de la jointure ci-dessus.  Dans une left_join(), R considérera toujours le premier argument comme l’ensemble de données principal (ensemble de données A) ; c’est-à-dire :\n\n# PSEUDO-CODE\nleft_join(dataset_a, dataset_b)\n\n\n\n\nAjoutez une dernière étape à votre pipeline de nettoyage qui utilise left_join() pour ajouter les données au niveau des ménages à chacune des lignes de df, puis convertit interview_date pour utiliser un format de date de base (comme vous l’avez fait avec born_date, etc.). Votre pipeline final devrait maintenant effectuer les opérations suivantes :\n\nUtiliser df_raw comme entrée\nRecoder les variables catégorielles\nConvertir les dates au format simple a-m-j\nCorriger les problèmes de données illogiques\nJoindre les indicateurs des ménages\n\nSi tout s’est bien passé, l’en-tête de df devrait ressembler à ceci :\n\nhead(df)\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n        died_cause hh interview_date clst_id present consent\n1 N'est pas décédé  1     2011-03-29       1       1       1\n2 N'est pas décédé  1     2011-03-29       1       1       1\n3 N'est pas décédé  1     2011-03-29       1       1       1\n4 N'est pas décédé  1     2011-03-29       1       1       1\n5 N'est pas décédé  1     2011-03-29       1       1       1\n6 N'est pas décédé  1     2011-03-29       1       1       1"
  },
  {
    "objectID": "sessions_companion/survey_basic.fr.html#calculs-de-mortalité",
    "href": "sessions_companion/survey_basic.fr.html#calculs-de-mortalité",
    "title": "Enquête standardisée sur la mortalité",
    "section": "Calculs de mortalité",
    "text": "Calculs de mortalité\nUne fois le nettoyage et la jonction terminés, nous pouvons enfin passer à la partie amusante, l’analyse. Nous voulons calculer les éléments suivants :\n\nTaux de mortalité brut\nTaux de mortalité des moins de cinq ans\nTaux de mortalité spécifique à la diarrhée\n\n\n(Sur papier) Écrivez la formule pour chacun de ces indicateurs. Disposons-nous déjà de toutes les variables nécessaires dans notre ensemble de données pour effectuer les calculs ?\n\nCes indicateurs sont des taux, ce qui signifie qu’ils nécessitent un dénominateur en temps-personne à risque. Notre ensemble de données ne comporte pas encore de colonne pour cela. Corrigeons cela.\n\nPersonne-temps à risque\nPour notre enquête, le temps à risque par personne correspond au temps pendant lequel chaque individu était :\n\nEn vie et\nMembre du ménage\n\nLa plupart des personnes étaient en vie et faisaient partie du ménage pendant toute la période de rappel. Pour ces personnes, leur temps à risque correspond à la totalité de la période de rappel. Il existe cependant plusieurs autres options. Par exemple :\n\n\n\n\n\n\nLa plupart de ces cas peuvent être traités de la même manière, à l’exception du dernier. Prenez une minute pour essayer de trouver sur papier une formule que nous pourrions utiliser pour le temps-personne. Vous gagnerez des points supplémentaires si vous parvenez à la convertir en code.\n\nIl n’est pas facile de trouver une bonne formule ici, alors examinons-la ensemble. Imaginons une personne qui a rejoint le ménage fin 2010 et qui est décédée en février 2011. Si nous insérons les données de cette personne dans un cadre de données, nous pourrions obtenir quelque chose comme ceci :\n\nexample &lt;- data.frame(\n  date_interview = as.Date(\"2011-04-07\"),\n  born = \"Non\",\n  date_born = NA,\n  joined = \"Oui\",\n  date_joined = as.Date(\"2010-12-08\"),\n  left = \"Non\",\n  date_left = NA,\n  died = \"Oui\",\n  date_died = as.Date(\"2011-02-13\")\n)\n\nexample\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07  Non        NA    Oui  2010-12-08  Non        NA  Oui\n   date_died\n1 2011-02-13\n\n\nPour calculer la durée d’exposition de cette personne, nous devons déterminer ‘quand sa période d’exposition a commencé’ et ‘quand elle s’est terminée’. Nous calculons ensuite la différence entre ces deux dates. Pour déterminer le début de la période d’exposition d’une personne, nous devons extraire la date à laquelle elle est née/a rejoint le foyer ou (si elle était présente pendant toute la période) la date de début de la période de rappel. Nous pouvons le faire à l’aide de case_when() :\n\nrecall_start &lt;- as.Date(\"2010-10-17\")\n\nexample |&gt;\n  mutate(\n    pt_start = case_when(\n      born == \"Oui\" ~ date_born,\n      joined == \"Oui\" ~ date_joined,\n      .default = recall_start\n    )\n  )\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07  Non        NA    Oui  2010-12-08  Non        NA  Oui\n   date_died   pt_start\n1 2011-02-13 2010-12-08\n\n\nDe même, leur période à risque prend fin lorsqu’ils décèdent / quittent l’entreprise ou à la fin du rappel (lorsqu’ils ont été interrogés) :\n\nexample |&gt;\n  mutate(pt_end = case_when(left == \"Oui\" ~ date_left,\n  died == \"Oui\" ~ date_died,\n  .default = date_interview))\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07  Non        NA    Oui  2010-12-08  Non        NA  Oui\n   date_died     pt_end\n1 2011-02-13 2011-02-13\n\n\nEn rassemblant ces informations, nous pouvons alors calculer le temps total passé à risque par une personne comme étant la différence entre la fin et le début de ce temps :\n\nexample |&gt;\n  mutate(\n  pt_start = case_when(\n    born == \"Oui\" ~ date_born,\n    joined == \"Oui\" ~ date_joined,\n    .default = recall_start),\n  pt_end = case_when(\n    left == \"Oui\" ~ date_left,\n    died == \"Oui\" ~ date_died,\n    .default = date_interview),\n  pt = pt_end - pt_start)\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07  Non        NA    Oui  2010-12-08  Non        NA  Oui\n   date_died   pt_start     pt_end      pt\n1 2011-02-13 2010-12-08 2011-02-13 67 days\n\n\n\nCréez une nouvelle section dans votre code intitulée ‘Calculer le temps de personne’ et initialisez un objet appelé ‘recall_start’ avec la date 2010-10-17. Ajoutez un bloc de code adaptant ce qui précède pour créer une colonne ‘pt’ dans ‘df’ qui calcule le temps de personne à risque. L’en-tête de ‘df’ devrait maintenant ressembler à ceci :\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n        died_cause hh interview_date clst_id present consent   pt_start\n1 N'est pas décédé  1     2011-03-29       1       1       1 2010-10-17\n2 N'est pas décédé  1     2011-03-29       1       1       1 2010-10-17\n3 N'est pas décédé  1     2011-03-29       1       1       1 2010-10-17\n4 N'est pas décédé  1     2011-03-29       1       1       1 2010-10-17\n5 N'est pas décédé  1     2011-03-29       1       1       1 2010-10-17\n6 N'est pas décédé  1     2011-03-29       1       1       1 2010-10-17\n      pt_end       pt\n1 2011-03-29 163 days\n2 2011-03-29 163 days\n3 2011-03-29 163 days\n4 2011-03-29 163 days\n5 2011-03-29 163 days\n6 2011-03-29 163 days\n\n\n\nUtilisons range() pour examiner les valeurs maximales et minimales de pt :\n\nrange(df$pt)\n\nTime differences in days\n[1] NA NA\n\n\nIl semble que la valeur du temps de risque par personne soit parfois manquante. Cela se produit lorsque, par exemple, une personne est née / a rejoint / a quitté / est décédée, mais que les informations relatives à la date de cet événement sont manquantes. Comment devons-nous traiter cela ? Une option consiste à laisser la valeur manquante, ce qui signifie que cette personne ne contribue pas au temps de risque par personne dans les calculs ultérieurs de la mortalité. Une autre option consiste à nous pouvons prendre la première valeur disponible pour laquelle nous avons une date. Ainsi, par exemple, si nous ne savons pas quand une personne est née, nous utiliserons le début de la période de rappel comme début de son temps de risque.\n\nQuels sont les avantages et les inconvénients de ces deux options ? Comment ajusteriez-vous votre code ci-dessus pour mettre en œuvre la deuxième option ?\n\nLa première option réduit artificiellement le dénominateur de nos calculs de mortalité, ce qui entraîne une surestimation de la mortalité. La deuxième option aura l’effet inverse. Pour l’analyse d’aujourd’hui, nous choisirons la première option et laisserons notre code tel quel (valeurs manquantes comprises). Examinons à nouveau notre plage, en ignorant cette fois les valeurs manquantes :\n\nrange(df$pt, na.rm = TRUE)\n\nTime differences in days\n[1] -141  172\n\n\nNous obtenons maintenant des chiffres, mais il semble que nous ayons des valeurs négatives. Que se passe-t-il ? Repensez à la figure au début de cette section. Si la plupart des cas peuvent être gérés avec notre calcul actuel, celui-ci ne tient pas compte des personnes qui ont quitté puis rejoint le ménage, car ces personnes auront ‘joined_date &gt; left_date’.\n\nRéfléchissez à ces personnes qui quittent puis rejoignent un ménage, ainsi qu’aux dates concernées. Pouvez-vous imaginer une équation pour leur temps de risque individuel ? Comment pensez-vous que cela pourrait être codé ?\n\nPour ces personnes, au lieu de prendre la différence (entre la fin et le début du temps de risque), nous devons calculer deux tranches de temps (avant leur départ et après leur retour), puis les additionner. Voici comment procéder :\n\ntmp &lt;- df |&gt;\n  mutate(\n    pt = case_when(\n    joined_date &gt; left_date & born == \"Oui\" ~ (left_date - born_date) + (interview_date - joined_date),\n    joined_date &gt; left_date ~ (left_date - recall_start) + (interview_date - joined_date),\n    .default = pt\n    )\n  )\n\nrange(tmp$pt, na.rm = TRUE)\n\nTime differences in days\n[1]   0 172\n\n\n\nAdaptez votre code pipe person-time afin d’inclure cette correction pour les personnes qui ont quitté puis réintégré le foyer. Ajoutez ensuite une ligne pour supprimer les colonnes pt_start et pt_end, car nous ne les utiliserons plus (et elles ne seront pas exactes pour les personnes qui ont quitté puis réintégré le foyer). La tête de df devrait désormais ressembler à ceci :\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n        died_cause hh interview_date clst_id present consent       pt\n1 N'est pas décédé  1     2011-03-29       1       1       1 163 days\n2 N'est pas décédé  1     2011-03-29       1       1       1 163 days\n3 N'est pas décédé  1     2011-03-29       1       1       1 163 days\n4 N'est pas décédé  1     2011-03-29       1       1       1 163 days\n5 N'est pas décédé  1     2011-03-29       1       1       1 163 days\n6 N'est pas décédé  1     2011-03-29       1       1       1 163 days\n\n\nBonus. Pourquoi notre case_when ci-dessus devait-il comporter un cas distinct pour les personnes nées pendant la période de rappel ?\n\nParfait, nous sommes presque prêts à calculer la mortalité ! Notez qu’à l’heure actuelle, nos valeurs pour le temps-personne sont représentées sous forme de différence de temps (classe difftime) en nombre de jours. Pour la suite de nos calculs, il serait préférable que ce soit un type numérique simple.\n\nAjoutez une dernière étape dans votre pipeline de temps-personne qui convertit pt en un type numérique à l’aide de as.numeric().\n\n\n\nCalculs de mortalité\nNous sommes (enfin) prêts à calculer les taux de mortalité. Nous pourrions effectuer un calcul de base directement à partir du nombre total de décès et du temps cumulé passé à risque :\n\nsum(df$died == \"Oui\") / sum(df$pt, na.rm = TRUE) * 10000\n\n[1] 0.5405712\n\n\n\nComment interpréteriez-vous ce taux de mortalité ? Est-il élevé ?  En 2010, le taux de mortalité de référence en Haïti était de 9 décès pour 1 000 personnes-années. Sachant cela, calculez la surmortalité observée pendant cette épidémie (exprimée en décès supplémentaires pour 10 000 personnes-jours).  Indice. Commencez par convertir le taux de référence en décès pour 10 000 jours-personnes.\n\nJusqu’ici tout va bien, mais nous n’avons n’avons pas inclus d’intervalles de confiance dans notre calcul et nous n’avons pas non plus pris en compte la conception de notre enquête. Pour ce faire, nous allons utiliser le package {srvyr}. Ce package a été conçu pour l’analyse complexe des données d’enquête et fournit des méthodes statistiques permettant d’ajuster l’effet de conception et la taille finie de la population. Une discussion approfondie de l’effet de conception et de la manière de l’ajuster dépasse le cadre de cette leçon, mais, en substance, les effets de conception apparaissent lorsque nous utilisons un processus d’échantillonnage qui n’est pas entièrement aléatoire . Par exemple, l’utilisation de l’échantillonnage en grappes dans cette enquête crée un effet de conception, car on peut s’attendre à ce que les personnes d’une même grappe soient plus similaires entre elles qu’elles ne le sont par rapport à d’autres personnes sélectionnées au hasard dans la population. Lorsque nous ajustons l’effet de conception, nous élargissons nos intervalles de confiance (nous réduisons notre précision) afin de tenir compte de cette similitude non aléatoire.\nPour effectuer ces ajustements, {srvyr} a besoin de connaître quelques informations :\n\nL’identifiant des unités d’échantillonnage nécessitant un ajustement (dans ce cas, les identifiants des grappes)\nLa taille de la population (nécessaire pour résoudre à la fois l’effet de conception et tenir compte de la taille finie de la population)\nLe poids de chaque grappe\n\nLe poids de la grappe est le produit de deux fractions :\n\nTaille totale de la population / taille de l’échantillon et\nCluster attendu / taille réelle du cluster donné\n\nEn principe, chacun de nos clusters aurait dû compter 32 ménages. Dans la pratique, la taille réelle des clusters a pu s’écarter dans certains cas. Nous pouvons utiliser la fonction n_distinct() dans summarize() pour ajouter une colonne avec la taille réelle du cluster associé à chaque individu :\n\ndf |&gt;\n  summarize(.by = clst_id,\n  hh_count = n_distinct(hh)) |&gt;\n  head()\n\n  clst_id hh_count\n1       1       32\n2       2       32\n3       3       32\n4       4       32\n5       5       32\n6       6       32\n\n\n\nCréez une nouvelle section de votre code intitulée ‘Calculer la mortalité’. Écrivez un tuyau qui utilise l’instruction summarize ci-dessus pour calculer le nombre de ménages observés par cluster, puis utilisez mutate() pour ajouter les colonnes weight et pop contenant respectivement les poids et la taille totale de la population (en 2010, celle-ci était de 228 425 personnes). Stockez la sortie de ce tuyau dans un objet appelé df_wt. La tête de df_wt devrait ressembler à ceci :\n\n\n  clst_id hh_count   weight    pop\n1       1       32 12.38546 228425\n2       2       32 12.38546 228425\n3       3       32 12.38546 228425\n4       4       32 12.38546 228425\n5       5       32 12.38546 228425\n6       6       32 12.38546 228425\n\n\nAstuce. La formule pour weight est (population_size / sample_size) * (32 / hh_count).  Maintenant, utilisez left_join() pour joindre les données de poids et de population nouvellement créées à df. La tête de df devrait maintenant ressembler à ceci :\n\n\nJoining with `by = join_by(clst_id)`\n\n\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;  Non    Non  Non  Non\n        died_cause hh interview_date clst_id present consent  pt hh_count\n1 N'est pas décédé  1     2011-03-29       1       1       1 163       32\n2 N'est pas décédé  1     2011-03-29       1       1       1 163       32\n3 N'est pas décédé  1     2011-03-29       1       1       1 163       32\n4 N'est pas décédé  1     2011-03-29       1       1       1 163       32\n5 N'est pas décédé  1     2011-03-29       1       1       1 163       32\n6 N'est pas décédé  1     2011-03-29       1       1       1 163       32\n    weight    pop\n1 12.38546 228425\n2 12.38546 228425\n3 12.38546 228425\n4 12.38546 228425\n5 12.38546 228425\n6 12.38546 228425\n\n\n\nPour que {srvyr} utilise nos variables nouvellement ajoutées et effectue des calculs, nous devons créer un ‘objet d’enquête’. Il s’agit d’une classe spéciale de dataframe spécifique à {srvyr} et créée à l’aide de la fonction as_survey_design() :\n\ntmp &lt;- df |&gt;\n  as_survey_design(\n    ids = clst_id,\n    wt = weight,\n    fpc = pop\n  )\n\n\nEssayez d’exécuter le code ci-dessus. Quelle est la classe de tmp ? Cet objet se comporte-t-il comme un cadre de données normal ? Essayez d’effectuer quelques manipulations de base, par exemple :\n\nExtrayez les données de la colonne ‘age’\nFiltrez pour ne voir que les personnes décédées\n\n\nComme vous pouvez le constater, une fois que nous avons appliqué as_survey_design(), nous n’avons plus de dataframe normal. Nous devons donc stocker son résultat dans un objet séparé, par exemple tmp ou df_srvy. Cela garantit que df reste un dataframe standard disponible pour d’autres calculs, visualisations, etc.\n{srvyr} offre un certain nombre de fonctions permettant de calculer des indicateurs à partir des données d’enquête, la plupart du temps utilisées à l’intérieur d’une fonction summarize(). Dans notre cas, nous utiliserons la fonction survey_ratio() pour calculer les taux de mortalité bruts et spécifiques. La syntaxe de base de survey_ratio() est assez simple. Par exemple, nous pouvons utiliser ce qui suit pour calculer le taux de mortalité brut :\n\n\n# A tibble: 1 × 4\n    cmr cmr_low cmr_upp cmr_deff\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 0.531   0.405   0.656     2.28\n\n\nC’est assez simple, mais analysons les arguments :\n\nnumerator est le numérateur de notre ratio, dans ce cas le nombre de personnes décédées (individus pour lesquels died était \"Oui\") multiplié par 10 000 (pour obtenir un résultat final en 10 000 jours-personnes)\ndenominator est le dénominateur de notre ratio, dans ce cas le nombre de personnes-jours à risque (pt)\nvartype toute variable que nous voulons inclure pour estimer l’erreur, ici nous avons choisi l’intervalle de confiance (\"ci\") mais nous pourrions également demander l’erreur type (\"se\")\ndeff indique si nous voulons inclure une estimation de l’effet de conception\nna.rm indique si {srvyr} doit ignorer les valeurs manquantes lors du calcul\n\nLe résultat de ce code est un nouveau cadre de données avec notre estimation ponctuelle (cmr), l’intervalle de confiance (cmr_low et cmr_upp) et l’effet de conception associé.\n\nDans l’exemple ci-dessus, nous avons calculé la mortalité brute. Comme cette enquête est associée à une épidémie particulière (de choléra), nous pourrions également nous intéresser à la mortalité spécifique à la maladie attribuable à la diarrhée. En utilisant le code de mortalité brute comme modèle, écrivez un code pour calculer le taux de mortalité spécifique à la diarrhée. Vous devriez obtenir les résultats suivants :\n\n\n# A tibble: 1 × 4\n   dsmr dsmr_low dsmr_upp dsmr_deff\n  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 0.369    0.257    0.482      2.64\n\n\nNous souhaitons également calculer le taux de mortalité des moins de 5 ans. Dans ce cas, nous calculons la mortalité brute sur le sous-ensemble de notre population âgé de moins de 5 ans, c’est-à-dire que nous devons filtrer notre cadre de données pour ne conserver que les enfants de moins de 5 ans. Écrivez du code pour calculer la mortalité des moins de 5 ans, en gardant à l’esprit que vous devrez filtrer avant de créer votre objet de conception d’enquête. Vous devriez obtenir le résultat suivant :\n\n\n# A tibble: 1 × 4\n   u5mr u5mr_low u5mr_upp u5mr_deff\n  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 0.677    0.363    0.990      1.14\n\n\nComment interpréteriez-vous tous les taux de mortalité ci-dessus ? Prenez une minute pour décrire comment vous pourriez présenter ces résultats. Y a-t-il d’autres indicateurs que vous aimeriez calculer pour une analyse plus complète ?"
  },
  {
    "objectID": "sessions_companion/survey_basic.fr.html#cest-terminé",
    "href": "sessions_companion/survey_basic.fr.html#cest-terminé",
    "title": "Enquête standardisée sur la mortalité",
    "section": "C’est terminé !",
    "text": "C’est terminé !\nBravo, vous avez maintenant appris à importer, nettoyer et calculer les taux de mortalité à partir de données d’enquête de base sur la mortalité.\n\n\n\n Solutions"
  },
  {
    "objectID": "sessions_companion/survey_basic.fr.html#aller-plus-loin",
    "href": "sessions_companion/survey_basic.fr.html#aller-plus-loin",
    "title": "Enquête standardisée sur la mortalité",
    "section": "Aller plus loin",
    "text": "Aller plus loin\n\nExercices supplémentaires\n\nPrécédemment, nous avons calculé le taux de mortalité excédentaire, c’est-à-dire la différence entre la mortalité observée et la mortalité de référence. Un autre indicateur que nous présentons souvent est le nombre de décès excédentaires observés pendant la période de rappel. Comment le calculeriez-vous ?\nUtilisez {ggplot2} pour créer un graphique à barres représentant les décès au fil du temps.\nConsultez la documentation relative à {srvyr} et voyez si vous pouvez utiliser survey_mean() pour calculer la mortalité proportionnelle par cause de décès.\nUtilisez {gt} pour créer un tableau attrayant présentant les données de mortalité proportionnelle que vous avez générées ci-dessus.\nDans la section consacrée au nettoyage, nous avons corrigé les cas où une personne était née pendant la période de rappel mais avait un âge &gt; 0. Comment cela aurait-il pu être évité avec une contrainte Kobo ?"
  },
  {
    "objectID": "sessions_companion/surveillance_companion.fr.html",
    "href": "sessions_companion/surveillance_companion.fr.html",
    "title": "Surveillance",
    "section": "",
    "text": "Travailler les compétences acquises dans les deux modules FETCH-R (importation, nettoyage et visualisation des données).\nAnalyser des données de surveillance rougeole pour détecter les alertes et aider à prioriser quelles alertes doivent être approfondies avec une enquête sur le terrain."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.fr.html#objectifs",
    "href": "sessions_companion/surveillance_companion.fr.html#objectifs",
    "title": "Surveillance",
    "section": "",
    "text": "Travailler les compétences acquises dans les deux modules FETCH-R (importation, nettoyage et visualisation des données).\nAnalyser des données de surveillance rougeole pour détecter les alertes et aider à prioriser quelles alertes doivent être approfondies avec une enquête sur le terrain."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.fr.html#introduction",
    "href": "sessions_companion/surveillance_companion.fr.html#introduction",
    "title": "Surveillance",
    "section": "Introduction",
    "text": "Introduction\nCette session accompagne l’étude de cas Réponse d’urgence contre la rougeole dans la région du Katanga (RDC) du module FETCH Surveillance. Ça n’a probablement pas beaucoup de sens d’essayer de le suivre sans les documents et les discussions de l’étude.\nEn ce qui concerne la partie R du module surveillance, nous nous appuierons sur les compétences acquises tout au long des modules FETCH-R, et introduirons quelques nouvelles fonctions utiles pour le nettoyage et les analyses.\n\n\n\n\n\n\nTip\n\n\n\nN’hésitez pas à checker vos notes, vos scripts ou les tutoriels des sessions précédentes pour vous rafraîchir sur le fonctionnement de certaines fonctions quand vous en éprouvez le besoin."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.fr.html#mise-en-place-question-2",
    "href": "sessions_companion/surveillance_companion.fr.html#mise-en-place-question-2",
    "title": "Surveillance",
    "section": "Mise en place (Question 2)",
    "text": "Mise en place (Question 2)\nComme cette session fait partie d’un module spécifique, vous allez créer un nouveau projet RStudio. Jetez un coup d’œil à la session principale si vous ne vous rappelez plus comment faire.\n\nCréer le projet\n\n\nCréez un dossier surveillance_case_study sur votre ordinateur, associé avec le module Surveillance du FETCH. Ajoutez les sous dossiers suivants à l’intérieur :\n\n\n📁 data\n\n📁 clean\n📁 raw\n\n📁 R\n📁 outputs\n\n\nCréez un projet RStudio à la racine du dossier surveillance_case_study.\nSi vous n’avez pas encore les données, téléchargez-les.\n\n\n\n\n Télécharger les données brutes\n\n\n\n 4. Dézippez l’archive si vous venez de télécharger les données. Quelle que soit la source, enregistrez les deux fichiers Excel dans le sous dossier data/raw.  5. Créez un nouveau script import_nettoyage.R et enregistrez-le dans le sous dossier R. Ajoutez les métadonnées et une section pour charger les paquets {here}, {rio}, et {tidyverse}.\n\n\n\nImporter les données\nRappel de l’étude de cas : vous avez demandé l’accès aux données de surveillance de routine et aux données de laboratoire au MSP de la RDC. Le ministère a accepté de les partager avec vous toutes les semaines. Vous recevez le premier fichier à la semaine 20 en 2022 (note : les données sur lesquelles nous travaillerons sont simulées).\n\nSi vous ne l’avez pas déjà fait, ouvrez les deux fichiers dans un tableur (Excel ou autre) pour les inspecter avant l’importation.\n\nLe jeu de données de surveillance a l’air facile à importer. En revanche, le jeu de données laboratoire pourrait vous inquiéter, avec ses lignes supplémentaires avant les données… Heureusement, la fonction import() que nous utilisons a un argument skip qui permet de gérer ce cas courant :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\nimport(\n  here(\"data\", \"raw\", \"fichier_exemple.xlsx\"), \n  skip = 3  # Sauter les trois premières lignes, l'import commence à la ligne 4\n) \n\n\n\nAjoutez une section pour l’import des données à votre script.\nImportez le jeu de données surveillance et stockez le dans un objet df_surv_brut. Ensuite, importez le jeu de données laboratoire et stockez le dans un objet df_labo_brut.\nVérifiez que l’importation s’est bien passée pour les deux data frames (vous avez plusieurs outils à votre disposition : Viewer, dimensions de l’objet, haut et bas du data frame…)."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.fr.html#nettoyage-question-2-and-3",
    "href": "sessions_companion/surveillance_companion.fr.html#nettoyage-question-2-and-3",
    "title": "Surveillance",
    "section": "Nettoyage (Question 2 and 3)",
    "text": "Nettoyage (Question 2 and 3)\nMaintenant que les données sont importées, nous pouvons effectuer quelques vérifications dessus, et les nettoyer.\n\nDonnées de surveillance (Q2)\n\nInspection rapide\nPendant l’étude de cas vous n’aurez peut-être pas le temps d’inspecter et nettoyer toutes les colonnes. Nous vous proposons donc de vous focaliser sur quelques colonnes clés : zone_sante, semaine, totalcas et totaldeces.\n\n\n\n\n\n\nNote\n\n\n\nSi vous revenez sur le tutoriel plus tard ou finissez en avance, n’hésitez pas à vérifier la qualité des autres variables, et à recouper les informations de différentes colonnes. Nous vous renvoyons à la discussion lors de l’étude ou aux documents du module de gestion des données pour des idées de vérifications à effectuer.\n\n\n\nAjoutez une section pour l’exploration et le nettoyage des données de surveillance dans votre script.  Maintenant, explorez le data frame et répondez aux questions suivantes :\n\nQuels sont les noms des colonnes ?\nCombien de provinces y a-t-il dans le jeu de données actuel ? Cela correspond-il à ce que vous attendez ?\nCombien de zones de santé y a-t-il dans le jeu de données ? Cela correspond-il à ce que vous attendez ?\nQuel est la plage des semaines ?\nQuelle est la valeur minimale de totalcas ?\nQuel est le maximum de totaldeces ?\nRemarquez-vous des données manquantes pour les colonnes ci-dessus ? Les chaînes de caractère (rappel : le texte) sont-elles propres ?\n\n\n\n\nNettoyer les chaînes de caractères\nMaintenant que nous avons une meilleure idée de l’état des données, nettoyons-les. Nous allons écrire un pipeline de nettoyage (ou chaîne de commandes) comme dans les modules R précédents (voir votre code à la fin du module de nettoyage).\n\n\n\n\n\n\nTip\n\n\n\nPour faciliter le débogage de la chaîne de commandes, ajoutez et testez les étapes unes par unes !\n\n\nNous allons améliorer un peu les colonnes de texte afin d’éliminer des problèmes potentiels :\n\npasser tout en minuscules (homogénéise)\nsupprimer les espaces surnuméraires (éventuels)\nremplacer - et les espaces par _.\n\nPeut être que vous n’aurez pas le temps de faire ces étapes pour toutes les colonnes. Pour commencer, choisissez une de ces deux colonnes : zone_sante ou prov pour appliquer les instructions. Vous pourrez faire les autres plus tard.\n\nCommencez un pipeline de nettoyage avec un mutate() pour transformer la colonne de votre choix en minuscules.\n\nNous allons maintenant voir deux petites fonctions très utiles pour le nettoyage du texte. La première est la fonction str_squish() du paquet {stringr} (la page d’aide), qui supprime les espaces au début ou à la fin des chaînes de caractères, et les espaces surnuméraires ou qu’ils soient :\n\nexemples &lt;- c(\" Espaces au début et à la fin     \",\n              \"Espaces     multiples\",\n              \" Tous les     problèmes  \")\n\nstr_squish(exemples)\n\n[1] \"Espaces au début et à la fin\" \"Espaces multiples\"           \n[3] \"Tous les problèmes\"          \n\n\nL’autre fonction, str_replace (également du paquet {stringr}) remplace un bout de texte dans une chaîne de caractères par un autre bout de texte, sans surprise. L’argument pattern accepte le texte à remplacer, et l’argument replacement le texte à utiliser comme remplacement.\n\nstr_replace(\n  \"HAUT-KATANGA\",    # Le texte sur lequel on travaille (peut être une colonne)\n  pattern = \"-\",     # Le bout à remplacer\n  replacement = \"_\"  # Le remplacement\n)\n\n[1] \"HAUT_KATANGA\"\n\n\n\nAjoutez des lignes à votre mutate pour, sur la colonne de votre choix :\n\nNettoyer les espaces\nChanger les - et les espaces en _ (deux étapes)\n\nLe début d’au moins une des colonnes devrait ressembler à :\n\n\n  pays     province     zone_sante  maladie\n1  rdc haut_katanga mufunga_sampwe rougeole\n2  rdc haut_katanga        sakania rougeole\n3  rdc haut_katanga        mitwaba rougeole\n4  rdc haut_katanga kilela_balanda rougeole\n5  rdc haut_katanga         likasi rougeole\n6  rdc haut_katanga         kikula rougeole\n\n\nStockez le résultat dans un data frame df_surv.\n\n\n\nEnregistrer les données nettoyées\n\nUtilisez le paquet {rio} pour exporter df_surv vers un fichier .rds appelé data_ids_2022-20_clean dans le sous dossier data/clean de votre projet.\n\n\n\n\nDonnées labo (Q2)\nNous allons suivre les mêmes étapes pour le jeu de données laboratoire. Nous nous focaliserons sur les colonnes zone_sante, igm_rougeole et igm_rubeole.\n\nInspection rapide\n\nInspectez les colonnes mentionnées, et les dimensions du data frame.\nQuelles sont les catégories des colonnes igm_rougeole et igm_rubeole ? Quel type de nettoyage sera à effectuer dessus ?\n\n\n\nNettoyage et recodage\n\n\nDémarrez un nouveau pipeline de nettoyage pour les données labo. Choisissez une colonne de texte et passez là en minuscules, puis supprimez les espaces surnuméraires. Enfin, remplacez les espaces et les - par _.\nRecodez au moins une des colonnes igm_rougeole ou igm_rubeole pour que les catégories soient negatif, positif et indeterminé.\nStockez la version nettoyée dans un data frame df_labo\n\nL’en-tête des colonnes nettoyées devrait maintenant être :\n\n\n   zone_sante igm_rougeole igm_rubeole\n1     kambove      negatif     negatif\n2     kambove      negatif     negatif\n3     kambove      negatif     positif\n4     kambove      negatif     negatif\n5     kambove      negatif     positif\n6     kambove      negatif     negatif\n7     kambove      negatif     negatif\n8     kambove      negatif     positif\n9      manika      negatif     negatif\n10  kamalondo      negatif     negatif\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nVous pouvez utiliser la fonction case_when() pour recoder les colonnes contenant les résultats des tests anticorps.\n\n\n\n\n\nEnregistrer les données nettoyées\n\nExportez le data frame df_labo vers un fichier .rds appelé df_labo_2022-w20_clean dans le sous dossier data/clean de votre projet.\n\n\n\n\nAller plus loin\nVous êtes arrivés au bout de la question 2. Si vous avez terminé en avance, utilisez les fonctions vues pour nettoyer les autres colonnes de texte dans les deux data frames, et recodez les deux colonnes IGM dans les données labo.\nSi vous avez encore du temps, inspectez vos données plus avant :\n\nAffichez la zone de santé pour laquelle les totaux par groupe d’âge sont différents de la colonne total (pour les cas, puis pour les décès)\nEst-ce qu’il y a une ZS où le nombre de décès est plus élevé que le nombre de cas ?\nY a-t-il des lignes dupliquées (entièrement dupliquées, ou plusieurs valeurs pour la zone de santé et la semaine) ?\nY a-t-il des nombres de cas que vous estimez aberrants ?\n\n\n\nDonnées de surveillance complétées (Q3)\nDurant l’inspection des données vous avez du vous rendre compte qu’il y a des semaines manquantes pour certaines ZS dans les données de surveillance. Normalement, vous avez discuté les raisons possibles et des problèmes associés en plénière. Dans ce tutoriel, nous allons fournir le code pour compléter le data frame surveillance pour que toutes les ZS aient toutes les semaines (en faisant l’hypothèse que les semaines manquantes n’ont pas eu de cas ou de décès).\nNous utiliserons la fonction complete() du paquet {tidyr} pour ajouter les lignes manquantes et remplir les colonnes contenant des nombres (totalcas et totaldeces) avec des zéros. A cause des contraintes de temps nous allons vous donner le code, mais quelques exemples et explications seront donnés dans la section Aller plus loin, que vous pourrez lire quand vous aurez le temps.\n\n\nCommencez un nouveau pipeline à partir de df_surv et ne conservez dedans que les colonnes province, zone_sante, semaine et totalcas.\nAjoutez une nouvelle étape à votre pipeline et collez le code ci-dessous pour compléter le data frame :\n\n\ncomplete(\n  # On travaille sur les combinaisons existantes de province et ZS\n  nesting(province, zone_sante),\n  \n  # On voudra toutes les semaines entre le minimum (1) et le maximum (20) de la colonne semaine\n  semaine = seq(min(semaine, na.rm = TRUE), \n                max(semaine, na.rm = TRUE)),\n  \n  # Remplir les nouvelles semaines de zeros pour ces colonnes :\n  fill = list(totalcas   = 0, \n              totaldeces = 0\n  )\n) \n\n\nStockez le résultat dans un data frame appelé df_surv_sem, qui devrait ressembler à :\n\n\n\n# A tibble: 10 × 5\n   province     zone_sante semaine totalcas totaldeces\n   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1 haut_katanga kafubu           1        0          0\n 2 haut_katanga kafubu           2        0          0\n 3 haut_katanga kafubu           3        0          0\n 4 haut_katanga kafubu           4        0          0\n 5 haut_katanga kafubu           5        0          0\n 6 haut_katanga kafubu           6        0          0\n 7 haut_katanga kafubu           7        0          0\n 8 haut_katanga kafubu           8        0          0\n 9 haut_katanga kafubu           9        0          0\n10 haut_katanga kafubu          10        0          0\n\n\n\nexportez ce data frame dans un fichier .rds appelé data_ids_2022-w20_weeks_clean dans le sous dossier data/clean de votre projet.\n\n\n\n\nAller plus loin\nC’est la fin de la question 3. Si vous terminez en avance, finissez l’inspection des données, listez les problèmes et nettoyez les colonnes que vous savez nettoyer avant de réexporter. Si c’est fait, lisez les eplications sur la fonction complete() et allez explorer sa page d’aide."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.fr.html#définir-les-alertes-question-4",
    "href": "sessions_companion/surveillance_companion.fr.html#définir-les-alertes-question-4",
    "title": "Surveillance",
    "section": "Définir les alertes (Question 4)",
    "text": "Définir les alertes (Question 4)\n\nPréparer le jeu de données\nNous allons continuer la préparation de data frames prêts pour l’analyse.\n\n\nSi vous n’avez pas eu le temps de nettoyer la zone de santé et la province dans les deux data frames, et les deux colonnes IGM dans le jeu de données labo, vous pouvez importer les jeux de données nettoyés :\n\n\n\n\n Télécharger données propres\n\n\n\n Dézippez l’archive et importez les données dans le sous dossier data/clean\n\nCréez un script analyse_surveillance.R dans le sous dossier R. Ajoutez les métadonnées, et une section pour importer les paquets {here}, {rio}, {tidyverse}, {lubridate} et {zoo}.\nAjoutez une section d’import des données propres et importez les fichiers .rds dans R en utilisant la fonction import() comme d’habitude (soit les vôtres, soit ceux que vous venez de télécharger). Assignez ces données nettoyées aux data frames df_surv, df_labo and df_surv_sem.\n\n\n\nSélection des ZS\nPour simplifier le travail nous allons nous focaliser sur quatre zones de santé : Dilolo, Kampemba, Kowe, et Lwamba.\n\nCommencez une nouvelle chaîne de commande à partir du data frame df_surv_sem. La première étape est de filtrer les données pour ne conserver que les zones de santé Dilolo, Kampemba, Kowe, et Lwamba.\n\n\n\nIndicateur hebdomadaire\nNotre premier indicateur regarde si une zone de santé a 20 cas suspects ou plus dans une semaine. Cet indicateur est dichotomique et ne prend en compte que les données d’une zone de santé pour une semaine donnée (ça tombe bien, ça correspond aux lignes du data frame).\n\nAjoutez un mutate() à votre chaîne pour créer une colonne cas20 qui contient la valeur 1 si une ZS a 20 cas ou plus cette semaine-là, et 0 sinon.\n Le début du data frame ressemble à ça :\n\n\n# A tibble: 10 × 6\n   province     zone_sante semaine totalcas totaldeces cas20\n   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 haut_katanga kampemba         1       75          0     1\n 2 haut_katanga kampemba         2       42          0     1\n 3 haut_katanga kampemba         3       46          0     1\n 4 haut_katanga kampemba         4       50          0     1\n 5 haut_katanga kampemba         5       43          0     1\n 6 haut_katanga kampemba         6       33          0     1\n 7 haut_katanga kampemba         7       45          0     1\n 8 haut_katanga kampemba         8       52          0     1\n 9 haut_katanga kampemba         9       38          0     1\n10 haut_katanga kampemba        10       46          0     1\n\n\n\n\n\nIndicateur cumulé\nNotre second indicateur regarde si une zone de santé compte plus de 35 cas suspects cumulés en trois semaines. C’est un peu plus compliqué à calculer que l’indicateur hebdomadaire : pour chaque zone de santé, il faut calculer la somme des cas par fenêtres de trois semaines, mais les groupes ne sont pas fixes, ils glissent dans le temps. Nous entrons ici dans le domaine des moyennes/sommes/etc. mobiles ou glissantes…\n\nSomme cumulée\nNous allons utiliser la fonction rollapply() du paquet {zoo} pour calculer la somme cumulée car elle est polyvalente et puissante. Comme son nom l’indique, la fonction rollapply() applique une fonction de manière glissante (roll peut être traduit ici en “rouler”) à un vecteur ou à une colonne d’un data frame.\nComme nous sommes contraint par le temps, nous allons vous fournir ici le code pour calculer la somme cumulée, et nous vous donnerons plus de détails sur la fonction dans la section Aller plus loin que vous pourrez lire quand vous aurez le temps.\nVoici comment utiliser la fonction pour une zone de santé :\n\n# Crée un mini data frame pour l'exemple\nexemple_df = data.frame(\n  province   = \"Haut Katanga\",\n  zone_sante = \"Dilolo\",\n  semaine    = 1:10,\n  totalcas   = rep(1, times = 10))\n\nexemple_df \n\n       province zone_sante semaine totalcas\n1  Haut Katanga     Dilolo       1        1\n2  Haut Katanga     Dilolo       2        1\n3  Haut Katanga     Dilolo       3        1\n4  Haut Katanga     Dilolo       4        1\n5  Haut Katanga     Dilolo       5        1\n6  Haut Katanga     Dilolo       6        1\n7  Haut Katanga     Dilolo       7        1\n8  Haut Katanga     Dilolo       8        1\n9  Haut Katanga     Dilolo       9        1\n10 Haut Katanga     Dilolo      10        1\n\nexemple_df |&gt; \n  mutate(cas_cumu = rollapply(\n    data  = totalcas,   # La colonne cible\n    width = 3,          # La taille de la fenêtre  \n    FUN   = sum,        # La fonction à appliquer, ici la somme\n    align = \"right\",    # On cumule les valeurs passées jusqu'à présent\n    partial = TRUE,     # Les somme partielles sont autorisées\n    na.rm = TRUE        # Argument en plus à passer à la fonction sum()\n  )\n  )\n\n       province zone_sante semaine totalcas cas_cumu\n1  Haut Katanga     Dilolo       1        1        1\n2  Haut Katanga     Dilolo       2        1        2\n3  Haut Katanga     Dilolo       3        1        3\n4  Haut Katanga     Dilolo       4        1        3\n5  Haut Katanga     Dilolo       5        1        3\n6  Haut Katanga     Dilolo       6        1        3\n7  Haut Katanga     Dilolo       7        1        3\n8  Haut Katanga     Dilolo       8        1        3\n9  Haut Katanga     Dilolo       9        1        3\n10 Haut Katanga     Dilolo      10        1        3\n\n\nOk, mais nous voudrions utiliser cette fonction dans un data frame qui contient plusieurs zones de santé, et faire la somme cumulée par zone de santé. Ce n’est pas si compliqué : nous allons trier notre jeu de données par zone de santé et semaine, puis utiliser l’argument .by dans le mutate pour effectuer les actions par zone de santé.\n\n\n\n\n\n\nNote\n\n\n\nRappelez-vous, nous avons déjà vu le .by. Nous l’avons utilisé au sein de la fonction summarize() lors de la session sur les tableaux agrégés pour faire des résumés par groupe.\nC’est la même idée aujourd’hui, sauf qu’au lieux d’utiliser une fonction qui ne renvoie qu’une seule valeur par groupe (summarize()) nous allons utiliser une fonction qui retourne une valeur par ligne (mutate()), mais prendra en compte les informations du groupe.\nPour petit rappel de comment summarize() + .by fonctionne, voici comment nous calculons le nombre total de cas suspects et décès par province :\n\ndf_surv_sem |&gt; \n  summarize(\n    .by = province,  # Fait les choses PAR province\n    cas_tot   = sum(totalcas, na.rm = TRUE),\n    deces_tot = sum(totaldeces, na.rm = TRUE)\n  )\n\n# A tibble: 4 × 3\n  province     cas_tot deces_tot\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 haut_katanga    5948        34\n2 haut_lomami     6928        70\n3 lualaba         1485         3\n4 tanganyika      7836       137\n\n\n\n\n\n\nAjoutez une étape à votre pipeline pour trier les données par province, zone de santé et semaine (dans cet ordre) avec la fonction arrange(), qui est une fonction de tri du package {dplyr}:\n\n\ndf_surv_sem |&gt;\n  arrange(province, zone_sante, semaine)\n\n\nAjoutez ensuite le code pour calculer la somme cumulée :\n\n\nmutate(\n  .by = c(province, zone_sante),\n  cas_cumu = rollapply(\n    data  = totalcas,   # La colonne cible\n    width = 3,          # La taille de la fenêtre  \n    FUN   = sum,        # La fonction à appliquer, ici la somme\n    align = \"right\",    # On cumule les valeurs passées jusqu'à présent\n    partial = TRUE,     # Les somme partielles sont autorisées\n    na.rm = TRUE        # Argument en plus à passer à la fonction sum()\n  )\n)\n\n\nMaintenant que la somme cumulée est calculée, il ne nous reste plus qu’à calculer l’indicateur dichotomique qui résume les données cumulées pour chaque semaine, puis un indicateur combiné qui résume les deux indicateurs précédents.\n\n\nAjoutez une nouvelle étape à votre pipeline pour créer une colonne cas_cumu35 qui contient 1 si la somme cumulée est supérieure ou égale à 35, et 0 sinon.\nDans le même mutate, ajoutez une colonne alerte, qui est 1 si ’l’indicateur cas20 OU l’indicateur cas_cumu35 est 1 et 0 sinon. Pour lest logique vous devrez utiliser l’opérateur | qui représente le OU logique (renverra TRUE si au moins une des conditions est remplie, à fortiori les deux).\nAssignez le résultat à un data frame data_alerte.\n\nCe data frame ressemble à ceci (quelques colonnes sont cachées pour l’affichage) :\n\n\n# A tibble: 10 × 7\n   zone_sante semaine totalcas cas20 cas_cumu cas_cumu35 alerte\n   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 kampemba         1       75     1       75          1      1\n 2 kampemba         2       42     1      117          1      1\n 3 kampemba         3       46     1      163          1      1\n 4 kampemba         4       50     1      138          1      1\n 5 kampemba         5       43     1      139          1      1\n 6 kampemba         6       33     1      126          1      1\n 7 kampemba         7       45     1      121          1      1\n 8 kampemba         8       52     1      130          1      1\n 9 kampemba         9       38     1      135          1      1\n10 kampemba        10       46     1      136          1      1\n\n\n\n\n\n\n\nZones de santé en alerte\nMaintenant que la préparation est finie, nous pouvons enfin regarder quelles zones de santé sont en alerte dans notre jeu de données, en particulier à la semaine 20 (les données les plus récentes selon l’étude de cas).\n\nAffichez les données filtrées pour ne voir que ce qui se passe à la semaine 20. Quelle zones sont en alerte en ce moment ?\nCréez un vecteur zs_alerte qui contient le nom des zones de santé qui sont en alerte à la semaine 20. Ce vecteur sera utilisé par la suite pour filtrer les données lors de l’analyse."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.fr.html#tracer-la-courbe-épidémique-question-4",
    "href": "sessions_companion/surveillance_companion.fr.html#tracer-la-courbe-épidémique-question-4",
    "title": "Surveillance",
    "section": "Tracer la courbe épidémique (Question 4)",
    "text": "Tracer la courbe épidémique (Question 4)\nNous allons à présent tracer la courbe épi pour les zones en alerte à la semaine 20. Nous pouvons réutiliser le code vu lors de la session sur les courbes épidémiques : nous utiliserons le paquet ggplot() et la fonction geom_col() pour créer un diagramme qui montre la distribution des cas par semaine. Petite nouveauté : par le passé nous avions une liste linaire où une ligne correspond à un patient, donc un cas. Aujourd’hui nous avons des données déjà agrégées par semaine et zone de santé : nul besoin de compter le nombre de cas nous même.\n\nTracez la courbe épidémique pour une des zones de santé en alerte.\n Le graphe devrait ressembler à ceci (peut être avez-vous choisi l’autre zone) :\n\n\n\n\n\n\n\n\n\n\nNouvelle fonction utile, la fonction facet_wrap() permet de créer plusieurs graphiques d’un seul coup rassemblés en une seule figure. Consultez le satellite sur le faceting si vous voulez en savoir plus.\n\ndata_alerte |&gt;\n  filter(zone_sante %in% zone_sante_alertee) |&gt;\n  ggplot(aes(x = semaine, \n             y = totalcas)) + \n  geom_col(fill = \"#2E4573\") + \n  theme_bw(base_size = 16) + \n  labs(x = \"Semaine\",\n       y = \"N cas\",\n       title = \"Zone de santé de Kampemba (en alerte)\") +\n  facet_wrap(vars(zone_sante))   # Un graphe par zone de santé"
  },
  {
    "objectID": "sessions_companion/surveillance_companion.fr.html#indicateurs-clés-question-6",
    "href": "sessions_companion/surveillance_companion.fr.html#indicateurs-clés-question-6",
    "title": "Surveillance",
    "section": "Indicateurs clés (Question 6)",
    "text": "Indicateurs clés (Question 6)\nNous pouvons calculer plus d’indicateurs sur les zones de santé pour nous aider à décider laquelle devrait faire l’objet d’une enquête (vu que vous n’avez pas le temps ni les ressources pour investiguer les deux).\n\n\n\n\n\n\nTip\n\n\n\nCette partie utilise les fonctions d’aggrégation vues quand nous avons appris à faire des tableaux résumés. N’hésitez pas à vous rafraîchir si besoin.\n\n\n\nPremière semaine en alerte\n\nUtilisez la fonction summarize() pour afficher les premières semaines où les ZS sont passées en alerte. Quelle zone de santé a été en alerte en premier ?\n\n\n\nIndicateurs des données de surveillance\nReprenons le jeu de données de surveillance avec toutes les colonnes, df_surv.\n\n\nAjoutez-lui une colonne cas_moins_5ans qui contient le nombre total de cas rapportés ayant moins de cinq ans.\nDérivez, pour chaque zone en alerte, les indicateurs suivants, organisés en un seul tableau :\n\n\nLe nombre de cas\nLe nombre de morts\nLe nombre de moins de cinq ans\nLa mortalité en pourcentage\nLe pourcentage de moins de cinq ans.\n\nLe résultat ressemble à :\n\n\n  zone_sante n_cas n_deces n_moins_5 p_moins_5 mortalite\n1   kampemba   730       0       544  74.52055   0.00000\n2     lwamba   256       2       233  91.01562   0.78125\n\n\n\n\n\nIndicateurs des données labo\nIntéressons-nous à présent aux données de laboratoire pour compléter les indicateurs précédents.\n\nPour chacune des zones en alerte, dérivez les indicateurs suivants :\n\nLe nombre de patients testés pour la rougeole\nLe nombre de positifs pour la rougeole\nLa proportion de positifs pour la rougeole\nLe nombre de patients testés pour la rubéole\nLe nombre de positifs pour la rubéole\nLa proportion de positifs pour la rubéole\n\nLe résultat devrait ressembler à ceci :\n\n\n  zone_sante n_test_roug n_test_roug_pos positivite_roug n_test_rub\n1     lwamba          10               5       0.5000000         10\n2   kampemba          14               4       0.2857143         14\n  n_test_rub_pos positivite_rub\n1              0     0.00000000\n2              1     0.07142857\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSi vous avez du mal avec cette question, rafraîchissez-vous sur les résumés conditionnels."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.fr.html#cest-fini",
    "href": "sessions_companion/surveillance_companion.fr.html#cest-fini",
    "title": "Surveillance",
    "section": "C’est fini !",
    "text": "C’est fini !\nBravo, vous êtes venus à bout de ce tutoriel !\n\n\n\n Solutions"
  },
  {
    "objectID": "sessions_companion/surveillance_companion.fr.html#sec-going-further",
    "href": "sessions_companion/surveillance_companion.fr.html#sec-going-further",
    "title": "Surveillance",
    "section": "Aller plus loin",
    "text": "Aller plus loin\n\nExplications sur complete()\nDans le mini exemple ci-dessous la zone de santé de Kitenge n’a pas de ligne pour la semaine 2 :\n\n# Data frame simplifié, avec seulement trois semaines\nexemple_df = data.frame(\n  province   = c(\"haut_katanga\", \"haut_katanga\", \"haut_katanga\", \"haut_lomami\", \"haut_lomami\"),\n  zone_sante = c(\"likasi\", \"likasi\", \"likasi\", \"kitenge\", \"kitenge\"),\n  semaine    = c(1, 2, 3, 1, 3),\n  totalcas  = c(2, 1, 3, 1, 2))\n\nexemple_df\n\n      province zone_sante semaine totalcas\n1 haut_katanga     likasi       1        2\n2 haut_katanga     likasi       2        1\n3 haut_katanga     likasi       3        3\n4  haut_lomami    kitenge       1        1\n5  haut_lomami    kitenge       3        2\n\n\nNous pouvons utiliser le code suivant pour compléter toutes les zones de santé pour qu’elles aient toutes les semaines possible, ici les semaine de un à trois :\n\n# Compléte la semaine manquante à Kitenge\nexemple_df |&gt; \n  complete(\n    nesting(province, zone_sante), \n    semaine = seq(1, 3),            # Vecteur de 1 à 3\n    fill = list(totalcas = 0)       # Remplir avec des zéros (sinon, NA par défaut)\n  ) \n\n# A tibble: 6 × 4\n  province     zone_sante semaine totalcas\n  &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 haut_katanga likasi           1        2\n2 haut_katanga likasi           2        1\n3 haut_katanga likasi           3        3\n4 haut_lomami  kitenge          1        1\n5 haut_lomami  kitenge          2        0\n6 haut_lomami  kitenge          3        2\n\n\nMaintenant les deux zones de santé dans les deux provinces ont toutes les semaines possibles.\nVous vous demandez peut être pourquoi nous avons écrit nesting(province, zone_sante) au lieu de juste zone_sante. La raison est qu’il peut y avoir deux zones de santé avec le même nom dans des provinces différentes. Nous devons donc tenir compte de la colonne province. L’argument nesting() indique à la fonction de n’utiliser que les combinaisons existantes des deux colonnes dans le data frame.\n\n\n\n\n\n\nNote\n\n\n\nPetit encart pour voir ce qui se serait passé si nous avions passé les deux colonnes à la fonction complete() sans utiliser nesting() : la fonction aurait créé toutes les combinaisons possibles entre les catégories des colonnes province et zone_sante, ce qui n’a pas de sens dans notre cas.\n\nexemple_df |&gt; \n  complete(\n    province, zone_sante, \n    semaine = seq(1, 3),  # Vecteur de 1 à 3\n    fill = list(totalcas = 0)\n  ) \n\n# A tibble: 12 × 4\n   province     zone_sante semaine totalcas\n   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 haut_katanga kitenge          1        0\n 2 haut_katanga kitenge          2        0\n 3 haut_katanga kitenge          3        0\n 4 haut_katanga likasi           1        2\n 5 haut_katanga likasi           2        1\n 6 haut_katanga likasi           3        3\n 7 haut_lomami  kitenge          1        1\n 8 haut_lomami  kitenge          2        0\n 9 haut_lomami  kitenge          3        2\n10 haut_lomami  likasi           1        0\n11 haut_lomami  likasi           2        0\n12 haut_lomami  likasi           3        0\n\n\n\n\nComme la base de données va être mise à jour chaque semaine, il serait pratique de choisir automatiquement la plage des semaines qui doivent être présentes dans les données. Pour ça, il nous suffit de remplacer les valeurs que nous avons codé en dur par la plus petite semaine existante dans les données et la plus grande :\n\nexemple_df |&gt; \n  complete(\n    nesting(province, zone_sante),\n    semaine = seq(min(semaine, na.rm = TRUE),   # Vecteur allant du minimum\n                  max(semaine, na.rm = TRUE)),  # au maximum de la colonne `semaine`\n    fill = list(totalcas = 0)\n  ) \n\n# A tibble: 6 × 4\n  province     zone_sante semaine totalcas\n  &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 haut_katanga likasi           1        2\n2 haut_katanga likasi           2        1\n3 haut_katanga likasi           3        3\n4 haut_lomami  kitenge          1        1\n5 haut_lomami  kitenge          2        0\n6 haut_lomami  kitenge          3        2\n\n\n\n\nExplications sur rollaply()\nPour calculer la somme cumulée des cas sur trois semaine il nous faut appliquer (apply en anglais) la fonction sur des fenêtres glissantes de trois semaines.\n\n# Vecteur pour les exemples\nexemple_vect &lt;- rep(1, time = 10)\nexemple_vect\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\nrollapply(\n  data  = exemple_vect,\n  width = 3,       # Largeur de la fenêtre\n  FUN   = sum,     # Fonction à appliquer (ici, la somme)\n  align = \"right\"  # Calcul dans le passé\n)\n\n[1] 3 3 3 3 3 3 3 3\n\n\nNous avons fourni en entrée un vecteur de 10 valeurs et obtenu en sortie un vecteur contenant les sommes. Manifestement, la fonction a fait des choix sur comment traiter les extrémités, et le résultat est plus court que le vecteur d’entré. Ce dernier point est un problème si l’on veut utiliser la fonction dans un mutate(), qi crée des colonnes dans un data frame, où toutes les colonnes ont la même taille.\nIl est possible de contrôler le comportement de la fonction aux extrémités :\n\nRemplir les valeurs pour lesquelles il n’y a pas assez de valeur dans la fenêtre avec des NA\nAutoriser des calculs partiels (ici, des sommes partielles, en d’autres termes des valeurs ne représentent pas la somme sur trois semaines).\n\nL’argument fill = NA remplit les valeurs manquantes aux extrémités concernées avec des NA (dans notre cas, à gauche, vu que l’on a aligné la fenêtre à droite) :\n\nrollapply(\n  data  = exemple_vect,\n  width = 3,       # Largeur de la fenêtre\n  FUN   = sum,     # Fonction à appliquer (ici, la somme)\n  align = \"right\", # Calcul dans le passé\n  fill  = NA\n)\n\n [1] NA NA  3  3  3  3  3  3  3  3\n\n\nC’est souvent une façon raisonnable de gérer les valeurs aux extrémités où les fenêtres ne sont pas complètes. Néanmoins, dans notre cas, nous pouvons faire mieux. En effet, imaginons qu’il y ait 40 cas lors de la première semaine : même si nous n’avons pas de valeurs pour les deux semaines précédentes, l’alerte devrait être déclenchée ! Nous voudrions donc que la somme cumulée soit calculée dès la première semaine pour pouvoir détecter des alertes précoces ! L’argument partial = TRUE permet cela :\n\nrollapply(\n  data    = exemple_vect,\n  width = 3,        # Largeur de la fenêtre\n  FUN   = sum,      # Fonction à appliquer (ici, la somme)\n  align = \"right\",  # Calcul dans le passé\n  partial = TRUE    # Autorise les sommes partielles aux extrémités\n  )\n\n [1] 1 2 3 3 3 3 3 3 3 3\n\n\nC’est mieux comme ça pour notre cas d’usage.\n\n\n\n\n\n\nImportant\n\n\n\nGardez à l’esprit que les deux premières valeurs du vecteur (ou de la colonne) contiennent donc des sommes partielles. En conséquence, une absence d’alerte dans les deux premières semaines ne veut pas forcement dire grand chose.\n\n\nUn dernier point : rappelez-vous qu’il faut utiliser na.rm = TRUE pour ignorer les valeurs manquantes dans la plupart des opérations arithmétiques dans R.\nSi nous avions un vecteur un peu moins complet, nous aurions des problèmes :\n\nexemple_vect_na &lt;- c(1, 1, 1, NA, 1, 1)\n\nrollapply(\n  data  = exemple_vect_na,\n  width = 3,       # Largeur de la fenêtre\n  FUN   = sum,     # Fonction à appliquer (ici, la somme)\n  align = \"right\", # Calcul dans le passé\n  partial = TRUE   # Autorise les sommes partielles aux extrémités\n)\n\n[1]  1  2  3 NA NA NA\n\n\nOups. Heureusement, nous pouvons passer l’argument na.rm = TRUE à la fonction rollapply() pour qu’elle le passe à la fonction sum().\n\nrollapply(\n  data  = exemple_vect_na,\n  width = 3,       # Largeur de la fenêtre\n  FUN   = sum,     # Fonction à appliquer (ici, la somme)\n  align = \"right\", # Calcul dans le passé\n  partial = TRUE,  # Autorise les sommes partielles aux extrémités\n  na.rm = TRUE     # Argument en plus non nommé à passer à sum()\n)\n\n[1] 1 2 3 2 2 2\n\n\nEnfin, quelues mots sur l’argument align. Il définit la position de la fenêtre glissante par rapport à la valeur en train d’être calculée. Par défaut la fenêtre est centrée autour de la valeur à cacluler : la valeur calculée i est la somme des valeurs i-1 (la valeur précédente) et la valeur i+1 (la valeur suivante).\nExemples des trois alignements (en mettant des valeurs manquantes aux extrémités pour voir plus facilement ce qui se passe) :\n\n# Alignement à gauche : la valeur est la somme des valeurs dans le futur\nrollapply(data  = c(5, 10, 1, 2, 5, 10),\n          width = 3, \n          FUN   = sum,\n          align = \"left\", \n          fill = NA)\n\n[1] 16 13  8 17 NA NA\n\n# Alignement centré : somme des valeurs de chaque côté\nrollapply(data  = c(5, 10, 1, 2, 5, 10),\n          width = 3, \n          FUN   = sum,\n          align = \"center\",\n          fill = NA)  # The default\n\n[1] NA 16 13  8 17 NA\n\n# Alignement à droite : somme des valeurs passées jusqu'à présent\nrollapply(data  = c(5, 10, 1, 2, 5, 10),\n          width = 3, \n          FUN   = sum,\n          align = \"right\",\n          fill = NA)\n\n[1] NA NA 16 13  8 17\n\n\nDans notre cas, nous voulons que la valeur pour une semaine donnée reflète cette semaine et les deux semaines précédentes, donc nous utilisons l’argument align = \"right\", pour calculer dans le passé.\n\n\n\n\n\n\nTip\n\n\n\nDans ce tutoriel nous avons appliqué la fonction sum() à des fenêtres de trois semaines pour calculer une somme cumulée. Mais le code peut être facilement modifié pour calculer une moyenne glissante sur une fenêtre de votre choix !\n\n\n\n\nFormatage des pourcentages\nLa fonction percent() du paquet {scales} formate une valeur ou un vecteur de valeurs en pourcentages.\n\nscales::percent(0.8556)\n\nIl y a un argument accuracy pour contrôler le nombre de décimales à afficher :\n\nscales::percent(0.8556,\n                accuracy = 0.1)\n\nVous pouvez fournir un vecteur (ou une colonne !) de proportions à la fonction pour afficher les valeurs en pourcentages, ce qui est plus lisible dans un tableau résumé.\n\n\n\n\n\n\nImportant\n\n\n\nLa colonne ainsi crée n’est plus une colonne numérique : l’ajout du signe % transforme la colonne en texte. Vous ne pourrez donc plus effectuer d’opérations arithmétiques dessus."
  },
  {
    "objectID": "index.fr.html",
    "href": "index.fr.html",
    "title": "{repicentre}",
    "section": "",
    "text": "Bienvenue à {repicentre}\nUne plateforme open source pour apprendre R dans les contextes humanitaires. Qu’aimeriez-vous faire ?\n\n\n\n\n\nApprendre Parcours linéaire en commençant par les bases  Start\n\n\n\n\n\nExplorer Catalogue complet de cours d’autoformation  Start\n\n\n\n\n\nRessources Ressources externes pour aller plus loin  Start"
  },
  {
    "objectID": "about.fr.html#salut",
    "href": "about.fr.html#salut",
    "title": "À Propos",
    "section": "Salut",
    "text": "Salut\nBienvenue sur {repicentre}, un site open source développé par Epicentre pour vous aider à apprendre R pour les contextes humanitaires. Le site est composé de tutoriels autodidactes et propose deux options principales d’apprentissage :\n\nLinéaire. Conçu pour les personnes n’ayant aucune expérience préalable de R, le cours linéaire vous guidera à travers les concepts de base de R en utilisant une étude de cas sur la rougeole au Tchad. Le cours couvre les concepts suivants :\n\nStructures de données et l’interface RStudio\nImportation de données\nManipulation de données\nNettoyage des données\nAgrégation de données\nVisualisation des données\n\nExploration. Si vous avez un peu plus d’expérience ou si vous recherchez un sujet particulier, n’hésitez pas à explorer la gamme complète des tutoriels. Les tutoriels sont classés par catégories et sont conçus pour être autonomes."
  },
  {
    "objectID": "about.fr.html#recommandations-et-demandes",
    "href": "about.fr.html#recommandations-et-demandes",
    "title": "À Propos",
    "section": "Recommandations et demandes",
    "text": "Recommandations et demandes\nY a-t-il un sujet sur lequel vous aimeriez voir un tutoriel qui n’est pas encore disponible ? C’est très bien ! N’hésitez pas à nous le faire savoir en ouvrant un “issue” sur le repo GitHub associé à ce site web. Si vous ne savez pas comment ouvrir un issue, veuillez contacter Cat Eisenhauer."
  },
  {
    "objectID": "about.fr.html#contribuer",
    "href": "about.fr.html#contribuer",
    "title": "À Propos",
    "section": "Contribuer",
    "text": "Contribuer\nVous souhaitez contribuer à la rédaction ou à la maintenance de tutoriels ? Incroyable ! Veuillez contacter Cat."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.fr.html",
    "href": "sessions_extra/weekly_epicurves.fr.html",
    "title": "Courbes épidémiques hebdomadaires",
    "section": "",
    "text": "Dans la session principale sur les grapiques, vous avez appris à tracer une courbe épidémique du nombre de cas journaliers :\n\n\n\n\n\n\n\n\n\nIci les données sont agrégées par jour, ce qui raisonnable si l’épidémie est de courte durée ou si vous souhaitez zoomer sur une période spécifique. Il nous arrivera néanmoins de souvent vouloir tracer des courbes hebdomadaires.\nDans cette tutoriel, nous apprendrons à agréger les données par semaine, à tracer le graphique et à améliorer les étiquettes de l’axe des abscisses.\nPrérequis : la session sur les courbes épidémiques."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.fr.html#objectifs",
    "href": "sessions_extra/weekly_epicurves.fr.html#objectifs",
    "title": "Courbes épidémiques hebdomadaires",
    "section": "",
    "text": "Dans la session principale sur les grapiques, vous avez appris à tracer une courbe épidémique du nombre de cas journaliers :\n\n\n\n\n\n\n\n\n\nIci les données sont agrégées par jour, ce qui raisonnable si l’épidémie est de courte durée ou si vous souhaitez zoomer sur une période spécifique. Il nous arrivera néanmoins de souvent vouloir tracer des courbes hebdomadaires.\nDans cette tutoriel, nous apprendrons à agréger les données par semaine, à tracer le graphique et à améliorer les étiquettes de l’axe des abscisses.\nPrérequis : la session sur les courbes épidémiques."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.fr.html#mise-en-place",
    "href": "sessions_extra/weekly_epicurves.fr.html#mise-en-place",
    "title": "Courbes épidémiques hebdomadaires",
    "section": "Mise en place",
    "text": "Mise en place\n\nNous utiliserons la même liste linéaire nettoyée que précédemment et qui peut être téléchargée ici :\n\n\n\n Télécharger les données\n\n\n\n Si ce n’est pas déjà fait, enregistrez le jeu de données dans data/clean puis créez un nouveau script appelé courbe_hebdo.R dans votre sous-dossier R (alternativement, vous pouvez rajouter une section au script sur les courbes épidémiques journalières).\n Si vous créez un nouveau script, ajoutez un en-tête approprié et chargez les paquets suivants : {here}, {rio}, {tidyverse} et {scales}. Importez ensuite les données propres (moissala_linelist_clean_FR.rds) dans R et enregistrez-les dans un objet df_linelist.\n\nAu cours du tutoriel, les exemples porteront sur les sorties et vous tracerez la courbe épidémique à partir de la date de début des symptômes."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.fr.html#données-hebdomadaires",
    "href": "sessions_extra/weekly_epicurves.fr.html#données-hebdomadaires",
    "title": "Courbes épidémiques hebdomadaires",
    "section": "Données hebdomadaires",
    "text": "Données hebdomadaires\nNous allons aborder deux façons d’agréger les données par semaine. Le concept de la première vous sera sans doute familier (semaines identifiées par leur numéros), mais nous nous concentrerons sur une méthode plus robuste (semaine identifiées par la date du premier jour de la semaine).\n\nNuméros de semaine\nLa manière la plus intuitive de d’agréger par semaine est d’utiliser des numéros de semaines, car les données du MSP sont souvent dans ce format. Vous avez sans doute créé de nombreuses courbes épidémiques dans ce format vous-mêmes.\nLa fonction isoweek() du paquet {lubridate} accepte une date (ou un vecteur de dates) et renvoie le numéro de semaine ISO.\n\nexemple_date &lt;- as.Date('2025-02-24')\n\nexemple_date\n\n[1] \"2025-02-24\"\n\nisoweek(exemple_date)\n\n[1] 9\n\n\nNous pouvons utiliser cette fonction pour créer une colonne sem_sortie_num dans nos données :\n\ndf_linelist &lt;- df_linelist |&gt; \n  mutate(sem_sortie_num = isoweek(date_sortie))\n\nLe début des colonnes date_sortie et sem_sortie_num ressemble à ceci (sans les NA) :\n\ndf_linelist |&gt; \n  tidyr::drop_na(date_sortie) |&gt; \n  select(date_sortie, sem_sortie_num) |&gt; \n  head()\n\n  date_sortie sem_sortie_num\n1  2022-08-18             33\n2  2022-08-28             34\n3  2022-09-03             35\n4  2022-09-12             37\n5  2022-09-10             36\n6  2022-09-18             37\n\n\n\nA vous de jouer. Utilisez les fonctions mutate() et isoweek() pour créer une nouvelle colonne dans votre data frame appelée sem_symptomes_num qui contient la semaine ISO associée à chaque date de début des symptômes. L’en-tête des colonnes date_debut et sem_symptomes_num devrait ressembler à ceci :\n\n\n  date_debut sem_symptomes_num\n1 2022-08-13                32\n2 2022-08-18                33\n3 2022-08-17                33\n4 2022-08-22                34\n5 2022-08-30                35\n6 2022-08-30                35\n\n\n\nNous pourrions maintenant utiliser count() sur cette colonne pour agréger les données par semaine, puis tracer le graphique avec {ggplot2} avec un code très similaire à la session principale.\nMalheureusement il y a un problème. Avec le numéro de semaine il y a une première semaine en 2022… mais aussi en 2023, 2024 etc. Dans le cas d’une épidémie courte qui n’aurait lieu qu’en 2022, cela ne poserait pas problème. Cependant, notre data frame contient des données de la région entière, et les dates s’étendent de 2022 à 2023. Donc si nous comptions le nombre de patient par numéro de semaine, le tableau suivant serait erroné :\n\n# FAUX\ndf_linelist |&gt; \n  count(sem_symptomes_num) |&gt; \n  head(10)\n\n   sem_symptomes_num  n\n1                  1 36\n2                  2 35\n3                  3 42\n4                  4 56\n5                  5 70\n6                  6 78\n7                  7 85\n8                  8 49\n9                  9 62\n10                10 81\n\n\nPour résoudre le problème nous pouvons stratifier par année :\n\ndf_linelist |&gt; \n  mutate(annee_symptomes = isoyear(date_debut)) |&gt; \n  count(annee_symptomes, sem_symptomes_num) |&gt; \n  head(10)\n\n   annee_symptomes sem_symptomes_num  n\n1             2022                32  1\n2             2022                33  2\n3             2022                34  1\n4             2022                35  8\n5             2022                36  8\n6             2022                37 10\n7             2022                38 17\n8             2022                39 17\n9             2022                40 19\n10            2022                41 16\n\n\nCes chiffres sont désormais corrects. Vous pourriez les représenter avec plusieurs mini graphes par année sur une même figure, ou simplement filtrer une année donnée et tracer la courbe avec les numéros de semaines sur l’axe des x. Dans le premier cas, cela donnerait ceci :\n\ndf_linelist |&gt; \n  mutate(annee_symptomes = isoyear(date_debut)) |&gt; \n  count(annee_symptomes, sem_symptomes_num) |&gt; \n  ggplot(aes(x = sem_symptomes_num,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  theme_classic(base_size = 16) +\n  facet_wrap(vars(annee_symptomes),  # Magie pour faire le graphe par année !\n             ncol = 1)\n\n\n\n\n\n\n\n\nSi vous n’avez pas lu le satellite sur facet_wrap(), ce n’est pas grave, voyez ce graphe comme une page de publicité pour la capacité de ggplot à faire des graphes multiples rapidement. Les explications sortent du cadre de ce tutoriel et nous allons vous montrer une autre façon d’agréger les données par semaine, qui est robuste aux données pluriannuelles.\n\n\nPremier jour de la semaine\nUne autre manière d’agréger par semaine est d’utiliser la fonction floor_date() (également du package {lubridate}), qui renvoie la première date d’une période donnée. Vous pouvez la considérer comme une sorte d’arrondi à la plus petite valeur, mais pour les dates.\nLa fonction a un argument unit pour choisir l’échelle de la période (semaine, mois…) et un argument week_start pour définir le premier jour de la semaine (les lundis sont 1).\n\ndf_linelist &lt;- df_linelist |&gt; \n  mutate(\n    sem_sortie_lundi = floor_date(date_sortie,\n                                  unit = \"week\",\n                                  week_start = 1)\n  )\n\nRegardons les différentes colonnes de plus près pour bien comprendre :\n\ndf_linelist |&gt; \n  select(date_sortie, sem_sortie_num, sem_sortie_lundi) |&gt;\n  arrange(date_sortie) |&gt;     # Trie par date\n  head(n = 10)\n\n   date_sortie sem_sortie_num sem_sortie_lundi\n1   2022-08-18             33       2022-08-15\n2   2022-08-28             34       2022-08-22\n3   2022-09-03             35       2022-08-29\n4   2022-09-10             36       2022-09-05\n5   2022-09-12             37       2022-09-12\n6   2022-09-12             37       2022-09-12\n7   2022-09-16             37       2022-09-12\n8   2022-09-17             37       2022-09-12\n9   2022-09-18             37       2022-09-12\n10  2022-09-19             38       2022-09-19\n\n\nPour aider à comprendre on peut calculer le jour de la semaine associé à chaque date en utilisant la fonction wday() (qui appartient aussi à {lubridate}, y a comme un thème 😉) [wday est une abréviation pour week day] :\n\ndf_linelist |&gt; \n  # Calcule le premier jour de la semaine\n  mutate(\n    jour_sortie = wday(date_sortie, \n                       label = TRUE, \n                       abbr = FALSE),\n    cest_bien_un_lundi  = wday(sem_sortie_lundi, \n                               label = TRUE, \n                               abbr = FALSE)) |&gt; \n  arrange(date_sortie) |&gt;      # Trie par date\n  select(date_sortie,\n         jour_sortie,\n         sem_sortie_num,\n         sem_sortie_lundi,\n         cest_bien_un_lundi) |&gt; \n  head(n = 10)\n\n   date_sortie jour_sortie sem_sortie_num sem_sortie_lundi cest_bien_un_lundi\n1   2022-08-18    Thursday             33       2022-08-15             Monday\n2   2022-08-28      Sunday             34       2022-08-22             Monday\n3   2022-09-03    Saturday             35       2022-08-29             Monday\n4   2022-09-10    Saturday             36       2022-09-05             Monday\n5   2022-09-12      Monday             37       2022-09-12             Monday\n6   2022-09-12      Monday             37       2022-09-12             Monday\n7   2022-09-16      Friday             37       2022-09-12             Monday\n8   2022-09-17    Saturday             37       2022-09-12             Monday\n9   2022-09-18      Sunday             37       2022-09-12             Monday\n10  2022-09-19      Monday             38       2022-09-19             Monday\n\n\nCeci illustre comment sem_sortie_num et sem_sortie_lundi sont deux façons de représenter une semaine donnée. Mais si les numéros de semaine ne sont pas uniques, les dates, elles, le sont !\n\nAjoutez une nouvelle instruction à votre mutate() pour créer la variable sem_symptomes_lundi qui contient le premier jour de la semaine pour la date d’apparition des symptômes. Le premier jour de la semaine est un lundi au Tchad.\n\n\n\n\n\n\n\nTip\n\n\n\nLisez la page d’aide de floor_date() pour connaître la liste des unités possibles.\n\n\n\n\nAgréger\nMaintenant que nous avons une variables qui identifie la semaine, nous pouvons enfin agréger nos données !\n\nComptez le nombre de patients par semaine de début des symptômes, en utilisant le début de la semaine pour identifier les semaines (sem_symptomes_lundi).\nVoici les dix premières lignes de ce à quoi il devrait ressembler :\n\n\n   sem_symptomes_lundi  n\n1           2022-08-08  1\n2           2022-08-15  2\n3           2022-08-22  1\n4           2022-08-29  8\n5           2022-09-05  8\n6           2022-09-12 10\n7           2022-09-19 17\n8           2022-09-26 17\n9           2022-10-03 19\n10          2022-10-10 16"
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.fr.html#tracer-le-graphique",
    "href": "sessions_extra/weekly_epicurves.fr.html#tracer-le-graphique",
    "title": "Courbes épidémiques hebdomadaires",
    "section": "Tracer le graphique",
    "text": "Tracer le graphique\nParfait. Nous pouvons maintenant passer nos données agrégées à la commande pour créer le graphique, en faisant quelques ajustements pour que le code précédent fonctionne.\n\nCréez un ggplot avec le même aspect que la courbé épidémique de la session principale, mais avec le premier jour de la semaine sur l’axe des abscisses. N’oubliez pas de mettre à jour les noms des axes !\nIl devrait ressembler à ceci :\n\n\n\n\n\n\n\n\n\n\nNotez que même si les étiquettes sur l’axe des abscisses sont des dates, une barre représente les données d’une semaine (sept jours à compter du lundi)."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.fr.html#améliorer-laxe",
    "href": "sessions_extra/weekly_epicurves.fr.html#améliorer-laxe",
    "title": "Courbes épidémiques hebdomadaires",
    "section": "Améliorer l’axe",
    "text": "Améliorer l’axe\nIl est maintenant temps d’améliorer cet axe des abscisses.\n{ggplot2} crée automatiquement des étiquettes pour l’axe des x, en essayant de s’adapter à l’étendue des données. Ces valeurs par défaut ne nous conviennent pas toujours, et nous voulons pouvoir manuellement changer les étiquettes (plus fréquentes ou plus espacées, améliorer le format etc.).\nPour modifier l’apparence de l’axe, nous allons utiliser une fonction de la famille scale de {ggplot2} : scale_x_date() [scale ici est l’échelle].\n\nModifier la fréquence des tirets\nDans {ggplot2}, les breaks [cassures] contrôlent la fréquence des tirets sur l’axe.\nLa fonction scale_x_date() a un argument date_breaks qui accepte l’intervalle entre deux étiquettes dans une chaîne de caractères aux formats suivants : \"1 week\", \"2 weeks\", \"4 months\", \"2 years\", etc.\n\ndf_linelist |&gt; \n  count(sem_sortie_lundi) |&gt; \n  ggplot(aes(x = sem_sortie_lundi,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date de sortie\",\n       y = \"Patients\",\n       title = \"Sorties rougeole dans la région de Mandoul (Tchad)\") +\n  scale_x_date(date_breaks = \"4 months\") +  # Définit l'intervalle entre étiquettes\n  theme_classic(base_size = 16)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\nModifiez votre code pour que la fréquence des tirets les rendent lisibles sur votre moniteur.\n\n\n\nAméliorer les étiquettes\nMaintenant que nous avons géré l’intervalle entre les tirets, nous pouvons modifier les étiquettes elles-mêmes (la façon dont les dates sont affichées sur l’axe, labels en anglais). Par défaut, elles sont sous la forme année-mois-jour. Nous allons voir deux manières de changer ça\n\nAvec le paquet {scales}Avec la syntaxe strptime\n\n\nLa fonction scale_x_date() a un argument label qui accepte plusieurs types d’entrées, telles qu’un vecteur contenant les dates ou une fonction qui génère des labels. Le paquet {scales} fournit une telle fonction, label_date_short(), qui tente de créer des étiquettes de dates efficaces et courtes.\n\ndf_linelist |&gt; \n  count(sem_sortie_lundi) |&gt; \n  ggplot(aes(x = sem_sortie_lundi,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date de sortie\",\n       y = \"Patients\",\n       title = \"Sorties rougeole dans la région de Mandoul (Tchad)\") +\n  scale_x_date(date_breaks = \"2 months\",\n               labels = scales::label_date_short()) + # Etiquettes courtes\n  theme_classic(base_size = 15)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\nModifiez votre code et usez label_date_short() pour créer des étiquettes courtes.\n\n\n\nL’automatisation c’est sympa, mais si vous préférez avoir le contrôle total, R dispose d’une syntaxe pour décrire les formats de date et d’heure. Il existe une longue page d’aide (accessibles avec la commande help(strptime)) avec tous les éléments de syntaxe, mais voici un résumé des éléments les plus utiles pour décrire le format d’une date :\nNuméro du jour :\n\n%d: de 01 à 31\n%e: de 1 à 31\n\nMois :\n\n%b : nom du mois, forme abréviée (la langue dépend de la locale de votre ordinateur)\n%B : nom du mois, complet (la langue dépend de la locale de votre ordinateur)\n%m : Numéro du mois\n\nAnnée :\n\n%y : année à deux chiffres (sans le siècle)\n%Y : année à quatre chiffres\n\nSéparateurs spéciaux :\n\n%n : nouvelle ligne\n%t : tab\n\nVous pouvez assembler ces éléments dans une chaîne de caractères, que vous passez à différentes fonctions qui acceptent un format comme argument.\nNous allons d’abord utiliser la fonction format() pour voir rapidement l’affichage qu’elle crée à partir d’une syntaxe strptime, puis nous illustrerons l’usage dans un graphe.\n\n# Crée un vecteur de dates pour explorer des formats différents\nquelques_dates &lt;- as.Date(c(\"2024-10-06\", \"2024-12-15\", \"2025-01-20\"))\n\n# Exemples de syntaxes possibles\nformat(quelques_dates, \"%Y-%b-%d\")\n\n[1] \"2024-Oct-06\" \"2024-Dec-15\" \"2025-Jan-20\"\n\nformat(quelques_dates, \"%Y-%b\")\n\n[1] \"2024-Oct\" \"2024-Dec\" \"2025-Jan\"\n\nformat(quelques_dates, \"%Y %B %d\")\n\n[1] \"2024 October 06\"  \"2024 December 15\" \"2025 January 20\" \n\nformat(quelques_dates, \"%y/%m/%d\")\n\n[1] \"24/10/06\" \"24/12/15\" \"25/01/20\"\n\nformat(quelques_dates, \"%d/%m/%Y\")\n\n[1] \"06/10/2024\" \"15/12/2024\" \"20/01/2025\"\n\n\nRevenons à notre graphe. La fonction scale_x_date() a un argument date_labels qui accepte une chaîne de caractère dans le format strptime pour formater les étiquettes de dates.\n\ndf_linelist |&gt; \n  count(sem_sortie_lundi) |&gt; \n  ggplot(aes(x = sem_sortie_lundi,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date de sortie\",\n       y = \"Patients\",\n       title = \"Sorties rougeole dans la région de Mandoul (Tchad)\") +\n  scale_x_date(\n    date_breaks = \"2 months\",      # Définit l'intervalle entre étiquettes\n    date_labels = \"%Y%n%b%n%d\") +  # Definit le format des étiquettes\n  theme_classic(base_size = 16)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\nModifiez votre graphe pour que les étiquettes soient comme ceci :\n\n\n\n\n\n\n\n\n\n\n\nC’est fini !\nBravo ! Les dates dans R sont un sujet compliqué, et leur format est souvent un peu effrayant. Nous espérons que cette petite introduction vous aura donné quelques astuces pour que vos courbes épidémiques soient lisibles.\n\n\n\n Solutions\n\n\n\n\n\nAller plus loin\n\nExercices supplémentaires\n\nUtilisez ce format dans cotre graphe : “2024-oct.”, “2024-dec.”\nCréez une courbe épidémique avec la date de consultation, avec le premier jour de la semaine sur l’axe des x (vous êtes libres du format de la date).\nCréez une courbe épidémique pour l’année 2023 qui montre le nombre d’admissions hospitalières hebdomadaires, avec le numéro ISO de la semaine en abscisse.\n\n\n\nDéfi\n\nTracez une courbe épidémique de la date d’apparition des symptômes par mois. Utilisez un format d’étiquette qui vous semble approprié et lisible.\n\n\n\n\nRessources\n\nLe chapitre (en anglais) “Elegant graphics for data analyses’ book on date scales\nLa page d’aide de lubridate"
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.fr.html#cest-fini",
    "href": "sessions_extra/weekly_epicurves.fr.html#cest-fini",
    "title": "Courbes épidémiques hebdomadaires",
    "section": "C’est fini !",
    "text": "C’est fini !\nBravo ! Les dates dans R sont un sujet compliqué, et leur format est souvent un peu effrayant. Nous espérons que cette petite introduction vous aura donné quelques astuces pour que vos courbes épidémiques soient lisibles.\n\n\n\n Solutions"
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.fr.html#aller-plus-loin",
    "href": "sessions_extra/weekly_epicurves.fr.html#aller-plus-loin",
    "title": "Courbes épidémiques hebdomadaires",
    "section": "Aller plus loin",
    "text": "Aller plus loin\n\nExercices supplémentaires\n\nUtilisez ce format dans cotre graphe : “2024-oct.”, “2024-dec.”\nCréez une courbe épidémique avec la date de consultation, avec le premier jour de la semaine sur l’axe des x (vous êtes libres du format de la date).\nCréez une courbe épidémique pour l’année 2023 qui montre le nombre d’admissions hospitalières hebdomadaires, avec le numéro ISO de la semaine en abscisse.\n\n\n\nDéfi\n\nTracez une courbe épidémique de la date d’apparition des symptômes par mois. Utilisez un format d’étiquette qui vous semble approprié et lisible."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.fr.html#ressources",
    "href": "sessions_extra/weekly_epicurves.fr.html#ressources",
    "title": "Courbes épidémiques hebdomadaires",
    "section": "Ressources",
    "text": "Ressources\n\nLe chapitre (en anglais) “Elegant graphics for data analyses’ book on date scales\nLa page d’aide de lubridate"
  },
  {
    "objectID": "sessions_core/01_introduction.fr.html",
    "href": "sessions_core/01_introduction.fr.html",
    "title": "Introduction à R",
    "section": "",
    "text": "Se familiariser avec RStudio\nApprendre le fonctionnement de la console\nCréer et exécuter un script\nCréer des objets de base dans R, tels que des vecteurs et des data frames"
  },
  {
    "objectID": "sessions_core/01_introduction.fr.html#objectifs",
    "href": "sessions_core/01_introduction.fr.html#objectifs",
    "title": "Introduction à R",
    "section": "",
    "text": "Se familiariser avec RStudio\nApprendre le fonctionnement de la console\nCréer et exécuter un script\nCréer des objets de base dans R, tels que des vecteurs et des data frames"
  },
  {
    "objectID": "sessions_core/01_introduction.fr.html#format-des-exercices",
    "href": "sessions_core/01_introduction.fr.html#format-des-exercices",
    "title": "Introduction à R",
    "section": "Format des exercices",
    "text": "Format des exercices\nCes exercices sont dans un format tutoriel contenant de brèves explications sur les concepts clés, des exemples et des instructions à suivre. Notre approche est très orientée sur la pratique, et à l’exception de cette première session partiellement axée sur l’interface, vous aurez beaucoup d’occasions de coder.\nLes instructions pour les exercices seront données dans les formats suivants :\n\nCet encadré contient des instructions généralistes. Vous le trouverez en général au début d’une session, avec des instructions de mise en place.\n Exemple : Ouvrez un script vide et nommez-le mon_premier_script.R.\n\n\nCet encadré contient des instructions de code que vous devez écrire dans votre script ou la console.\n Exemple : Créez un objet region qui contient la valeur \"Mandoul\".\n\n\nCet encadré vous demande d’observer ou étudier quelque chose.\n Exemple : Inspectez l’interface de RStudio.\n\nAu cours de ces exercices, vous rencontrerez certainement des erreurs, qui se produisent lorsque R n’est pas en mesure d’exécuter une commande. Cela peut se produire pour de nombreuses raisons : une faute d’orthographe dans le nom d’un objet ou d’une fonction, le mauvais type de données fournis etc. Lorsqu’une erreur se produit, R arrête les calculs en cours et affiche un message expliquant ce qu’il s’est passé. Il est tout à fait normal d’avoir des erreurs, ça arrive tout le temps, à tous les programmeurs, qu’ils soient novices ou experts. Comme lorsque vous apprenez une langue (non informatique), vous vous améliorerez avec la pratique, en faisant des erreurs et en apprenant à les corriger."
  },
  {
    "objectID": "sessions_core/01_introduction.fr.html#rstudio-et-r",
    "href": "sessions_core/01_introduction.fr.html#rstudio-et-r",
    "title": "Introduction à R",
    "section": "RStudio et R",
    "text": "RStudio et R\nR est un langage de programmation fonctionnel qui peut être utilisé pour nettoyer et manipuler des données, effectuer des analyses (en particulier des analyses statistiques), visualiser des résultats, et bien plus encore.\nRStudio est un logiciel qui fournit une interface facile à utiliser pour R (également appelé IDE, pour “Integrated Development Environment”). Son utilisation n’est pas obligatoire, mais très fortement recommandée pour les débutants.\n\nPremiers pas avec RStudio\n\nOuvrez RStudio en utilisant le menu de démarrage de votre ordinateur ou le raccourci créé par défaut sur le bureau ; si RStudio était déjà ouvert, fermez-le et ouvrez-le à nouveau.\n\nVous devriez voir une interface qui ressemble à ceci :\n\n\n\nVue de l’interface de l’IDE Rstudio à l’ouverture\n\n\n\nInspectez l’interface de RStudio.\n\nVous verrez trois ou quatre panneaux.\n\nPanneau supérieur droit\nEn haut à droite se trouve un panneau avec plusieurs onglets. La plupart d’entre eux dépassent le cadre de ce cours, mais nous utiliserons les deux onglets suivants :\n\nEnvironment : liste les objets enregistrés par l’utilisateur dans la session en cours. Comme vous venez de démarrer une nouvelle session, votre environnement devrait être vide.\nHistory : comprend l’historique de toutes les commandes que vous avez exécutées au cours de la session actuelle.\n\n\n\n\n\n\n\nNote\n\n\n\nOuvrir une nouvelle session R, c’est comme redémarer son ordinateur : tout est vide et prêt pour le calcul, de la même manière qu’il n’y a aucun programme ouvert lorsque vous allumez votre ordinateur pour la première fois.\nNous vous encourageons à arrêter et à re-démarrer vos sessions R régulièrement. Parfois cela corrigera certains de vos problèmes !\n\n\n\n\nPanneau inférieur droit\nEn bas à droite se trouve un autre panneau comprenant les onglets suivants :\n\nFiles : un explorateur de fichiers pour le répertoire de travail, qui est l’emplacement du dossier dans lequel R travaille actuellement.\nPlots : là où RStudio affichera les graphiques statiques. Cet onglet devrait être vide pour le moment.\nPackages : liste de tous les paquets R installés sur votre ordinateur. Les paquets sont des collections de fonctions qui permettent d’étendre les fonctionnalités de R. Nous les aborderons plus en détail dans la prochaine leçon.\nHelp : un endroit pour lire les pages d’aide et la documentation pour les fonctions et les paquets.\nViewer : un emplacement où RStudio affichera des sorties html telles que des tableaux, des widgets interactifs ou même des tableaux de bord.\n\n\n\nPartie gauche\n\nA gauche (ou en bas à gauche si vous avez déjà quatre panneaux), vous devriez voir l’onglet console, où le code R est exécuté.\nEn haut à gauche (si vous avez quatre panneaux) se trouvent les scripts R ouverts.\n\n\n\n\nLa console\nLa console est l’endroit où le code R s’exécute.\nAu début d’une nouvelle session, un texte d’information sur votre cofiguration apparaît tout en haut de la console, dont le numéro et nom de la version de R. En dessous de ces informations, il y a une ligne avec le symbole &gt; et un curseur clignotant.\nPour exécuter une commande dans R, tapez-la à la suite du &gt; et pressez Entrée. R traitera alors votre code et affichera le résultat (s’il y en a un). Un nouveau &gt; s’affichera alors sur la ligne suivante, indiquant que la console est prête pour la commande suivante.\n\n\n\n\n\n\nImportant\n\n\n\nSi la dernière ligne est préfacée d’un + au lieu d’un &gt;, cela signifie que la console n’est pas prête. Soit elle attend qu’un calcul d’une commande précédente finisse, soit elle attend la fin d’une commande incomplète. A tout moment, vous pouvez interrompre l’exécution en pressant la touche Echap.\n\n\n\nExécutez les commandes suivantes dans la console, une ligne à la fois, et observez les résultats.\n\n5 + 90\n\n6 * 171\n\n189 / 36.6\n\n92^3\n\n(12 + 9)^4 / 1000\n\nExécutez maintenant la commande suivante. Notez que le ) fermant est manquant, ce qui rend la commande incomplète. Que se passe-t-il ?\n\n3 / (2 + 97\n\n\nVous avez peut-être noté dans les exemples précédents que notre code contient beaucoup d’espaces. C’est en effet une bonne pratique que d’inclure des espaces autour de la plupart des opérateurs, tels que +, -, *, /, &lt;, &gt;, = et &lt;-. Ces espaces facilitent la lecture et la compréhension de votre code, et dans certains cas (rares) ils permettent d’éviter des erreurs. Néanmoins, certains opérateurs ne doivent pas être entourés d’espaces, tels que ^, . et :.\n\n1+29+4.8/3*3           # Mauvais\n1 + 29 + 4.8 / 3 * 3   # Bien\n\n1 ^ 2  # Mauvais\n1^2    # Bien\n\nNous pouvons également exécuter des fonctions dans la console. Nous aborderons les fonctions plus en détail plus tard mais sachez que les fonctions dans R sont similaires aux fonctions dans Excel (telles que SOMME ou MOYENNE).\n\nExécutez les commandes suivantes dans la console (une ligne à la fois).\n\n# Trouvez la valeur minimale\nmin(5, 10)\nmin(1, 8, 56, 0.3)\n\n# Trouvez la valeur maximale\nmax(568, 258, 314)\n\n\n\n\nScripts\nLes scripts sont des fichiers texte qui contiennent une série de commandes pour un langage de programmation particulier. L’extension du fichier indique le langage dans lequel les commandes sont écrites. Ici nous utiliserons l’extension .R. Les scripts nous permettent de créer du code qui peut être réutilisé, partagé et même automatisé.\n\nÉcrire son premier script\nPour créer un nouveau script, allez dans le menu File &gt; New File &gt; R Script. Alternativement, cliquez sur la petite icône avec un + vert sur une page blanche située en dessous du menu File. Ou encore, utilisez le raccourci clavier CTRL + MAJ + N. Ce nouveau script non sauvegardé apparaîtra sous la forme d’un document vierge dans le panneau supérieur gauche.\n\n\n\nEtapes pour créer un nouveau script dans RStudio\n\n\nPour enregistrer votre script, utilisez le menu File &gt; Save As ou le raccourci clavier CTRL + S.\n\nCréez un script et enregistrez-le sous le nom decouverte.R(n’oubliez l’extension !). Pour l’instant, vous pouvez l’enregistrer sur votre bureau ou à tout autre endroit pratique, mais nous aborderons l’organisation des scripts dans la prochaine session.\n\n\n\nExécuter du code à partir d’un script\nPour exécuter du code à partir d’un script, placez votre curseur sur la ligne que vous souhaitez exécuter (ou sélectionnez plusieurs lignes) et effectuez l’une des opérations suivantes :\n\nCliquez sur le bouton Run en haut à droite de la fenêtre de script\nUtilisez le raccourci CTRL + Entrée (le curseur passera ensuite à la ligne suivante)\nUtiliser le raccourci ALT + Entrée (le curseur restera sur la ligne actuelle)\n\n\nCopiez le code que vous aviez exécuté dans la console lors des exercices précédents dans votre script et exécutez-le en testant les différentes méthodes ci-dessus.\nA partir de maintenant, vous écrirez votre code dans votre script et l’exécuterez à partir de là, sauf indication contraire de notre part.\n\n\n\nCommentaires\nDans R, le code qui est précédé d’un # (dièse) n’est pas exécuté, il est juste ignoré jusqu’à la fin de la ligne. C’est donc un bon moyen de documenter votre code.\n\n# Ceci est un commentaire\n\n2 + 3  # Ceci est aussi un commentaire\n\nIl est utile pour vous et vos collègues de commencer vos scripts par quelques lignes commentées fournissant des informations importantes sur le contenu de votre script.\n\n# IMPORT & PREPARATION DES DONNEES #\n# Auteure :  Mathilde Mousset\n# Date de création : 23/11/2024\n# Dernière mise à jour : 28/01/2024\n# Description : Importat et nettoyage des données de surveillance rougeole de Moissala\n\n\nAjoutez quelques commentaires au début de votre script pour le décrire.\n\nLes commentaires sont également un moyen pratique de diviser les scripts longs en sections thématiques, telles que “Import des données”, “Analyse”, “Visualisation”, etc. Par exemple :\n\n# NOM DE LA SECTION 1 -----------------------------------------------             \n\n# NOM DE LA SECTION 2 -----------------------------------------------             \n\n\nUtilisez les commentaires pour créer des sections dans votre script qui correspondent aux sections principales de ce tutoriel.\n\nEnfin, les commentaires permettent de prendre des notes sur votre code pour aider à la compréhension (celle de votre “moi futur” et celle de vos collègues). On entend souvent le conseil de se focaliser sur les commentaires qui expliquent le “pourquoi” plutôt que le “quoi”, car le “quoi” d’un code bien écrit devrait être clair.\nPar exemple, ce commentaire est superflu :\n\n1 + 3  # Code pour additionner un et trois\n\nEn comparaison, voici quelques cas où un commentaire est mérité :\n\nVous définissez une constante, une valeur seuil de séroprévalence par exemple. Ajoutez un commentaire indiquant la référence d’où provient la valeur.\nVotre code contient une valeur ou un nom de fichier qui doit être mis à jour chaque semaine. Indiquez le dans un commentaire afin que toute personne utilisant le code en soit informée.\nVous utilisez une commande contre-intuitive de premier abord, ou un paquet rare que votre collègue ne connaît peut-être pas. Commentez pour expliquer vos raisons.\n\nCeci étant dit, vous êtes en plein apprentissage, et les scripts que vous écrivez pendant ce cours sont l’équivalent de vos notes de cours, alors n’hésitez pas à utiliser autant de commentaires que vous le souhaitez pour expliquer les commandes et vous rappeler de ce qu’elles font. Vous écrirez naturellement moins de commentaires avec la pratique, lorsque les choses qui nouvelles aujourd’hui deviendront naturelles.\n\n\n\n\n\n\nTip\n\n\n\nCommentez une ligne sélectionnée avec le raccourci CTRL + MAJ + C.\nAjoutez une section de premier niveau avec CTRL + MAJ + R.\n\n\n\nAjoutez quelques commentaires pour décrire le code que vous avez écrit jusqu’à présent dans votre script."
  },
  {
    "objectID": "sessions_core/01_introduction.fr.html#types-de-données",
    "href": "sessions_core/01_introduction.fr.html#types-de-données",
    "title": "Introduction à R",
    "section": "Types de données",
    "text": "Types de données\nR dispose de plusieurs types de données. Ceux que nous verrons le plus souvent dans ce cours sont les suivants :\n\nnumérique [numeric en anglais]\nchaîne de caractères (texte) [string en anglais]\nbooléen (VRAI / FAUX) [boolean en anglais]\ndate [date]\nfacteur [factor]\n\n\nNumérique\nLe type numérique englobe les entiers [integers en anglais] et les doubles (nombres décimaux). Les nombres en R n’ont pas de signalétique, tapez simplement la valeur brute dans votre script ou votre console.\n\n\nChaînes de caractères\nLes chaînes de caractères [strings] représentent le texte en R. Elles sont tapées en entourant votre texte de guillemets simples ou doubles, \"district\" ou 'cas' par exemple (les guillemets doubles sont généralement considérés comme la meilleure pratique).\n\nComparez la sortie dans la console pour les commandes suivantes :\n\n28         # numérique\n\"28\"       # texte\n28 + \"28\"  # donne une erreur\n\n\nLa dernière commande ci-dessus a renvoyé une erreur car nous ne pouvons pas effectuer d’opérations arithmétiques combinant du texte et des nombres.\n\n\n\n\n\n\nImportant\n\n\n\nR est sensible à la casse (majuscules ou minuscules), ce qui signifie que \"ABC\" n’est pas équivalent à \"abc\".\n\n\n\nSi vous souhaitez créer une chaîne de caractères contenant des guillemets, il faut échapper les guillements les faisant précéder d’un \\. Par exemple : \"Elle dit \\\"Bonjour\\\" et s'en alla\" ou 'C\\'est une belle journée'. Si vous avez utilisé des guillements doubles pour créer votre chaîne de caractères, vous pouvez utiliser des guillemets simples à l’intérieur de celle-ci (par exemple : \"C'est une belle journée\") et vice versa (par exemple : 'Elle dit \"Bonjour\" et s'en alla').\n\n\n\nBooléen (logique)\nLe type booléen (ou logique) stocke des valeurs vrai/faux et est créé en écrivant soit TRUE [VRAI] ou FALSE [FAUX] sans guillemets.\nEn interne, R traduit TRUE et FALSE en équivalents numériques 1 et 0 respectivement, ce qui peut être utile pour des opérations arithmétiques.\n\n\n\n\n\n\nNote\n\n\n\nVous verrez peut-être des personnes qui utilisent T ou F mais c’est déconseillé car T et F peuvent également être utilisés comme noms d’objets ou de variables. En revanche, les valeurs TRUE et FALSE sont réservées (protégées), ce qui signifie qu’elles ne peuvent pas être réaffectés à une autre valeur.\n\n\n\n\nDéterminer le type d’un objet\nIl existe plusieurs fonctions permettant de déterminer le type d’un objet (souvent appelé la classe de l’objet en R [class].\n\nTapez les commandes suivantes dans votre script et exécutez-les :\n\n# Obtenir le type\nclass(28)  \nclass(\"Mandoul\")\n\n# Test du type\nis.numeric(28)\nis.numeric(\"Mandoul\")\nis.character(\"Mandoul\")\n\nis.numeric(TRUE)\nis.character(TRUE)\nis.logical(FALSE)"
  },
  {
    "objectID": "sessions_core/01_introduction.fr.html#sec-assignement-operator",
    "href": "sessions_core/01_introduction.fr.html#sec-assignement-operator",
    "title": "Introduction à R",
    "section": "Enregistrer un objet",
    "text": "Enregistrer un objet\nEn R, presque tout est un objet y compris les fonctions, les vecteurs et les structures plus complexes. Souvent, nous souhaitons réutiliser certains objets tout au long d’un script (un jeu de données par exemple). Il est donc très utile de les stocker dans notre environnement (la mémoire de R). Pour ce faire, nous utilisons l’opérateur d’assignation &lt;-.\n\nRegardez le panneau environnement en haut à droite. Il devrait être vide. Tapez la commande suivante dans votre script et exécutez-la. Elle enregistre une variable appelée cas dans votre environnement.\n\ncas &lt;- 28\n\nInspectez à nouveau l’environnement. Est-il toujours vide ?\n\nSi vous souhaitez accéder à la valeur de votre nouvel objet, cas il vous suffit d’exécuter son nom.\n\ncas\n\n[1] 28\n\n\n\n\n\n\n\n\nNote\n\n\n\nNous écrivons les chaînes de caractères entre guillements pour permettre à R de faire la différence entre un objet cas et le texte \"cas\".\n\n\nUne fois créés, les objets peuvent être utilisés dans d’autres commandes :\n\ncas + 5\n\n[1] 33\n\n\n\nDans votre script, créez un objet appelé region qui contient la valeur \"Mandoul\". Est-il bien apparu dans votre environnement ?\n\n\n\n\n\n\n\nTip\n\n\n\nN’oubliez pas que nous devons toujours entourer l’opérateur &lt;- par des espaces afin d’améliorer la lisibilité et d’éviter les erreurs.\n\nx&lt;-3     # MAUVAIS\nx &lt;- 3   # BIEN\n\n\n\n\nMettre à jour d’un objet\nNous souhaitons souvent mettre à jour la valeur stockée dans un objet. Pour ce faire, il suffit d’assigner une nouvelle valeur avec la même syntaxe que celle utilisée lors de la création de l’objet :\n\ncas &lt;- 32\n\n\nMettez à jour l’objet region avec la valeur \"Moyen Chari\".\n\n\n\nNoms d’objets\nPour nommer vos objets, il existe quelques règles (relativement) strictes :\n\nNe pas commencer par un chiffre\nNe pas utiliser d’espaces (utiliser un _ à la place)\nNe pas utiliser de valeurs réservées (comme TRUE et FALSE) ou des noms de fonctions (comme mean)\nNe pas utiliser de majuscules (c’est plus une convention qu’une règle dure)\n\nAu-delà de ces règles, il existe également des bonnes pratiques plus subjectives et des styles personnels. En règle générale, les noms doivent être courts et descriptifs :\n\na &lt;- 19                              # Pas informatif\nage_du_patient_a_l_admission &lt;- 19   # Trop long\nage &lt;- 19                            # Concis et précis\n\nDes noms clairs et informatifs contribuent à rendre votre code plus lisible, ce qui permet aux autres de le comprendre facilement sans avoir à constamment consulter le dictionnaire de données."
  },
  {
    "objectID": "sessions_core/01_introduction.fr.html#structures-de-données",
    "href": "sessions_core/01_introduction.fr.html#structures-de-données",
    "title": "Introduction à R",
    "section": "Structures de données",
    "text": "Structures de données\nJusqu’à maintenant, nous avons créé des objets simples qui contenaient une seule valeur. A présent nous allons nous intéresser à des structures plus complexes qui peuvent contenir des jeux de données.\n\nVecteurs\nIl est possible de rassembler plusieurs valeurs (telles que des valeurs numériques ou des chaînes de caractères) en un seul objet, appelé vecteur.\nTechniquement, il existe plusieurs types de vecteurs, dont :\n\nles vecteurs simples (ou vecteurs atomiques) ne peuvent contenir qu’un seul type de valeurs. Par exemple, un vecteur d’entiers contenant 2, 4, 6 ou un vecteur de texte contenant \"Mandoul\", \"Moyen Chari\".\nles vecteurs récursifs (généralement appelés listes) sont plus complexes et peuvent contenir plusieurs dimensions et types de données. Nous ne les aborderons pas dans cette leçon.\n\nCette leçon n’entrera pas dans les détails abstraits de ces structures et se concentrera sur celles que vous rencontrerez le plus souvent dans votre travail.\n\nVecteurs simples\nLes vecteurs simples peuvent contenir une ou plusieurs valeurs d’un seul type de données. Ils ont donc deux propriétés essentielles : une longueur et un type. Dans le cadre de ce cours, nous utiliserons indifféremment les termes “vecteur simple” et “vecteur”, comme c’est généralement le cas dans la communauté R.\nTechniquement, vous avez déjà créé vos premiers vecteurs simples lorsque vous avez construit les objets cas et region. Il s’agissait de vecteurs avec une longueur de taille une. Pour créer un vecteur avec plus d’une valeur, nous utiliserons la fonction c() (moyen mnémotechnique) :\n\ncas &lt;- c(2, 5, 8, 0, 4)\n\n\nMettez à jour cas avec les valeurs ci-dessus et mettez à jour region pour créer un vecteur de chaînes de caractères contenant les valeurs suivantes : Mandoul, Moyen-Chari, Logone Oriental, Tibesti et Logone Occidental.\n\nNous pouvons maintenant utiliser des fonctions sur les objets que nous avons créés :\n\nmean(cas)  # Calcule la moyenne des valeurs stockées dans le vecteur\n\n[1] 3.8\n\ntoupper(region)  # Convertit les valeurs du vecteur en majuscules\n\n[1] \"MANDOUL\"           \"MOYEN-CHARI\"       \"LOGONE ORIENTAL\"  \n[4] \"TIBESTI\"           \"LOGONE OCCIDENTAL\"\n\n\n\nEcrivez des commandes dans votre script pour effectuer les actions suivantes :\n\ncalculer la somme des valeurs de cas avec la fonction sum()\nconvertir le texte de region en minuscules à l’aide de la fonction tolower()\n\n\n\n\n\nAccès aux valeurs d’un vecteur\nIl est possible d’accéder à une valeur d’un vecteur en donnant son indice (i.e. sa position dans le vecteur) entre crochets :\n\ncas[2]   # Deuxième valeur de cas\n\n[1] 5\n\ncas[10]  # Dixième valeur de cas\n\n[1] NA\n\n\nOups il n’y a pas de dixième valeur dans cas ! Nous reviendrons sur ce que ce NA signifie dans la section valeurs manquantes.\nNous pouvons également accéder à une plage de valeurs, comme nous pourrions le faire dans Excel. Nous utilisons l’opérateur : entre la position minimum et maximum de la plage :\n\ncas[2:4]  # de la deuxième à la quatrième valeur\n\n[1] 5 8 0\n\n\n\nAffichez la 3ème valeur du vecteur region.\nAccédez aux valeurs “Mandoul” et “Moyen-Chari” du vecteur region.\n\n\n\nData frames\nLes data frames sont des structures tabulaires / tableaux en 2D avec des lignes et des colonnes. Il s’agit d’une structure très similaire à celle d’un “tableau” dans Excel. En tant qu’épidémiologistes, ce type d’objet est l’un des plus utiles et vous l’utiliserez quotidiennement pour stocker des jeux de données (des listes linéaires par exemple).\n\nCréation d’un data frame\nNous créons un data frame avec la fonction data.frame() :\n\ndata.frame(col1 = c(1, 4, 2, 9),\n           col2 = c(\"un peu de texte\", \"plus de text\", \"Salut !\", \"les epidemiologistes !\"))\n\n  col1                   col2\n1    1        un peu de texte\n2    4           plus de text\n3    2                Salut !\n4    9 les epidemiologistes !\n\n\nIci, on a crée col1 à partir d’un vecteur numérique, et col2 à partir d’un vecteur de chaînes de caractères. Nous avons choisi les noms des colonnes (col1 et col2), ce qui est normal, mais vous pouvez exécuter le code sans nommer les colonnes pour voir comment R crée lui même des noms.\n\nDans votre script, créez un data frame nomé data_cas qui contient cas dans une colonne et region dans l’autre.\n\n\n\nExploration d’un data frame\nL’objet data_cas devrait maintenant apparaître dans votre environnement. Vous pouvez cliquer sur le cercle bleu avec un triangle blanc pour dérouler des informations supplémentaires, ou cliquer sur son nom pour le visualiser dans un onglet dans le même volet que votre script.\n\n\n\nLe data frame data_cas apparaît désormais dans l’onglet Environnement.\n\n\nIl existe plusieurs fonctions pratiques pour explorer un data frame :\n\nExécutez les commandes suivantes et essayez de déterminer le type d’informations qu’elles renvoient.\n\nstr(data_cas)     # STRucture de l'object\ndim(data_cas)     # DIMension de l'object\nnrow(data_cas)    # Nombre de lignes (row = ligne)\nncol(data_cas)    # Nombre de COLonnes\nnames(data_cas)   # noms des colonnes\n\n\nPratiquons un peu plus ! R est livré avec quelques data frames intégrés auxquels il est possible d’accéder directement, dont un appelé iris. C’est pratique pour cette session car nous n’avons pas encore appris à importer des données dans R (ne vous inquiétez pas, nous travaillerons sur des données de liste linéaire dès la prochaine session !).\nNous pouvons afficher les premières lignes de ce data frame grâce à la fonction head() [head = la tête en anglais] :\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\nCombien de lignes et de colonnes y a-t-il dans iris? Quels sont les noms des colonnes de ce data frame ?\n\n\n\nAccéder aux données d’un data frame\nEn R, il existe plusieurs méthodes pour accéder aux lignes et/ou colonnes d’un data frame. Dans cette session d’introduction, nous nous concentrerons sur la syntaxe [row, column].\nNous pouvons utiliser un numéro (ou un intervalle) de ligne pour extraire des lignes, et des numéros (ou un intervalle) de colonnes pour extraire les colonnes. Ont peut également utiliser le nom des colonnes pour y accéder.\n\ndata_cas[1, 2]    # Afficher la valeur de la ligne 1, deuxième colonne\n\n[1] \"Mandoul\"\n\ndata_cas[1, \"region\"]   # Afficher la valeur de la lignbe 1, pour la colonne région\n\n[1] \"Mandoul\"\n\n\nSi nous voulons isoler toutes les lignes (ou colonnes), nous pouvons simplement laisser un espace à la place du numéro/nom :\n\ndata_cas[1, ]  # Extrait la première ligne (garde toutes les colonnes)\n\n  cas  region\n1   2 Mandoul\n\ndata_cas[2:4, ]   # Valeurs des lignes 2 à 4, pour toutes les colonnes\n\n  cas         region\n2   5       Sud Kivu\n3   8 Kasai oriental\n4   0          Kasai\n\ndata_cas[ , \"region\"]   # Garde toutes les lignes mais que la colonne région\n\n[1] \"Mandoul\"        \"Sud Kivu\"       \"Kasai oriental\" \"Kasai\"         \n[5] \"Haut Katanga\"  \n\n\nNous pouvons même sélectionner plusieurs indices non consécutifs en utilisant un vecteur :\n\ndata_cas[c(1, 3), ]  # Ligne 1 et 3 (toutes les colonnes)\n\n  cas         region\n1   2        Mandoul\n3   8 Kasai oriental\n\n\nSoyez attentifs, le type de l’objet renvoyé par [ ] dépend de l’indexation utilisée :\n\nstr(data_cas[1 , ])   # Renvoit un data frame\n\n'data.frame':   1 obs. of  2 variables:\n $ cas   : num 2\n $ region: chr \"Mandoul\"\n\nstr(data_cas[ , 1])   # Renvoit un vecteur\n\n num [1:5] 2 5 8 0 4\n\n\nUne syntaxe simplifiée existe pour extraire des colonnes d’un data frame :\n\ndata_cas[2]           # Renvoit la deuxième colonne (format data frame)\n\n          region\n1        Mandoul\n2       Sud Kivu\n3 Kasai oriental\n4          Kasai\n5   Haut Katanga\n\ndata_cas[\"region\"]    # Renvoit la colonne région (format data frame)\n\n          region\n1        Mandoul\n2       Sud Kivu\n3 Kasai oriental\n4          Kasai\n5   Haut Katanga\n\n\n\nEcrivez le code pour :\n\nextraire la troisième valeur de la colonne region de votre data frame\nextraire les deuxième et troisième valeurs de la colonne cas\ncalculer la somme des valeurs de la colonne cas"
  },
  {
    "objectID": "sessions_core/01_introduction.fr.html#sec-missing-values",
    "href": "sessions_core/01_introduction.fr.html#sec-missing-values",
    "title": "Introduction à R",
    "section": "Valeurs manquantes",
    "text": "Valeurs manquantes\nEn tant qu’épidémiologistes, nous sommes constamment confrontés aux données manquantes. Dans R, celles-ci sont codées à l’aide d’une valeur spéciale : NA [signifiant Not Available]. La valeur NA n’a pas de type fixe, elle prend celui des valeurs qui l’entourent. Par exemple, un NA dans une colonne numérique est traitée comme une valeur numérique. Nous aurons des occasions de manipuler les NA dans la suite du cours."
  },
  {
    "objectID": "sessions_core/01_introduction.fr.html#sec-functions",
    "href": "sessions_core/01_introduction.fr.html#sec-functions",
    "title": "Introduction à R",
    "section": "Fonctions",
    "text": "Fonctions\nLes fonctions sont des objets qui contiennent des commandes (au lieu de valeurs) qui sont exécutées chaque fois que la fonction est lancée. Vous êtes sans doute familiers avec les fonctions dans Excel, telles que la fonction SOMME() ou la fonction MOYENNE(). Bonne nouvelle, les fonctions sont similaires dans R !\nLa majorité des fonctions que vous allez utiliser ont besoin d’informations complémentaires : a minima des données, mais aussi d’autres paramètres. On appelle ces informations des arguments. Les arguments sont normalement nommés.\nPar exemple, lorsque nous avons exécuté la commande sum(cas), nous avons fourni le vecteur cas comme premier (et seul) argument de la fonction sum().\nParmis les arguments d’une fonction, certains peuvent être obligatoires, d’autres facultatifs. Le premier argument est presque toujours obligatoire et est souvent un data frame ou un vecteur de données. Comme c’est un argument évident, on omet souvent son nom (il vous a sans doute semblé naturel de taper mean(cas) au lieu de mean(x = cas)).\nLes arguments facultatifs, en revanche, sont généralement utilisés avec neur nom. Par exemple : mean(cas, na.rm = TRUE). Les arguments facultatifs sont souvent fournis avec des valeurs par défaut raisonnables, ce qui fait que l’utilisateur ne les spécifie que lorsqu’il a besoin de changer ces valeurs par défaut. Par exemple, l’argument na.rm de la fonction mean() controle comment les valeurs manquantes sont gérées lors du calcul de la moyenne [“na” en référence aux valeurs manquantes NA, et “rm” comme raccourci de “ReMove”, que l’on peut traduire dans ce contexte par enlever ou ignorer]. Par défault, la valeur de na.rm est FALSE Ainsi, par défaut, la moyenne de données avec des valeurs manquantes renverra toujours NA :\n\nmean(c(1, 3, NA))\n\n[1] NA\n\n\nCeci est vrai pour de nombreuses opérations arithmétiques dans R. Si l’on veut que que R calcule la moyenne sur toutes les données disponibles et ignore les valeurs manquantes, nous devons explicitement fournir l’argument na.rm = TRUE:\n\nmean(c(1, 3, NA), na.rm = TRUE)\n\n[1] 2\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nLes arguments sont séparés par des virgules.\nCes virgules doivent toujours être suivies d’un espace\nChaque fois qu’un argument nommé est utilisé, l’attribut = doit être entouré d’espaces :\n\n\nmean(cas,na.rm=TRUE)     # MAUVAIS\nmean(cas, na.rm = TRUE)  # BON\n\nSi vous écrivez une commande avec de nombreux arguments, séparez chaque argument sur sa propre ligne pour améliorer la lisibilité :\n\nmean(cas, \n     na.rm = TRUE) \n\n\n\nQue se passe-t-il si l’on fournit plusieurs arguments dans le désordre ? Si vous avez nommé les arguments la fonction s’exécutera correctement, mais le code sera contre-intuitif et peu lisible. Nous vous conseillons de respecter l’ordre standard, en plaçant les arguments obligatoires tels que les données en premier.\n\n# Fonctionnel mais dur à lire\nmean(na.rm = TRUE,  \n     x = cas) \n\n# mieux\nmean(cas,         \n     na.rm = TRUE)\n\nEn revanche, si vous ne nommez pas les arguments et les passez dans le désordre, alors la fonction ne fonctionnera pas comme prévu, voire renverra une erreur :\n\nmean(TRUE, cas)  # Pas ce que vous attendez"
  },
  {
    "objectID": "sessions_core/01_introduction.fr.html#terminé",
    "href": "sessions_core/01_introduction.fr.html#terminé",
    "title": "Introduction à R",
    "section": "Terminé !",
    "text": "Terminé !\nC’est tout pour cette session, bravo pour vos débuts avec R et RStudio !\n\n\n\n Solutions exercices"
  },
  {
    "objectID": "sessions_core/03_data_verbs.fr.html",
    "href": "sessions_core/03_data_verbs.fr.html",
    "title": "Traitement de données, les bases",
    "section": "",
    "text": "Découvrir les fonctions de {dplyr} pour effectuer les actions essentielles sur les données :\n\nSélectionner des colonnes avec select()\nRenommer des colonnes avec rename()\nCréer de nouvelles colonnes ou modifier des colonnes existantes avec mutate()\nSupprimer les doublons avec distinct()\n\nEnchaîner ces actions avec l’opérateur “pipe” |&gt;"
  },
  {
    "objectID": "sessions_core/03_data_verbs.fr.html#objectifs",
    "href": "sessions_core/03_data_verbs.fr.html#objectifs",
    "title": "Traitement de données, les bases",
    "section": "",
    "text": "Découvrir les fonctions de {dplyr} pour effectuer les actions essentielles sur les données :\n\nSélectionner des colonnes avec select()\nRenommer des colonnes avec rename()\nCréer de nouvelles colonnes ou modifier des colonnes existantes avec mutate()\nSupprimer les doublons avec distinct()\n\nEnchaîner ces actions avec l’opérateur “pipe” |&gt;"
  },
  {
    "objectID": "sessions_core/03_data_verbs.fr.html#mise-en-place",
    "href": "sessions_core/03_data_verbs.fr.html#mise-en-place",
    "title": "Traitement de données, les bases",
    "section": "Mise en place",
    "text": "Mise en place\nPrérequis : cette leçon part du principe que vous savez comment utiliser RStudio et que vous êtes capable d’importer des données. Rafraîchissez-vous si besoin avec les deux premières leçons.\n\nNous utiliserons la linelist avec des données brutes que vous avez importée lors de la leçon précédente, et qui peut être téléchargée ici :\n\n\n\n Télécharger les données\n\n\n\n Si ce n’est pas déjà fait, enregistrez le jeu de données dans le sous-dossier approprié de votre projet RStudio puis créez un nouveau script appelé fonctions_donnees.R dans votre sous-dossier R. Ajoutez un en-tête approprié et chargez les paquets suivants : {here}, {rio} et {tidyverse}.  Enfin, ajoutez une section dédiée à l’import des données, utilisez {here} et {rio} pour importer vos données dans R, et assignez-les à un objet que nous appellerons df_brut."
  },
  {
    "objectID": "sessions_core/03_data_verbs.fr.html#traiter-ses-données-avec-dplyr",
    "href": "sessions_core/03_data_verbs.fr.html#traiter-ses-données-avec-dplyr",
    "title": "Traitement de données, les bases",
    "section": "Traiter ses données avec {dplyr}",
    "text": "Traiter ses données avec {dplyr}\nLa mise en place est terminée et nous pouvons maintenant nous focaliser sur nos données ! Cette leçon et les suivantes s’appuieront lourdement sur plusieurs paquets de la collection de paquets tidyverse pour manipuler des data frames, résumer et visualiser les données, et en particulier paquet {dplyr} pour aujourd’hui.\nLe traitement des données (aussi appelé manipulation des données) est un ensemble d’actions essentielles pour préparer et nettoyer les données avant une analyse (que ce soit dans R ou Excel). {dplyr} fournit un grand nombre de fonctions qui nous aident à manipuler les data frames et à réaliser de nombreuses tâches quotidiennes telles que :\n\ncréer des sous-ensembles de nos données en ne gardant que les variables d’intérêt\nrenommer des colonnes\najouter ou modifier des colonnes\nsupprimer les doublons\n\nCes fonctions ont en général un nom intuitif, qui correspond à un verbe. Par exemple, pour renommer des colonnes, on utilisera la fonction rename().\nAujourd’hui nous nous focaliserons sur les quatre verbes (fonctions !) qui permettent d’effectuer les tâches mentionnées précédemment, et que vous utiliserez en permanence. Nous vous montrerons également comment enchaîner les actions dans un “pipeline” pour plus de fluidité.\n\n\n\n\n\n\nNote\n\n\n\nPeut-être avez-vous noté que nous vous parlons du paquet {dplyr} mais nous vous avons fait charger le paquet {tidyverse} lors de la mise en place. C’est que le {tidyverse} est un méta-paquet, et le charger charge automatiquement plusieurs des paquets les plus utiles de l’univers du tidyverse, dont fait partie {dplyr} et d’autres paquets que nous verrons dans la session."
  },
  {
    "objectID": "sessions_core/03_data_verbs.fr.html#actions-de-base-sur-les-données",
    "href": "sessions_core/03_data_verbs.fr.html#actions-de-base-sur-les-données",
    "title": "Traitement de données, les bases",
    "section": "Actions de base sur les données",
    "text": "Actions de base sur les données\n\nSélectionner des colonnes\nIl est fréquent de souhaiter écarter des variables d’un jeu de données, soit car ces colonnes contiennent des données sensibles, soit parce que nous n’avons pas besoin d’elles pour une analyse donnée. Nous utiliserons pour cela la fonction select(), qui la syntaxe suivante :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\nselect(un_dataframe, colonne_a_garder, autre_colonne_a_garder)\n\nIci, le premier argument est le data frame contenant les données. Les arguments suivants sont les noms des colonnes que nous voulons conserver. Dans le tidyverse, les noms de colonnes n’ont pas besoin d’être écrits entre guillemets.\nLa commande suivante nous permet de sélectionner les colonnes id, sex et age, par exemple :\n\nselect(df_brut, id, sexe, age)\n\n\nUtilisez la fonction select() pour sélectionner les variables suivantes de votre data frame : id, sexe, age, sous_prefecture, date_debut et issue. L’en-tête du data frame renvoyé par la fonction ressemble à ceci :\n\n\n  id  sexe age date_debut issue\n1  1 femme  36 2022-08-13 gueri\n2  2     f   5 2022-08-18  &lt;NA&gt;\n3  3     f 156 2022-08-17 gueri\n4  6 homme   8 2022-08-22 gueri\n5  7     h   7 2022-08-30 gueri\n6 10     h   4 2022-08-30 gueri\n\n\n Comparez ce résultat à df_brut. Ce dernier contient toujours toutes les colonnes importées (ce qui est le comportement désiré). Comprenez-vous pourquoi c’est le cas ?\n\nIl arrive que nous ne voulions supprimer que quelques colonnes d’un jeu de données et si le jeu de données est large ça serait fastidieux d’écrire toutes les colonnes à garder comme nous l’avons fait ci-dessus… Heureusement, nous pouvons préfacer un nom de colonne par l’opérateur soustraction (-) pour indiquer à R de la supprimer.\nPar exemple, pour créer un data frame sans la colonne village_commune :\n\nselect(df_brut, -village_commune)\n\n\nUtilisez cette syntaxe pour créer un data frame qui conserve toutes les colonnes de df_brut à l’exception de nom_complet et unite_age.\n\n\n\nRenommer les colonnes\nIl arrive souvent que nous souhaitions renommer des colonnes d’un jeu de données. La fonction rename() est alors votre meilleure amie.\n\n# NE PAS EXÉCUTER (PSEUDO CODE)\nrename(un_dataframe,\n       nouveau_nom_1 = nom_tout_moche,\n       nouveau_nom_2 = autre_nom_pas_fou)\n\nComme pour select(), le premier argument est le data frame qui contient les données (ce sera le cas pour la majorité des verbes de {dplyr}). Ensuite, chaque nouvel argument est une paire nouveau_nom = ancien_nom indiquant à R les colonnes à renommer et leurs nouveaux noms. Nous vous conseillons d’aller à la ligne pour chaque nouvelle paire pour aider à la lisibilité.\nRenommons la colonne village_commune en village par exemple :\n\nrename(df_brut,\n       village = village_commune)\n\n\nUtilisez la fonction rename() sur df_brut pour renommer les colonnes :\n\nsous_prefecture en prefecture\nvillage_commune en village\nnom_structure_sante en structure\n\n\nIl peut être difficile de vérifier si une commande fonctionne car R affiche le data frame en entier. Dans ce cas, une première solution consiste à créer un objet temporaire plus facile à manipuler. Vous pouvez le nommer comme vous voulez, mais un nom commun est temp (ou tmp en anglais).\n\nRépétez le dernier exercice en assignant la sortie de la commande à un objet appelé temp. Vous pouvez alors utiliser la fonction names() pour vérifier que les noms des colonnes ont bien changé. La sortie de names() devrait être :\n\n\n [1] \"id\"                \"nom_complet\"       \"sexe\"             \n [4] \"age\"               \"unite_age\"         \"region\"           \n [7] \"prefecture\"        \"village\"           \"date_debut\"       \n[10] \"date_consultation\" \"hospitalisation\"   \"date_admission\"   \n[13] \"structure\"         \"tdr_paludisme\"     \"fievre\"           \n[16] \"eruption\"          \"toux\"              \"yeux_rouges\"      \n[19] \"pneumonie\"         \"encephalite\"       \"pb\"               \n[22] \"statut_vaccinal\"   \"doses_vaccin\"      \"issue\"            \n[25] \"date_issue\"       \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLes objets comme le data frame temp sont généralement utilisés pour tester si quelque chose a fonctionné et peuvent donc être écrasés lorsque vous testez autre chose. Ils ne doivent pas être utilisés comme entrée pour d’autres parties de votre code. Utilisez des noms clairs et appropriés pour vos data frames destinés à être réutilisés, tels que df_linelist ou df_propre.\n\n\n\n\nModifier et ajouter des colonnes\nUne autre tâche essentielle du traitement de données est de modifier des colonnes ou d’en créer de nouvelles. La fonction mutate() permet de faire les deux [to mutate veut dire muter en anglais], avec la syntaxe suivante :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\nmutate(un_dataframe,\n       nouvelle_colonne_1 = action(colonne_existante),\n       nouvelle_colonne_2 = autre_action(une_autre_colonne_existante))\n\nDans le code ci-dessus nous créons une nouvelle colonne (nouvelle_colonne_1) en effectuant une action (des calculs par exemple) sur une colonne existante (colonne_existante) dans le data frame un_dataframe. Puis nous créons une autre colonne (nouvelle_colonne_2) sur le même principe. L’action en question peut être variée et plus ou moins complexe : calcul arithmétique, application d’une fonction sur une colonne (ou même plusieurs !) etc.\nPar exemple, nous pourrions créer une nouvelle colonne exprimant le périmètre brachial en cm :\n\nmutate(df_brut,\n       pb_cm = pb / 100) # une opération arithmétique simple\n\n\nUtilisez mutate() pour créer une nouvelle colonne age_ans qui exprime l’âge en années plutôt qu’en mois. L’en-tête de la colonne ressemble à ceci :\n\n\n   age_years\n1  3.0000000\n2  0.4166667\n3 13.0000000\n4  0.6666667\n5  0.5833333\n6  0.3333333\n\n\n\nPour modifier une colonne existante il suffit d’utiliser le nom de la colonne existante à gauche du = au lieu de fournir un nouveau nom.\nPar exemple, si nous voulions remplacer la colonne pb qui contenait des valeurs en mm par une colonne pb contenant les valeurs en cm :\n\nmutate(df_brut,\n       pb = pb / 100)\n\nNous voulons souvent conserver la colonne originelle, mais il existe des cas raisonnables où nous souhaitons écraser les données par une nouvelle version. Par exemple :\n\nmodifier des chaînes de caractères (format, correction de typos etc.)\ncorriger le type de données d’une colonne\n\nNotre jeu de données présente ces deux cas. Par exemple les colonnes region et sous_prefecture sont en majuscules, ce qui n’est pas un problème en soi, mais peut être améliorable. Pour corriger cela nous pouvons utiliser la fonction str_to_title() du paquet {stringr} (qui fait également partie du {tidyverse}) pour passer les valeurs en casse “titre”.\n\nmutate(df_brut,\n       region = str_to_title(region),\n       sous_prefecture = str_to_title(sous_prefecture))\n\n\nUtilisez la fonction mutate() pour mettre à jour le format de tdr_paludisme et issue afin d’utiliser la casse “titres”. L’en-tête de la sortie pour ces deux colonnes devrait maintenant être :\n\n\n  tdr_paludisme issue\n1       Negatif Gueri\n2       Negatif  &lt;NA&gt;\n3       Negatif Gueri\n4       Negatif Gueri\n5       Negatif Gueri\n6       Negatif Gueri\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNous n’avons pas eu besoin de charger {stringr} car comme {dplyr}, ce paquet est chargé automatiquement lorsque nous chargeons{tidyverse}.\n\n\nPassons maintenant au problème des variables avec le mauvais type.\n\nVérifiez le type de vos colonnes. Y a-t-il des problèmes ?  Indice : str() peut être utile ici.\n\nLes classes semblent raisonnables sauf pour les dates : certaines colonnes ont la classe caractère et d’autres sont POSIXct. Nous préférerions que toutes ces colonnes utilisent le type Date.\nNous allons utiliser la fonction ymd() du paquet {lubridate} pour faire la conversion en Date. “ymd” est l’abréviation de year month day, c’est à dire année-mois-jour. Cela veut dire que la fonction attend une date fournie dans cet ordre-là (les séparateurs peuvent varier).\nPour corriger la date de décharge :\n\nmutate(df_brut,\n       date_issue = ymd(date_issue))\n\n\nUtilisez mutate() et ymd() pour modifier les colonnes date_debut et date_admission afin qu’elles soient de type Date.\nAstuce : n’hésitez pas à stocker la sortie de la fonction dans un data frame temporaire temp pour vérifier le type des variables modifiées.\n\n\n\nSupprimer les doublons\nNous connaissons désormais les fonctions pour sélectionner, renommer et modifier nos variables. Il est temps à présent de passer à une autre tâche essentielle du nettoyage : la suppression doublons.\nLa fonction distinct() permet de rapidement enlever les lignes identiques d’un data frame avec la syntaxe suivante :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\ndistinct(un_dataframe)\n\nPar défaut, nous n’avons besoin que d’un seul argument : le jeu de données lui-même. Cela supprime alors toutes les lignes qui sont complètement en double en ne gardant qu’une seule copie. Il existe des usages plus sophistiqués de distinct() pour chercher des doublons partiels, mais leur correction dépasserait du cadre de cette session…\n\nUtilisez la fonction distinct() et créez un data frame temporaire, temp qui contient toutes les observations uniques dans df_brut. Comparez le nombre de lignes de temp à celui de df_brut. Avions-nous des doublons ?"
  },
  {
    "objectID": "sessions_core/03_data_verbs.fr.html#lopérateur-pipe",
    "href": "sessions_core/03_data_verbs.fr.html#lopérateur-pipe",
    "title": "Traitement de données, les bases",
    "section": "L’opérateur “pipe”",
    "text": "L’opérateur “pipe”\nNous avons profité de la présentation des fonctions essentielles de {dplyr} pour commencer le nettoyage du jeu de données. Il est temps de rassembler les commandes écrites dans les exercices en un ensemble cohérent pour créer un data frame contenant les données netoyées (au moins en partie) que nous appelerons df_linelist.\n\n\n\n\n\n\nTip\n\n\n\nEn général, il est recommandé de conserver une version brute de votre ensemble de données, ici df_brut, qui reste inchangée dans votre code. Ainsi, vous l’avez toujours à disposition dans votre environnement comme référence et elle est toujours disponible au début de votre pipeline de nettoyage pour améliorer la reproductibilité.\n\n\nIl y a plusieurs manières d’enchaîner les différentes étapes que nous avons vues. Intuitivement, nous pourrions commencer comme ceci :\n\ndf_linelist &lt;- rename(df_brut, \n                      prefecture = sous_prefecture,\n                      village    = village_commune,\n                      structure  = nom_structure_sante)\n\nPuis mettre à jour df_linelist :\n\n# Étape 1 : Renommer les variables\ndf_linelist &lt;- rename(df_brut, \n                      prefecture = sub_prefecture,\n                      village    = village_commune,\n                      structure  = nom_structure_sante)\n\n# Étape 2 : Sélectionner les variables à conserver\ndf_linelist &lt;- select(df_linelist,\n                      -nom_complet)\n\nNotez que dans cette deuxième étape, nous utilisons df_linelist comme entrée de select() plutôt que df_brut car nous voulons continuer à travailler sur la version modifiée des données.\nPuis nous ajoutons l’âge en années :\n\n# Étape 1 : Renommer les variables\ndf_linelist &lt;- rename(df_brut, \n                      prefecture = sub_prefecture,\n                      village    = village_commune,\n                      structure  = nom_structure_sante)\n\n# Étape 2 : Sélectionner les variables à conserver\ndf_linelist &lt;- select(df_linelist,\n                      -nom_complet)\n\n# Étape 3 : Ajouter l'âge en années\ndf &lt;- mutate(df_linelist,\n             age_ans = age / 12)\n\nEt caetera. Ce code est tout à fait fonctionnel, mais devient lourd et répétitif si de nombreuses étapes s’enchaînent : à chaque étape nous utilisons en entrée le data frame renvoyé par l’étape précédente pour le mettre à jour…\nIl existe un raccourcis ! L’opérateur pipe (|&gt;) permet d’enchainer des actions de manière plus fluide avec cette syntaxe :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\nune_entree |&gt; une_action\n\n# En particulier :\nun_dataframe |&gt; une_fonction()\n\nIci, le pipe prend l’entrée fournie à gauche et la transmet à la fonction à droite. Ainsi, par exemple, au lieu d’écrire\n\nselect(df_brut, id, sexe)\n\nnous pouvons écrire\n\ndf_brut |&gt; select(id, sexe)\n\n\nTestez le code ci-dessus de votre côté.\n\nOn peut se servir de l’opérateur pipe pour enchaîner plusieurs actions. C’est le style de code dit “du tidyverse”, qui ressemble à ceci :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\ndf_linelist &lt;- df_brut |&gt;\n  action_1() |&gt;\n  action_2() |&gt;\n  action_3() |&gt;\n  ...\n\n\n\n\n\n\n\nTip\n\n\n\nAller à la ligne entre chaque action est considéré comme une bonne pratique pour rendre le code plus facile à lire et à comprendre.\n\n\nNous pourrions donc remplacer le code précédent par ceci :\n\ndf_linelist &lt;- df_brut |&gt;\n  rename(prefecture = sub_prefecture,\n         village    = village_commune,\n         facility   = health_facility_name) |&gt;\n  select(-full_name) |&gt;\n  mutate(age_years = age / 12)\n\nC’est beaucoup plus fluide que de réaffecter df_linelist après chaque étape !\n\nA votre tour ! Rassemblez maintenant toutes les étapes de nettoyage de la leçon en une seule commande en un seul pipeline.\nUtilisez l’opérateur pipe |&gt;, les fonctions select() rename(), mutate(), str_to_title(), ymd() et distinct() pour créer un data frame df_linelist partiellement nettoyé.  Rappel des étapes :\n\nSupprimer les variables nom_complet et unite_age\nRenommer les variables suivantes :\n\nage devient age_ans\nsous_prefecture devient prefecture\nvillage_commune devient village\nnom_structure_sante devient structure\n\nAjouter une variable age_ans avec l’âge du patient en années\nMettre à jour region et prefecture pour utiliser la casse de titre\nMettre à jour toutes les colonnes contenant des dates pour utiliser le type Date\nSupprimer toutes les lignes en double\n\nL’en-tête de vos données finales devrait ressembler à ceci :\n\n\n  id  sexe age_mois  region prefecture        village date_debut\n1  1 femme       36 Mandoul   Moissala Sangana Koïtan 2022-08-13\n2  2     f        5 Mandoul   Moissala      Mousdan 1 2022-08-18\n3  3     f      156 Mandoul   Moissala     Djaroua Ii 2022-08-17\n4  6 homme        8 Mandoul   Moissala     Monakoumba 2022-08-22\n5  7     h        7 Mandoul   Moissala      Tétindaya 2022-08-30\n6 10     h        4 Mandoul   Moissala      Danamadja 2022-08-30\n  date_consultation hospitalisation date_admission\n1        2022-08-14             oui     2022-08-14\n2        2022-08-25             oui     2022-08-25\n3        2022-08-20            &lt;NA&gt;           &lt;NA&gt;\n4        2022-08-25             non           &lt;NA&gt;\n5        2022-09-02             non           &lt;NA&gt;\n6        2022-09-02             oui     2022-09-02\n                        structure tdr_paludisme fievre eruption toux\n1 Hôpital du District de Moissala       negatif     No     &lt;NA&gt;  Yes\n2 Hôpital du District de Moissala       negatif     No       No  Yes\n3                      CS Silambi       negatif    Yes     &lt;NA&gt;   No\n4 Hôpital du District de Moissala       negatif     No       No   No\n5                      CS Silambi       negatif   &lt;NA&gt;       No  Yes\n6                    Moissala Est       negatif    Yes       No   No\n  yeux_rouges pneumonie encephalite  pb statut_vaccinal doses_vaccin issue\n1          No        No          No 244            &lt;NA&gt;         &lt;NA&gt; gueri\n2          No      &lt;NA&gt;          No 232             Non         &lt;NA&gt;  &lt;NA&gt;\n3          No        No        &lt;NA&gt; 123      Oui - oral         &lt;NA&gt; gueri\n4        &lt;NA&gt;        No          No 210             Non         &lt;NA&gt; gueri\n5         Yes        No          No  80             Non         &lt;NA&gt; gueri\n6        &lt;NA&gt;        No          No 220             Non         &lt;NA&gt; gueri\n  date_issue    age_ans\n1 2022-08-18  3.0000000\n2 2022-08-28  0.4166667\n3       &lt;NA&gt; 13.0000000\n4       &lt;NA&gt;  0.6666667\n5       &lt;NA&gt;  0.5833333\n6 2022-09-03  0.3333333\n\n\nAstuce :  soyez attentifs à vos noms de colonne. Si vous rennomez une colonne, il faudra utiliser le nouveau nom dans les étapes suivantes du pipeline.\n\nFantastique ! C’est un excellent début de pipeline de nettoyage de vos données. Sauvegardez ce code, car nous le complèterons lors de la prochaine session, durant laquelle nous apprendron une autre étape essentielle du traitement de données : recoder les variables !"
  },
  {
    "objectID": "sessions_core/03_data_verbs.fr.html#cest-fini",
    "href": "sessions_core/03_data_verbs.fr.html#cest-fini",
    "title": "Traitement de données, les bases",
    "section": "C’est fini !",
    "text": "C’est fini !\nBravo, vous avez appris les bases de la manipulation de données et comment enchaîner plusieurs commandes dans un pipeline. À partir de maintenant, les fichiers contenant les solutions des exercices fourniront le code final plutôt qu’une correction par exercice, afin d’avoir un script plus réaliste. Par exemple, la solution fournira le pipe final créé à la fin de la session d’aujourd’hui.\n\n\n\n Solutions exercices"
  },
  {
    "objectID": "sessions_core/03_data_verbs.fr.html#aller-plus-loin",
    "href": "sessions_core/03_data_verbs.fr.html#aller-plus-loin",
    "title": "Traitement de données, les bases",
    "section": "Aller plus loin",
    "text": "Aller plus loin\n\nExercices supplémentaires\n\nAjoutez une ligne à votre mutate() pour mettre à jour la variable hospitalisation afin que son texte soit également en casse “titre”\nPeut-être préféreriez-vous utiliser des minuscules pour la colonne region plutôt que la casse “titre” ? Mettez votre code à jour pour le faire. Astuce : vous pouvez utiliser la fonction apprises dans la première session ou tester la fonction str_to_lower() de {stringr}.\nCréez une colonne delai_consultation, qui contient le nombre de jours entre l’apparition des symptômes et la consultation."
  },
  {
    "objectID": "sessions_core/05_summary_table.fr.html",
    "href": "sessions_core/05_summary_table.fr.html",
    "title": "Tableaux récapitulatifs",
    "section": "",
    "text": "Créer des tableaux de contingence avec count()\nCalculer des statistiques récapitulatives par groupe à l’aide de summarize()\nRéviser comment sélectionner les lignes en utilisant filter() et créer/modifier des variables avec mutate()\nCréer des variables catégorielles ordonnées"
  },
  {
    "objectID": "sessions_core/05_summary_table.fr.html#objectifs",
    "href": "sessions_core/05_summary_table.fr.html#objectifs",
    "title": "Tableaux récapitulatifs",
    "section": "",
    "text": "Créer des tableaux de contingence avec count()\nCalculer des statistiques récapitulatives par groupe à l’aide de summarize()\nRéviser comment sélectionner les lignes en utilisant filter() et créer/modifier des variables avec mutate()\nCréer des variables catégorielles ordonnées"
  },
  {
    "objectID": "sessions_core/05_summary_table.fr.html#setup",
    "href": "sessions_core/05_summary_table.fr.html#setup",
    "title": "Tableaux récapitulatifs",
    "section": "Setup",
    "text": "Setup\nDépendances. Cette session suppose que vous savez utiliser RStudio, que vous êtes capable d’importer des données et que vous connaissez les verbes de base de manipulation des données que nous avons vus dans les sessions de base jusqu’à présent. Si vous avez besoin d’un rappel sur l’un de ces sujets, nous vous encourageons à revoir les sessions de base du parcours d’apprentissage.\n\nCette session utilisera la version nettoyée de l’ensemble de données Moissala sur la rougeole.\n\n\n\n Linelist rougeole nettoyée\n\n\n\n Ouvrez votre projet RStudio et créez un nouveau script dans le sous-dossier R appelé tables.R avec les métadonnées appropriées et une section “Packages” qui importe : {rio}, {here} et {tidyverse}. Ajoutez une section “Import Data” qui charge la version nettoyée de la linelist de la rougeole dans R."
  },
  {
    "objectID": "sessions_core/05_summary_table.fr.html#introduction-agrégation-des-données",
    "href": "sessions_core/05_summary_table.fr.html#introduction-agrégation-des-données",
    "title": "Tableaux récapitulatifs",
    "section": "Introduction : agrégation des données",
    "text": "Introduction : agrégation des données\nRécapitulons. Vous venez d’effectuer l’une des tâches les [plus importantes]{.hovertip bs-toggle=‘tooltip’ bs-title=’Certains considèrent que cela représente 80 % du travail !} d’un épidémiologiste : le nettoyage des données. Maintenant que vous disposez de données propres et normalisées, vous pouvez vous mettre au travail et commencer à les analyser. Les analyses commencent généralement par des tableaux et des résumés qui décrivent nos données :\n\nTableaux de fréquence univariés pour compter les occurrences de différentes valeurs\nStatistiques sommaires des variables numériques (moyenne, médiane, écart-type)\nTableaux croisés pour examiner les relations entre les variables catégorielles\nRésumés par groupe pour comparer les statistiques entre différents sous-ensembles de données"
  },
  {
    "objectID": "sessions_core/05_summary_table.fr.html#compter-plusieurs-colonnes-tableaux-de-contingence",
    "href": "sessions_core/05_summary_table.fr.html#compter-plusieurs-colonnes-tableaux-de-contingence",
    "title": "Tableaux récapitulatifs",
    "section": "Compter plusieurs colonnes (tableaux de contingence)",
    "text": "Compter plusieurs colonnes (tableaux de contingence)\nAu cours de la session d’exploration des données, vous avez appris à créer un tableau de fréquence pour une variable catégorielle unique à l’aide de la fonction count(). C’est bien, mais nous voulons souvent compter le nombre d’observations en fonction de deux variables (ou plus !).\nCes tableaux sont appelés tableaux de contingence. Par exemple, connaître le nombre de patients par sous-préfecture est très utile, mais nous pourrions vouloir stratifier à la fois par sous-préfecture et par groupe d’âge pour voir si certaines zones ont des patients anormalement âgés. Ce type de stratification est un moyen utile d’essayer de trouver des zones qui pourraient être de bons candidats pour des campagnes de rattrapage. C’est facile, il suffit de passer plusieurs noms de colonnes à count() :\n\ndf_linelist |&gt;\n  count(sous_prefecture, age_groupe)\n\n\nCréez un nouveau tableau récapitulatif en comptant le nombre de patients stratifiés par sous_prefecture et hospitalisation. Que se passe-t-il si vous modifiez l’ordre des arguments donnés à count() ?\nMaintenant, en utilisant count(), réponds aux questions suivantes :\n\nCombien de patients étaient des femmes ? Quelle est la proportion ?\nQuelles sont toutes les valeurs possibles de la variable statut_sortie ?\nCombien de patients âgés de 1 à 4 ans se sont rétablis ?"
  },
  {
    "objectID": "sessions_core/05_summary_table.fr.html#filtrer-les-valeurs-na",
    "href": "sessions_core/05_summary_table.fr.html#filtrer-les-valeurs-na",
    "title": "Tableaux récapitulatifs",
    "section": "Filtrer les valeurs NA",
    "text": "Filtrer les valeurs NA\nEn examinant les catégories du statut_sortie, vous devriez avoir remarqué que certaines patientes ont des valeurs manquantes (NA):\n\ndf_linelist |&gt;\n  count(statut_sortie) |&gt;\n  mutate(prop = n / sum(n))\n\n\nObservez le résultat du code ci-dessus. Comment pouvez-vous également calculer la proportion de patients décédés ? Êtes-vous satisfait de ce calcul ?\n\nLa proportion de cas décédés est également appelée taux de létalité. Pour calculer précisément le CFR, nous devons nous assurer que le dénominateur ne comprend que les patients dont nous sommes sûrs du résultat (c’est-à-dire que nous devons supprimer tous les cas avec “NA” ou “contre avis médical”).\nRappelons que nous pouvons le faire en utilisant filter(). Pour garder les valeurs manquantes (NA) dans une variable, nous pouvons utiliser la petite fonction is.na(statut_sortie). L’ajout d’un ! devant fera l’inverse : supprimer les valeurs manquantes de statut_sortie :\n\ndf_linelist |&gt;\n  filter(statut_sortie != \"sortie contre avis medical\", \n         !is.na(statut_sortie)) |&gt;\n  count(statut_sortie)\n\n\nQuelle autre instruction conditionnelle pourriez-vous utiliser dans filter() pour obtenir les mêmes résultats ?\n\nMaintenant que nous avons supprimé les patients dont l’issue est inconnue, nous pouvons ajouter ceci avant de créer notre tableau de fréquence pour obtenir le CFR correct.\n\nÀ l’aide de votre filtre, mettez à jour votre code pour résumer le nombre observé de patients qui ont survécu et sont décédés ainsi que le taux de létalité (proportion de décès). Stockez ce nouveau dataframe dans un objet, cfr_df.\n\n\n\n\n\n\n\nTip\n\n\n\nBonus. Une fonction de “raccourci” utile est drop_na() du package {tidyr} qui équivaut à filter(!is.na()).\n\ndf_linelist |&gt;\n  drop_na(statut_sortie) |&gt;\n  count(statut_sortie)\n\ndrop_na() est particulièrement utile car vous pouvez lui donner plusieurs noms de colonnes pour filtrer. Mais attention, cela supprimera toutes les lignes où une ou plusieurs de ces colonnes ont une valeur manquante."
  },
  {
    "objectID": "sessions_core/05_summary_table.fr.html#tableau-récapitulatif-statistiques-par-sous-préfecture",
    "href": "sessions_core/05_summary_table.fr.html#tableau-récapitulatif-statistiques-par-sous-préfecture",
    "title": "Tableaux récapitulatifs",
    "section": "Tableau récapitulatif : statistiques par sous-préfecture",
    "text": "Tableau récapitulatif : statistiques par sous-préfecture\nMaintenant que nous avons produit quelques tableaux de fréquence et de contingence simples, nous pouvons augmenter la complexité. Une tâche courante en épidémiologie consiste à examiner les statistiques résumées dans des sous-ensembles de données.\nPar exemple, on peut nous demander de produire des statistiques sur les patients au niveau des sous-préfectures, c’est-à-dire que pour chaque sous-préfecture dans les données, nous devons répondre aux questions suivantes :\n\nCombien de patients ont été consultés ?\nQuel est leur âge moyen ?\nQuelle a été la date d’admission la plus ancienne ?\nCombien de patients ont été hospitalisés ?\nParmi les enfants de moins de 6 mois, combien sont décédés ?\n\nC’est exactement pour cela que la fonction summarize() a été créée ! Elle nous permet de calculer des statistiques résumées sur un ensemble de données, et la syntaxe est similaire à celle de mutate() :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\ndf |&gt;\n  mutate(new_col = function_to_create(existing_col))\n\ndf |&gt;\n  summarize(.by = grouping_variable,\n            new_col = summary_function(existing_col))\n\nConsidérons le code suivant, où nous résumons les données pour calculer l’âge moyen de tous les patients.\n\ndf_linelist |&gt;\n  summarize(moy_age = mean(age))\n\n# A tibble: 1 × 1\n  moy_age\n    &lt;dbl&gt;\n1    6.82\n\n\nNotez que ce code donne une seule valeur pour l’âge moyen. Aucune variable de regroupement n’a été fournie, donc summarize() a renvoyé une statistique récapitulative pour l’ensemble du jeu de données. Pour calculer l’âge moyen par strate spécifique, nous devons spécifier une variable de regroupement en utilisant l’argument .by :\n\ndf_linelist |&gt;\n  summarize(.by = sexe, # Faire le résumé (ici, la moyenne) par sexe\n            moy_age = mean(age))\n\n# A tibble: 2 × 2\n  sexe  moy_age\n  &lt;chr&gt;   &lt;dbl&gt;\n1 f        6.77\n2 m        6.87\n\n\n\nJetez un œil aux résultats ci-dessus. Comment les interprétez-vous ?\n\nMaintenant que nous pouvons utiliser summarize(), nous pouvons l’utiliser pour calculer des statistiques récapitulatives appropriées par sous-préfecture. Commençons par appeler un summarize() vide et regrouper les données sur sous_prefecture.\n\nExécutez le code suivant :\n\ndf_linelist |&gt;\n  summarize(.by = sous_prefecture)\n\nQue se passe-t-il lorsque vous exécutez ces lignes ?\n\n\nComptages\nNous voulons d’abord examiner le nombre de cas dans chaque sous_prefecture. Cela peut être fait en utilisant la fonction d’aide n() :\n\ndf_linelist |&gt;\n  summarize(.by = sous_prefecture,\n            n_patients = n()  # pour compter\n)\n\n\nOk, maintenant construisons un tableau récapitulatif pour chaque sous-préfecture. Commençons par reproduire les lignes ci-dessus.\n\n\n\nRécapitulatifs des variables continus\nNous pouvons ensuite utiliser les fonctions mean(), median(), min(), max() (et autres) pour produire des récapitulatifs pour les variables continues. Par exemple, l’âge moyen :\n\ndf_linelist |&gt;\n  summarize(.by = sous_prefecture,\n            n_patients = n(),\n            mean_age = mean(age))\n\n\nAjoutez la date d’admission minimale à votre tableau pour chacune des sous_prefecture ? Êtes-vous satisfait des résultats ?\n\n\n\n\n\n\n\nTip\n\n\n\nN’oubliez pas qu’avec les fonctions arithmétiques telles que mean(), median(), min(), max(), vous devez indiquer explicitement à R de supprimer NA.\n\n\n\n\nComptage avec une condition\nNous pouvons également être intéressés par le nombre de patients (lignes) qui répondent à une condition : le nombre de patients de sexe féminin. Le comptage par condition logique peut être effectué avec la syntaxe suivante :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\nsummarize(sum_category = sum(LOGIC_TEST, na.rm = TRUE))\n\nCette somme nous permet de compter toutes les lignes où notre condition a été remplie (retourne TRUE). Par exemple :\nCette somme nous permet de compter toutes les lignes où notre condition a été remplie (retourne TRUE). Par exemple :\n\ndf_linelist |&gt;\n  summarize(.by = sous_prefecture,\n            n_female = sum(sexe == \"f\", na.rm = TRUE))\n\n\nAjoutez une variable à votre tableau qui compte le nombre de patients qui ont été hospitalisés. (c’est-à-dire : les lignes qui ont “oui” dans la variable “hospitalisation”)\n\n\n\nAutres statistiques\nParfois, nous voulons produire une statistique plus compliquée, par exemple l’âge moyen de tous les patients hospitalisés. Ici, la syntaxe est un peu différente :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\ndf |&gt;\n  summarize(mean_category = mean(col_to_use[LOGIC_TEST], na.rm = TRUE))\n\nIci, nous avons : - Indiqué quelle statistique de synthèse nous voulons utiliser (mean()) - Indiqué sur quelle colonne nous voulons calculer cette statistique (col_to_use) - Création d’une condition indiquant les observations de cette colonne à utiliser dans le calcul ([LOGIC_TEST])\nPour donner un exemple concret, si nous voulions calculer la moyenne de la variable age mais uniquement pour les patients hospitalisés (c’est-à-dire dans les lignes où hospitalisation == \"oui\") nous écririons :\n\ndf_linelist |&gt;\n  summarize(.by = sous_prefecture,\n            n_patients = n(),\n            moy_age_hosp = mean(age[hospitalisation == \"oui\"], na.rm = TRUE))\n\nL’utilisation d’un test logique dans l’exemple ci-dessus est appelée indexation logique, où une condition est essentiellement utilisée pour filtrer les observations que vous souhaitez prendre en compte lors d’un calcul. L’indexation logique est très puissante, mais elle peut aussi demander un certain temps d’adaptation, alors ne vous inquiétez pas trop si ce n’est pas parfaitement clair à ce stade.\n\nPouvez-vous utiliser cette syntaxe pour calculer l’âge moyen des patientes dans votre tableau ?\n\nC’est très bien ! Nous commençons à obtenir un tableau récapitulatif groupé assez exhaustif avec beaucoup d’informations utiles par “sous-préfecture” ! Un défi supplémentaire pour vous :\n\nDÉFI : Pourriez-vous ajouter une variable à votre tableau qui compte le nombre de patients décédés parmi ceux qui ont &lt; 6 mois.\n Note. Vous voulez compter les lignes (donc utiliser sum()) qui remplissent une condition spécifique pour le résultat (statut_sortie == \"deces\"), mais uniquement lorsque age_group == \"&lt; 6 months\"\n\n\n\nUtiliser la sortie\nEnfin, n’oubliez pas que summarize() renvoie un dataframe que nous pouvons ensuite manipuler davantage (par exemple : avec filter() et mutate()).\n\nAjoutez un mutate() après avoir produit votre tableau récapitulatif pour calculer :\n\nLa proportion de patients hospitalisés par sous-préfecture\nLa proportion de patientes par sous-préfecture\n\n\nL’en-tête de votre tableau final devrait ressembler à ceci :\n\n\n# A tibble: 6 × 11\n  sous_prefecture n_patients moy_age min_admission n_femme n_hosp moy_age_hosp\n  &lt;chr&gt;                &lt;int&gt;   &lt;dbl&gt; &lt;date&gt;          &lt;int&gt;  &lt;int&gt;        &lt;dbl&gt;\n1 Moissala              1808    6.84 2022-08-14        923    612         5.49\n2 Bouna                 1376    6.56 2023-01-11        669    412         5.67\n3 Bedjondo               534    7.07 2023-06-09        251    184         5.21\n4 Bekourou               496    6.84 2023-06-17        251    164         6.04\n5 Bedaya                 435    7.10 2023-07-04        209    147         6.16\n6 Koumra                 253    7.11 2023-08-14        138     84         6.26\n# ℹ 4 more variables: moy_age_femme &lt;dbl&gt;, n_deces_moins_6m &lt;int&gt;,\n#   prop_female &lt;dbl&gt;, prop_hosp &lt;dbl&gt;"
  },
  {
    "objectID": "sessions_core/05_summary_table.fr.html#cest-fait",
    "href": "sessions_core/05_summary_table.fr.html#cest-fait",
    "title": "Tableaux récapitulatifs",
    "section": "C’est fait !",
    "text": "C’est fait !\nVous devriez être fiers de vous, la création de tableaux récapitulatifs est une compétence importante pour un épidémiologiste, et le faire en R est très efficace ! N’oubliez pas de sauvegarder votre code !\n\n\n\n Solutions exercices"
  },
  {
    "objectID": "sessions_core/05_summary_table.fr.html#pour-aller-plus-loin",
    "href": "sessions_core/05_summary_table.fr.html#pour-aller-plus-loin",
    "title": "Tableaux récapitulatifs",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\n\nExercices supplémentaires\n\nCréez un tableau récapitulatif qui calcule les statistiques suivantes par groupes d’âge :\n\n\nLe nombre de patients\nLa proportion d’hommes\nLe nombre de décès\nLe CFR\nLe nombre de décès parmi les patients atteints de pneumonie\n\n\nFaites un tableau qui montre la proportion de patients par âge ayant reçu un vaccin contre la rougeole (par rappel oral ou par carte) et ceux qui ont reçu 1 ou 2 doses.\nFaites un tableau comparant la proportion de patients hospitalisés et non hospitalisés présentant un TDR positif pour le paludisme, de la fièvre, une éruption cutanée, une toux, des yeux rouges, une pneumonie, une encéphalite et un MUAC « rouge » ou « jaune » (moins de 125 mm).\nCalculer le nombre moyen de jours entre l’apparition des premiers symptômes et la consultation par sous-préfecture.\nCalculer le temps moyen passé à l’hôpital (i.e. jours entre l’admission et le résultat) par résultat (i.e. chez ceux qui ont guéri et ceux qui sont décédés).\n\n\n\nRessources supplémentaires\n\nLe chapitre du manuel EpiR sur le regroupement des données\nUne fois que vous avez des tableaux, vous pouvez les personnaliser en profondeur pour l’affichage/la publication à l’aide du paquetage {gt} :\n\nSite web de gt\nLivre sur gt"
  },
  {
    "objectID": "sessions_core/06_epicurves.fr.html",
    "href": "sessions_core/06_epicurves.fr.html",
    "title": "Introduction to data visualization with ggplot2",
    "section": "",
    "text": "Découvrir les bases de la visualisation de données en R avec le package {ggplot2}\nConstruire une courbe épidémique simple"
  },
  {
    "objectID": "sessions_core/06_epicurves.fr.html#objectifs",
    "href": "sessions_core/06_epicurves.fr.html#objectifs",
    "title": "Introduction to data visualization with ggplot2",
    "section": "",
    "text": "Découvrir les bases de la visualisation de données en R avec le package {ggplot2}\nConstruire une courbe épidémique simple"
  },
  {
    "objectID": "sessions_core/06_epicurves.fr.html#introduction",
    "href": "sessions_core/06_epicurves.fr.html#introduction",
    "title": "Introduction to data visualization with ggplot2",
    "section": "Introduction",
    "text": "Introduction\nPour cette dernière session, nous allons vous donner une courte introduction à la visualisation de données à l’aide du package {ggplot2}, un outil populaire. Gardez en tête que la visualisation de données est un énorme sujet, et {ggplot2} un vaste package et il n’est pas réaliste de tout traiter en trois heures. La session d’aujourd’hui est une introduction que nous espérons douce aux concepts de base de la visualisation, en prenant pour objet un graphe fameux en épidémiologie, la courbe épidémique.\nNotre visualisation finale ressemblera à ceci :"
  },
  {
    "objectID": "sessions_core/06_epicurves.fr.html#setup",
    "href": "sessions_core/06_epicurves.fr.html#setup",
    "title": "Introduction to data visualization with ggplot2",
    "section": "Setup",
    "text": "Setup\nDépendances. Cette session suppose que vous savez utiliser RStudio, que vous êtes capable d’importer des données et que vous connaissez les verbes de base de manipulation des données que nous avons vus dans les sessions de base jusqu’à présent. Si vous avez besoin d’un rappel sur l’un de ces sujets, nous vous encourageons à revoir les sessions de base du parcours d’apprentissage.\n\nCette session utilisera la version nettoyée de l’ensemble de données Moissala sur la rougeole.\n\n\n\n  Course Folder\n\n\n\n Ouvrez votre projet Rstudio du cours et créez un nouveau script appelé “courbe_epi.R” avec les métadonnées appropriées. Enregistrez le dans R/. Pour cette session, nous aurons besoin de charger les packages {here}, {rio}, {dplyr}, {lubridate}, et{ggplot2}. Ajoutez une section # IMPORTATION DONNÉES où vous importez les données nettoyées du cours (linelist_moissala_clean_FR.RDS)."
  },
  {
    "objectID": "sessions_core/06_epicurves.fr.html#les-paradigmes-de-la-création-de-graphiques",
    "href": "sessions_core/06_epicurves.fr.html#les-paradigmes-de-la-création-de-graphiques",
    "title": "Introduction to data visualization with ggplot2",
    "section": "Les paradigmes de la création de graphiques",
    "text": "Les paradigmes de la création de graphiques\nIn R, and indeed in everything, there are a loIl y a de nombreuses approches à la visualisation de données, en général et en R en particulier. Les deux plus grands paradigmes sont :\n\nTout en un : cette approche est caractérisée par l’existence d’une fonction (en général complexe) pour gérer tous les aspects de la construction d’un graphique. Base R par exemple, utilise cette approche (et n’est pas le seul).\nGraphiques en couches (ou modulaires) : le graphique est décomposé en éléments (formes, titres, barres d’erreurs, thèmes…) associées à des couches. Différentes fonctions ajoutent ou modifient ces éléments. Ce paradigme est utilisé par les packages {ggplot2}, {highcharter}, ou {echarts4r} et un certain nombre d’outils modernes.\n\nUne discussion approfondie sur les raisons pour lesquelles on peut utiliser une approche plutôt qu’une autre dépasse le cadre de ce cours, mais nous noterons que la plupart des paquets de visualisation modernes ont tendance à utiliser un [modèle en couches] {.hovertip bs-toggle=‘tooltip’ bs-title=“C’est parce que les modèles en couches ont tendance à être plus pratiques lors de la construction de visualisations complexes ou hautement personnalisées.”}. En gardant cela à l’esprit, examinons les types de couches dont nous parlons dans notre approche « en couches ».\n\nDécomposition d’un graphique\nDans ce tutoriel, nous décomposons les graphiques en quatre composantes (couches) :\n\nLe canevas / les données\nLes formes géométriques primaires\nLes titres et labels\nLe thème\n\nOn peut illustrer ces composants avec la courbé épidémique schématique suivante :\n\n\n\n\n\nLa première couche, le caneva (ou la toile) est fondamentale. Comme un artiste prépare sa toile vierge et ses outils avant de se lancer dans une peinture, R doit en premier lieu créer un canevas prêt à accueillir les éléments de représentation graphique. C’est lors de la création du canevas que nous indiquons à R que nous voulons créer un graphique, et avec quelles variables.\nIci, nous allons spécifier à R que nous voulons un graphique où l’axe horizontal représente la date, et l’axe vertical représente le nombre de cas. Une fois le canevas mise en place, nous ajouterons d’autres couches, comme un artiste ajouterait de la peinture, leur signature ou un cadre.\n\n\nOssature d’un ggplot\nLa recette pour construire un ggplot (un graphe produit par le package {ggplot}) est de la forme suivante :\n\nCréation d’un canevas à l’aide de ggplot(aes(...))\nAjout de couches sur le canevas avec +\n\nNotez que {ggplot2} utilise l’opérateur + pour ajouter des couches sur le graphe.\nLa syntaxe générale d’un ggplot est :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\ndf |&gt;                    # passer les données\n  ggplot(aes(x = ...,    # étape 1 : créer le canevas\n             y = ...)) +\n  couche_1(...) +        # étape 2 : ajout de la première couche\n  couche_2(...) +        # étape 3 : ajout d'une autre couche\n  ...                    # continuer à ajouter des couches...\n\nLe nombre de couches à ajouter dépend de la complexité du graphique que vous souhaitez créer. Dans notre cas, nous ajouterons trois couches en utilisant les fonctions suivantes :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\ndf |&gt;                    # passer les données\n  ggplot(aes(x = ...,    # étape 1 : créer le canevas\n\n             y = ...)) +\n  geom_col(...) +        # étape 2 : ajout des formes (barres)\n  labs(...) +            # étape 3 : ajouter des titres\n  theme_classic(...)     # étape 4 : amélioration du thème\n\nNous pouvons mettre à jour notre précédent schéma avec ces fonctions :\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDans l’exemple précédent, nous passons le jeu de données à la fonction ggplot() à l’aide de l’opérateur pipe (comme nous l’avons souvent fait avec d’autres fonctions). C’est possible car le premier argument nécessaire de la fonction est le dataframe contenant les variables à représenter. Soyez attentifs, il est facile de se tromper et de chercher à utiliser un + à la place du |&gt;\n\n\nDans la section suivante, nous allons décrire les différentes étapes plus en détail, en utilisant notre jeu de données rougeole à Moissala pour faire notre première courbe épidémique."
  },
  {
    "objectID": "sessions_core/06_epicurves.fr.html#sec-epicurve-steps",
    "href": "sessions_core/06_epicurves.fr.html#sec-epicurve-steps",
    "title": "Introduction to data visualization with ggplot2",
    "section": "Votre premier ggplot",
    "text": "Votre premier ggplot\n\nPréparer vos données : Agrégation par jour\nNous aimerions tracer une courbe des cas quotidiens. Vous l’aurez peut-être remarqué, nos données actuelles sont quotidiennes, mais il est évident que plusieurs cas peuvent se produire certains jours. Donc,il faut agréger les données par jour. Heureusement, vous avez déjà appris à résumer les données lors des sessions précédentes.\n\nEn utilisant count(), créez un nouveau dataframe appelé df_cases qui résume le nombre total de cas observés par jour. L’en-tête de ce cadre de données devrait ressembler à ceci :\n\n\n  date_debut n\n1 2022-08-13 1\n2 2022-08-17 1\n3 2022-08-18 1\n4 2022-08-22 1\n5 2022-08-30 2\n6 2022-09-01 1\n\n\n\nBien !\nDans les étapes suivantes, vous allez utiliser df_cas pour tracer une courbe épidémique du nombre de cas par semaine. En revanche, les exemples données dans les exercices pour illustrer le fonctionnement des fonctions seront faits sur le nombre de hospitalisations par semaine. Pour cela, j’utiliserai un dataframe df_hopital, qui ressemble à ceci :\n\n\n  date_admission patients\n1     2022-08-14        1\n2     2022-08-25        1\n3     2022-09-02        1\n4     2022-09-06        1\n5     2022-09-09        1\n6     2022-09-10        1\n\n\n\n\nCréer le canevas\nLa première étape est de créer votre “canevas” en spécifiant votre jeu de données et le nom des colonnes que vous voulez représenter sur le graphique. Cela est fait à l’aide de la fonction ggplot(aes()) selon la syntaxe suivante :\n\n# NE PAS EXÉCUTER (PSEUDO-CODE)\ndf_data |&gt;\n  ggplot(aes(x = x_axis_variable_name,\n             y = y_axis_variable_name))\n\nPour l’exemple, je vais placer la date (date_admission) sur l’axe des x et le nombre de patients (patients) sur l’axe des y :\n\ndf_hopital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients))\n\n\n\n\n\n\n\n\nDans Rstudio, ce graphique devrait apparaître dans l’onglet “Plots” dans le panneau en bas à droite (par défaut) :\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nJust like with {dplyr}, we write our column names without quotation marks. This is unsurprising as {ggplot2}, like {dplyr}, is a member of the {tidyverse} and therefore uses similar syntax.\n\n\nQu’est ce que cette fonction aes() que nous avons imbriqué dans la fonction ggplot() ?\nLa fonction aes() n’est jamais utilisée seule, elle est toujours passée à ggplot(). Elle sert à faire correspondre les variables du jeu de données aux éléments visuels du graphique (en anglais on parle de “mapping”, qui est occasionnellement traduit par “mappage”). Les plus basiques de ces éléments graphiques sont les axes, mais on peut aussi définir comment la couleur ou la taille d’éléments varie en fonction de variables dans les données (par exemple, statut à la sortie).\n\nCréez une nouvelle section # PLOT COURBE EPI. Ensuite, en vous inspirant de l’exemple précédent, créez la base d’un ggplot avec le dataframe df_cas, et définissez l’axe des x et des x.\n\nPour le moment, le résultat devrait ressembler à ceci :\n\n\n\n\n\n\n\n\n\nTrès bien. Maintenant, ajoutons les barres\n\n\nAjouter les formes\nMaintenant que la toile est prête, commençons à dessiner dessus, et ajoutons des formes. Dans {ggplot2}, les formes géométriques sont surnommées des “géométries” ou “geom” en raccourci, et représentent les données. Les geoms les plus courants sont :\n\nDiagrammes en bâtons (geom_col() or geom_bar())\nHistogrammes (geom_histogram())\nNuages de points(geom_point())\nCourbes(geom_line())\nDiagramme en boîte à moustache (boxplots) (geom_boxplot())\n\nAujourd’hui nous allons nous concentrer sur les diagrammes en bâton, pour créer une courbe épidémique. Nous allons utiliser la fonction geom_col().\nNous allons maintenant rajouter les barres à la courbe des cas hospitalisés. Rappelez-vous que l’on ajoute une nouvelle couche à notre objet ggplot à l’aide de +.\n\ndf_hopital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col()\n\n\n\n\n\n\n\n\nC’est génial, cela ressemble vraiment à une épicurve. Bien qu’elle ait l’air un peu… grise. Si nous voulons mettre à jour la couleur de nos barres (appelée le fill), nous devons simplement ajouter l’argument fill to geom_col().\nFaisons un essai :\n\ndf_hopital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\")\n\n\n\n\n\n\n\n\n\nMettez à jour votre graphe pour ajouter les barres avec la couleur #2E4573.\n\nVotre graphe devrait ressembler à ceci :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDans {ggplot2}, les couches doivent être ajoutées à un objet ggplot existant (le canevas définit à l’étape 1). Exécuter la fonction geom_col() toute seule ne produira pas un graphe. Si l’on reprend notre analogie avec la peinture, ce serait comme essayer d’utiliser la peinture sans support (toile).\n\n\nCe graphe s’améliore d’instant en instant ! Maintenant il est temps de le rendre un petit peu plus informatif…\n\n\nAjouter les titres\nUn bon graphique doit avoir des titres et des labels informatifs or pour le moment, ce n’est pas le cas de nos graphiques (n n’est pas très informatif).\nLa fonction lab() permet d’ajouter des titres et labels à plusieurs éléments du graphique :\n\nTitre des axes (x = ety =)\nTitre du graphique (title =)\nCaption\n\nComme avec les autres couches, nous pouvons ajouter la couche contenant les titres et labels à notre graphe avec le signe + :\n\ndf_hopital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date\",\n       y = \"Patients par Jour\",\n       title = \"Hospitalisations pour rougeole dans la région de Madoul (Tchad)\")\n\n\n\n\n\n\n\n\n\nAjoutez des titres raisonnables à votre graphe.  Bonus. Ajoutez une source des données en utilisant caption.\n\nVotre graphe pourrait maintenant ressembler à celle-ci (par exemple) :\n\n\n\n\n\n\n\n\n\n\n\nChanger le thème\nLe thème de base de ggplot n’est pas très attractif, et la taille des polices est trop petite pour être lisible sur la majorité des supports. Si vous voulez utiliser votre graphique dans des rapports ou des présentations, il vaudrait mieux améliorer son apparence.\nPour cela, il suffit d’ajouter une couche “thème” à notre graphe (la dernière couche pour aujourd’hui !). Si le nom des fonctions des geoms commençait toujours par geom_, le nom de toutes les fonctions de thème commence par theme_. Il existe plusieurs thèmes prédéfinis, et vous pouvez aller les regarder sur le site de {ggplot2}.\nAujourd’hui, nous allons utiliser theme_classic(), qui offre une alternative élégante au thème de base :\n\ndf_hopital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date\",\n       y = \"Patients par Jour\",\n       title = \"Hospitalisations pour rougeole dans la région de Madoul (Tchad)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nC’est déjà plus joli. Maintenant, nous voudrions augmenter la taille de la police. Nous pouvons faire ça en ajustant la taille de la police à l’aide de l’argument base_size:\n\ndf_hopital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date\",\n       y = \"Patients par Jour\",\n       title = \"Hospitalisations pour rougeole dans la région de Madoul (Tchad)\") +\n  theme_classic(base_size = 17)\n\n\n\n\n\n\n\n\n’est beaucoup mieux !\nRappelez-vous que la taille de la police doit être choisie en fonction de la destination du graphe (présentation, rapport informel, rapport final ?). Il en va de même pour le choix du thème, qui reste un choix partiellement subjectif. Il existe des principes de visualisation qui peuvent guider vos choix lors de la création d’un graphe (ou d’une table), mais la visualisation de données est autant un art qu’une science.\n\nAjoutez une dernière couche à votre graphe pour ajouter un thème de votre choix, avec une taille de police plus appropriée.\n\n\n\nSauvegarder votre graphique\nSi vous souhaitez enregistrer votre graphe, vous pouvez cliquer sur le bouton « Exporter » dans le panneau de tracé de RStudio :"
  },
  {
    "objectID": "sessions_core/06_epicurves.fr.html#la-fin",
    "href": "sessions_core/06_epicurves.fr.html#la-fin",
    "title": "Introduction to data visualization with ggplot2",
    "section": "La Fin !",
    "text": "La Fin !\nBravo! Vous avez créé votre première courbe épidémique en R !\n\n\n\n Solutions file"
  },
  {
    "objectID": "sessions_core/06_epicurves.fr.html#aller-plus-loin",
    "href": "sessions_core/06_epicurves.fr.html#aller-plus-loin",
    "title": "Introduction to data visualization with ggplot2",
    "section": "Aller plus loin",
    "text": "Aller plus loin\n\nExercices supplémentaires\n\nUtilisez le theme_minimal() sur un de vos graphiques, avec une police de taille de base de 18.\nAllez sur ce site, choisissez une couleur et mettez à jour la couleur de vos barres.\n\n\n\nExercices de défi\n\nAu lieu d’agréger par date, comptez le nombre de patients par sous-préfecture. Essayez d’adapter votre code pour créer un diagramme à barres du nombre de patients par sous-préfecture.\n\n\n\nSatellites\n\nCourbes épidémiques hebdomadaires\nGraphiques multiples (facetting)"
  },
  {
    "objectID": "sessions_core/06_epicurves.fr.html#ressources",
    "href": "sessions_core/06_epicurves.fr.html#ressources",
    "title": "Introduction to data visualization with ggplot2",
    "section": "Ressources",
    "text": "Ressources\n\nUn livre complet sur l’utilisation de {ggplot2}.\n\nUn chapitre entier sur les épicurves"
  }
]