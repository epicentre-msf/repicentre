[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "{repicentre}",
    "section": "",
    "text": "Welcome to {repicentre}\nAn open source platform to learn R for humanitarian contexts. What would you like to do?\n\n\n\n\n\nLearn Follow a linear path starting with the basics  Start\n\n\n\n\n\nExplore Browse our full catelogue of self paced tutorisals  Start\n\n\n\n\n\nExpand Go even further with a list of external resouces  Start"
  },
  {
    "objectID": "sessions_core/02_import_data.html",
    "href": "sessions_core/02_import_data.html",
    "title": "Data Importation",
    "section": "",
    "text": "Create a RStudio Project\nSet up an organized and well documented code\nInstall and load packages\nWrite robust file paths\n\nImport and inspect data\n\n\n\n\n\n\n\nImportant\n\n\n\nThe principles you learned in the Introduction to R session will apply here as well: we should do our best to ensure that our projects won‚Äôt just work today but can also be reused and shared in the future. While doing this is not always easy, there are several best practices that can help us, and one of the most important is to start with a good, organized code base."
  },
  {
    "objectID": "sessions_core/02_import_data.html#objectives",
    "href": "sessions_core/02_import_data.html#objectives",
    "title": "Data Importation",
    "section": "",
    "text": "Create a RStudio Project\nSet up an organized and well documented code\nInstall and load packages\nWrite robust file paths\n\nImport and inspect data\n\n\n\n\n\n\n\nImportant\n\n\n\nThe principles you learned in the Introduction to R session will apply here as well: we should do our best to ensure that our projects won‚Äôt just work today but can also be reused and shared in the future. While doing this is not always easy, there are several best practices that can help us, and one of the most important is to start with a good, organized code base."
  },
  {
    "objectID": "sessions_core/02_import_data.html#setting-up-your-project",
    "href": "sessions_core/02_import_data.html#setting-up-your-project",
    "title": "Data Importation",
    "section": "Setting up your Project",
    "text": "Setting up your Project\n\nFolder Structure\n\nIf not done already, download and unzip the course folder. Save the uncompressed folder to a location that is not connected to OneDrive and navigate into it.\n\n\n\n  Course Folder\n\n\n\n\nThis folder gives an example of a typical (and highly recommended) structure for your code projects:\n\nüìÅ data\n\nüìÅ clean\nüìÅ raw\n\nüìÅ R\nüìÅ outputs\n\nThis folder will be you working directory for all the sessions of this course. You will create an Rstudio project in it (explanations below), and save all your scripts in /R. The course datasets are already in data/raw.\n\n\nDefinitions\nTo work through this session you need to understand the two following concepts:\nWorking directory. The working directory is the location (folder) where your R session is actively working. If you save a file, for example, it will be saved into this folder by default. Similarly, when you want to open a file, this folder will be shown by default. All relative paths will be relative to this working directory. By default, R usually picks the ‚ÄúDocuments‚Äù folder as the working directory on Windows machines.\nRoot. The root refers to the top-most folder level of the working directory. If your course folder was called FETCHR, the root would then be directly inside it (as opposed to being inside one of its subfolders like R or Data).\n\n\nRStudio Projects\nAn RStudio Project can be used to make your life easier and help orient RStudio around the various files used in your code\nAs a quick reminder, your interface should look something like this:\n\n\n\n\n\n\nFigure¬†1: Screenshot of a typical Rstudio interface\n\n\n\n\nOpen RStudio and create a new project by clicking File &gt; New Project &gt; Existing Directory &gt; Browse, navigating into (opening) the course folder, and clicking Create Project.\n\n\nIn the Windows Explorer, look at the course folder. You should now see a new file with the extention .Rproj that has a small blue icon with an R in it.\n\n\n\n\nIcon associated with RStudio projects\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you don‚Äôt see this file, it‚Äôs probably because it is hidden by default on your computer. To change this setting in Windows Explorer, go into the View menu and select Filename Extensions.\n\n\nWhen you open an RStudio Project, RStudio starts a new R session, opens the associated project files, and sets your working directory to the root of the course folder. At this time, RStudio also displays the subfolders of this directory in the panel on the bottom right.\n\n\n\n\n\n\nTip\n\n\n\nIt is strongly recommended to set up a separate RStudio Project for each of your analyses to ensure that your project files remain organized and manageable.\n\n\nThere are several ways to open an RStudio Project:\n\nUse the RStudio menu File &gt; Open Project and then select the relevant .Rproj file\nClick on the Project: (none) button on the top right of the RStudio interface\nNavigate in the folder explorer to the analysis folder and double click on the file with the .Rproj extension\n\n\n\nRStudio Options\nBefore continuing, let‚Äôs update some of RStudio‚Äôs problematic default settings:\n\nOpen the global options (Tools &gt; Global Options) and open the tab General (left menu). Make sure that none of the boxes in the sections R Sessions, Workspace, or History are checked.\n\n\n\n\nScreenshot of the Rstudio options\n\n\nWhen checked, these options cause RStudio to save the objects in your environment and reload them as well as any files you previously had open when you open a new R session. While these default may seem like a good idea, it is better to always start your work from a fresh, empty R session to avoid bugs.\n\n\n\n\n\n\nImportant\n\n\n\nRemember that any commands or outputs that is needed for the cleaning and analysis should be saved explicitly in a script in the correct, functional order."
  },
  {
    "objectID": "sessions_core/02_import_data.html#creating-a-new-script",
    "href": "sessions_core/02_import_data.html#creating-a-new-script",
    "title": "Data Importation",
    "section": "Creating a New Script",
    "text": "Creating a New Script\n\nOpen a new script and save it in the R folder of your project under the name import_data.R.\nAdd some metadata to the top of the script as seen in the first session using comments. Be sure to include:\n\nTitle\nAuthor\nCreation Date\nDescription\n\n\nNow you‚Äôre ready to start coding!"
  },
  {
    "objectID": "sessions_core/02_import_data.html#sec-packages",
    "href": "sessions_core/02_import_data.html#sec-packages",
    "title": "Data Importation",
    "section": "Packages",
    "text": "Packages\nPackages are collections of functions that extend the functionality of R. You‚Äôll use them a lot, both in this course and in your daily life. Fortunately, as an open source language, R packages can be downloaded and installed for free from the internet.\n\n\n\n\n\n\nNote\n\n\n\nIn R, packages are referenced using {}. For example {ggplot2} is the name of the ggplot2 package that contains new plotting functions such as ggplot(), geom_point() etc‚Ä¶\n\n\n\nInstallation\nWe can install a new package using the function install.packages(), which downloads and installs it into the package library on your computer. This is done once per computer.\n\ninstall.packages(\"here\") # install the {here} package\n\nDon‚Äôt forget to wrap the package name in quotation marks when using install.packages(). What happens if you don‚Äôt do this?\n\n\n\n\n\n\nNote\n\n\n\nIf you are following this session as part of a course, to avoid any potential internet connectivity issues during the training we already had you install most of the course packages.\nIf are following this tutorial on your own or have not installed the packages yet, you will have to manually install each new package that we encounter.\n\n\n\n\nUsage\nOnce a package is installed we can use it but we have to specify to R that we will be using it every single session. This process is called loading the package and is achieved using the function library().\n\nlibrary(here) # load the \"here\" package\n\n\nUse the library() function to load the packages here and rio, which will be used in the next section.\n\nBased on your computer‚Äôs set up and the package you are trying to load, you may get a warning message noting that some functions have been masked or that the current version of the package was built for a different version of R. These messages are not usually a problem but are still important to note.\n\nTry to run the following code. Can you work out what the error means?\n\nlibrary(ggplot)\n\n\nThe above code throws an error because you have asked for a library that doesn‚Äôt exist. Remember that R is fickle and case sensitive and many of your errors will come from small typos in the names of functions or objects. Here, for example, we wanted to load the package ggplot2 but wrote ggplot instead.\n\n\n\n\n\n\nTip\n\n\n\nMost of the time, you‚Äôll need to load a number of packages for your script and it is recommended to have a section at the start of your code that loads everything you‚Äôll need in one place:\n\n# Packages ----------------------------\nlibrary(tidyverse)   # data manipulation\nlibrary(lubridate)   # date manipulation\n\nThis practice makes it easy to tell which packages need to be installed to run a script.\n\n\n\nUse comments to create a ‚ÄúPackages‚Äù section to your script.\n\n\n\nUpdating Packages\nR has a very active community of developers and it‚Äôs pretty common for packages to be updated from time to time as their owners add in new functions and fix existing bugs. In order to update the packages in your library, you can go into the Packages tab of the bottom right panel and click Update. Don‚Äôt forget that you‚Äôll need to be connected to the internet during this process.\n\n\n\n\n\n\nImportant\n\n\n\nSometimes packages are updated in a way that might remove or change a function that you used in some of your scripts, causing your code to no longer work. Don‚Äôt panic if this happens. The best practice is to adapt your code, although in the worst case scenario you can forcibly install an old version of a package. This is however out of the scope of this session."
  },
  {
    "objectID": "sessions_core/02_import_data.html#data-importation",
    "href": "sessions_core/02_import_data.html#data-importation",
    "title": "Data Importation",
    "section": "Data Importation",
    "text": "Data Importation\n\nUnderstanding File Paths\nTo open a file in R you need to provide a file path. A file path is simply a longer name for a file, that includes not only its name but also its location on your computer. There are several ways of defining these paths, including absolute and relative paths.\n\nAbsolute Paths\nAbsolute paths are specific to your computer and go all the way up to the level of your hard drive. For example, an absolute path may look something like this: D:/OneDrive - MSF/Documents/monitoring/cholera/fancy_project/data/raw/example_linelist.xlsx. Clearly, this path will only work on one specific computer.\nThe use of hard coded absolute paths is strongly discouraged as it makes your code inflexible and prone to break: the paths need to be updated every time your code is shared or the project folder is moved on your computer.\n\n\nRelative Paths\nRelative paths are defined relatively to your current working directory. For example, keeping in mind that our handy .Rproj file set our working directory to the root of our project folder, we could create a relative path that looked like data/raw/example_linelist.xlsx. This means that as long as we maintain the internal structure of our project folder and have an .Rproj file, our code would theoretically run on multiple computers.\n\n\nRobust Paths with the here() function\nThe {here} package has a here() function that really helps defining paths. It has two advantages:\n\nWhen used with RStudio projects, you can give it only the part of the path within the project, (the relative path in other words), and the function uses it to create the absolute path dynamically.\nIt does so using the separator adapted to you operating system, whether it‚Äôs /, \\, or //\n\n\nlibrary(here)\nhere(\"data\", \"raw\", \"example_linelist.xlsx\")\n\n[1] \"/tmp/RtmpRJBDYU/file780468e75bd3/data/raw/example_linelist.xlsx\"\n\n\nSee how we only defined the relative path and the function created an absolute path. This way of defining the path will work on your colleagues computer, even if they run on another operating system, as long as you both respect the internal structure of the working directory.\nWe strongly encourage you to use here() whenever you need to create a file path.\n\nRun the above code in the console. What file path does here(\"data\", \"raw\") give you?\n\n\nUsing here(), create a complete file path for the file Moissalla-measles-linelist-EN.xlsx. Keep this path around, we will use it soon.\n\n\n\n\n\n\n\nImportant\n\n\n\nhere() simply creates a file path, it doesn‚Äôt actually check if a file exists on your computer: if the file is absent or there is a typo in your code, the command will yield an error when the path is used. If you would like to use a function to check if a file exists, check out the file.exists() function.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe will often want to source multiple data files in a single project. To make that process easier, it can be helpful to create a section at the start of the script, after loading the packages to define paths and store them in variables.\n\n\n\n\n\nImport function\nIn R different file formats are often imported using different, often specialized functions. This can be tedious as it requires you to memorize and load a large number of functions just to get your data imported. To avoid this problem, we recommend that you use the import() function from the package {rio}. This function is able to open a large variety of files (including Excel, csv, Stata, and many others) by recognizing the file extension of your data and calling a relevant specialized function from another package so that you don‚Äôt have to.\nBecause import() is actually just calling other functions in the background, it is possible that it will need different arguments depending on the type of file you want to load.\n\n\n\n\n\n\nTip\n\n\n\nTo see the full list of all the file types you can load (and save!) with rio, check out the list of supported formats on their website. In the rest of the lesson we will focus on importing data from Excel .xlsx files.\n\n\n\nImporting from the First Sheet\nIn general, the usage of import() is pretty simple, at minima you need to pass the path of the file to the file argument\n\nimport(file = here(\"data\", \"raw\", \"example_linelist.xlsx\"))\n\nNotice that we have nested the command here() inside the import() command. Nesting functions is absolutely allowed in R and is something you will do all the time. When functions are nested, R will evaluate them in the order of the innermost function (in this case here()) to the outermost (in this case import()). In this way, the output of here() is being used as the input of import().\n\nImport the file Moissalla-measles-linelist-EN.xlsx that is in your raw data subfolder into R using here() and import().\n\nIf your import worked correctly, R will print the data into the console but not save it into the environment because we have not assigned them to an object.\n\n\n\n\n\n\nTip\n\n\n\nYou may not want to have R print very large datasets into the console and assign them directly to an object.\n\n\n\nReimport your data but this time save it to an object called df_linelist.\n\n\n\nImporting Data from Any Sheet\nAs you just saw, R selects the first sheet by default. It is however possible to pass the number (or name) of a specific worksheet in your Excel data to import() using the argument which:\n\nimport(file = here(\"data\", \"raw\", \"example_linelist.xlsx\"),\n       which = 2)  # imports the second sheet\n\nNote that the which argument is specific to the file types that have multiple sheets, such as Excel or .Rdata files. If you try to use it on a .csv file the argument will be ignored."
  },
  {
    "objectID": "sessions_core/02_import_data.html#taking-a-first-look-at-your-data",
    "href": "sessions_core/02_import_data.html#taking-a-first-look-at-your-data",
    "title": "Data Importation",
    "section": "Taking a First Look at your Data",
    "text": "Taking a First Look at your Data\nWe have now imported a dataset into R and assigned it to a dataframe (df_linelist). The natural next step is to inspect this dataset, to check that the import went well, get to know it a bit better, and assess if it requires any cleaning before analysis.\nWe can start by taking a quick look at the first few lines of the dataframe using the function head(). This function takes a dataframe as its first argument and optionally accepts a second argument n indicating the number of lines we would like to see.\n\nhead(df_linelist, n = 10) # Inspect 10 first lines\n\n\nUse head() to examine the 12 first lines of df_linelist.\n\nWe can also check out our data by looking at the Environment tab of the top-right panel. Here, we can see our dataframe in the environment, look at its structure, or open it in the data viewer of RStudio.\n\nClick on the round blue button next to df_linelist in your environment to see its structure. Then click on the name of the dataset to open it in the viewer.\n\nThe data viewer displays dataframes as tables and is a convenient way to quickly look at your data. You can even sort and filter your data in the ‚ÄúView‚Äù, though be aware that these actions will not make any changes to the actual object df_linelist. The View can also be opened by passing the dataframe to the function View()."
  },
  {
    "objectID": "sessions_core/02_import_data.html#done",
    "href": "sessions_core/02_import_data.html#done",
    "title": "Data Importation",
    "section": "Done!",
    "text": "Done!\nWell done and don‚Äôt forget to save your code.\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_core/02_import_data.html#going-further",
    "href": "sessions_core/02_import_data.html#going-further",
    "title": "Data Importation",
    "section": "Going Further",
    "text": "Going Further\n\nExtra Exercises\n\nUse dim() to take a look at the dimensions of your dataset.\nUse str() to check the data type of each column. Does anything look odd? Remember that you can also use functions like is.character() and is.numeric() if you‚Äôd like to test the type of a particular column.\nUsing a function learned in the first session, can you extract the names of the columns of the dataset? Do these results match what you see when you open the data in Excel?\nTry passing your dataframe to the function summary(). What does this function tell you?\n\n\n\nAdditional Resources\n\nThe {rio} website\nMore examples on importing data of various file types"
  },
  {
    "objectID": "sessions_core/03_data_verbs.html",
    "href": "sessions_core/03_data_verbs.html",
    "title": "Data Manipulation, Basics",
    "section": "",
    "text": "Learn basic data verbs from {dplyr} to:\n\nSelect specific columns (select())\nRename columns (rename())\nAdd new columns and change existing ones (mutate())\nRemove duplicate observations\n\nUnderstand the pipe operator |&gt;"
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#objectives",
    "href": "sessions_core/03_data_verbs.html#objectives",
    "title": "Data Manipulation, Basics",
    "section": "",
    "text": "Learn basic data verbs from {dplyr} to:\n\nSelect specific columns (select())\nRename columns (rename())\nAdd new columns and change existing ones (mutate())\nRemove duplicate observations\n\nUnderstand the pipe operator |&gt;"
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#setup",
    "href": "sessions_core/03_data_verbs.html#setup",
    "title": "Data Manipulation, Basics",
    "section": "Setup",
    "text": "Setup\nDependencies. This session assumes that you know how to use RStudio and that you are able to import data. If you need a refresher on either of these topics, we encourage you to review the first two sessions in the learning pathway.\n\nThis session will work with the raw Moissala linelist data, which can be downloaded here:\n\n\n\n  Download Data\n\n\n\n Make sure this dataset is saved into the appropriate subdirectory of your R project and create a new script called data_verbs_practice.R in your R directory. Add an appropriate header and load the following packages: {here}, {rio}, and {tidyverse}.  Finally, add an import section where you use {here} and {rio} to load your data into an object called df_raw."
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#manipulating-data-with-dplyr",
    "href": "sessions_core/03_data_verbs.html#manipulating-data-with-dplyr",
    "title": "Data Manipulation, Basics",
    "section": "Manipulating Data with {dplyr}",
    "text": "Manipulating Data with {dplyr}\nNow that we know how to set up a project and import data, we can finally start to play around with it. Going forward we will be using several packages from the ‚Äútidyverse‚Äù to help us manipulate, summarize, and visualize our data. Today‚Äôs session will focus on data manipulation using a package called {dplyr}.\n\nWhat is {dplyr}\nData manipulation is the foundation of working with data in R and as such is foundational to the work we do as epidemiologists. In particular, data manipulation skills will be critical when trying to clean our data.\nIn R, the package {dplyr} provides a large number of functions to help us manipulate data frames and perform many of the tasks that we will need to use on a daily basis, for example:\n\nSubsetting our data to remove certain variables\nRenaming certain variables\nAdding or modifying a variable\nRemoving duplicate entries\n\nIn {dplyr} each of these actions can be done with a particular function, which typically have an intuitive verb for a name. For example, renaming columns will use the function rename().\nIn today‚Äôs session we will look at the ‚Äúdata manipulation verb‚Äù, ie the function, needed for each of the above tasks as well as how to chain them all together into an efficient data pipeline.\n\n\n\n\n\n\nNote\n\n\n\nYou may have noticed that we asked you to load a package called {tidyverse} rather than {dplyr} in the setup. Loading {tidyverse} will load several of the most useful packages from the broader tidyverse, including {dplyr} and a couple other packages that we will see later in the session."
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#basic-data-verbs",
    "href": "sessions_core/03_data_verbs.html#basic-data-verbs",
    "title": "Data Manipulation, Basics",
    "section": "Basic Data Verbs",
    "text": "Basic Data Verbs\n\nSelecting Specific Columns\nA lot of the time when we receive a dataset it will have extra columns that we don‚Äôt need, either because those columns contain sensitive data or because our analysis will only focus on a subset of the data. This is where a function like select() comes in handy.\nHere is the basic syntax, note that this is pseudo-code and isn‚Äôt something you are intended to run yourself.\n\n# DO NOT RUN (PSEUDO-CODE)\nselect(df_raw, first_column_to_keep, second_column_to_keep)\n\nHere we see that the first argument is our dataset and each subsequent argument is the name of a column that we would like to keep. In the tidyverse, variables (ie column names) don‚Äôt need to be put into quotation marks. So for example, if we want to select the columns id, sex, and age we can use the following:\n\nselect(df_raw, id, sex, age)\n\n\nUse select() to select the following variables in your dataset: id, sex, age, sub_prefecture, date_onset, and outcome. The head of your output should look something like this:\n\n\n  id   sex age date_onset   outcome\n1  1 femme  36 2022-08-13 recovered\n2  2     f   5 2022-08-18      &lt;NA&gt;\n3  3     f 156 2022-08-17 recovered\n4  6 homme   8 2022-08-22 recovered\n5  7     m   7 2022-08-30 recovered\n6 10     m   4 2022-08-30 recovered\n\n\n Take a look at this output and then at df_raw. We can see that df_raw still contains all of the columns, which is what we want. But can you tell why it didn‚Äôt change?\n\nOften, we want to keep most of the variables in our dataset and only remove one or two. We can use the above syntax to do this, but it can become pretty tedious to write out every column name. In these cases, instead of telling select what to **keep**, we can use a subtraction sign (-) to tell it what to **remove**. For example, if we wanted to remove thevillage_commune` column from our dataframe we can use the following:\n\nselect(df_raw, -village_commune)\n\nWay easier!\n\nUse the - syntax in select() to select all of the columns in df_raw except: full_name and age_unit from your dataset.\n\n\n\nRenaming Columns\nAnother common issue when we get new datasets is that the variable names are inconvenient. In those cases, rename() can work wonders. Here‚Äôs the basic syntax:\n\n# DO NOT RUN (PSEUDO CODE)\nrename(df,\n       new_column_name = old_column_name,\n       another_new_name = another_old_name)\n\nAs in the case of select(), and indeed in essentially all {dplyr} verbs, the first argument is our daframe. Then each subsequent argument is a statement of new_column_name = old_column_name telling R which columns to rename and the new names that we want to use, with each pair given its own line to improve readability. If we wanted to change village_commune to simply be village, for example, we can write:\n\nrename(df_raw,\n       village = village_commune)\n\n\nUse rename() on df_raw to change the columns sub_prefecture, village_commune, and health_facility_name to be prefecture, village, and facility respectively.\n\nIn the above exercise it may have been difficult to check if the output looked correct because R would have printed out the full data frame. In these cases it can be helpful to create a temporary object just to check if everything looks alright. You can call this object whatever you want, but a common name is tmp.\n\nRepeat the last exercise but this time assign the output to an object called tmp and use names() to check that the column names changed as you expected. The output of names() should give you something like this:\n\n\n [1] \"id\"                \"full_name\"         \"sex\"              \n [4] \"age\"               \"age_unit\"          \"region\"           \n [7] \"prefecture\"        \"village\"           \"date_onset\"       \n[10] \"date_consultation\" \"hospitalisation\"   \"date_admission\"   \n[13] \"facility\"          \"malaria_rdt\"       \"fever\"            \n[16] \"rash\"              \"cough\"             \"red_eye\"          \n[19] \"pneumonia\"         \"encephalitis\"      \"muac\"             \n[22] \"vacc_status\"       \"vacc_doses\"        \"outcome\"          \n[25] \"date_outcome\"     \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTemporary objects, like the tmp data frame you just created are just that: temporary. They are usually used to test if something has worked and designed to be overwritten each time you need to test something else. As such, you should not use these temporary objects as the input for other parts of your code. If you want to make a data frame that will be reused, such as a clean version of df_raw, this should be done using an object with a proper name like df or df_clean.\n\n\n\n\nChanging and Adding Columns\nSo now we know how to select and rename columns, but how do we modify them? This is where mutate() comes into play. This function can be used both to add new columns and to change existing ones.\nLet‚Äôs start with the basic mutate() syntax needed to add a new column:\n\n# DO NOT RUN (PSEUDO-CODE)\nmutate(df,\n       new_column = action(existing_column),\n       another_new_column = another_action(another_existing_column))\n\nIn the above code, we are creating a new column (new_column) by performing some sort of action (action()) on an existing column in the dataframe (existing_column). This action could be anything, it could use a function or be a simple arithmetic operation and can use one or more columns. For example, if we wanted to create a new column expressing MUAC in cm we could use the following:\n\nmutate(df_raw,\n       muac_cm = muac / 100)\n\n\nUse mutate() to create a new column called age_years that expresses age in years rather than months. The head of your new age_years column should look like this:\n\n\n   age_years\n1  3.0000000\n2  0.4166667\n3 13.0000000\n4  0.6666667\n5  0.5833333\n6  0.3333333\n\n\n\nGreat! But what if instead of creating a new column we instead wanted to change an existing one? You just need to use the existing column name on the left side of the = instead of giving a new column name. For example, in the above MUAC code we would write:\n\nmutate(df_raw,\n       muac = muac / 100)\n\nWe might want to keep age in months as well as years, so we won‚Äôt reassign that column. But there are some other columns that could stand to be changed. There are a lot of reasons we might want to change a column, two of the most common ones are:\n\nThe format of a string needs changing\nThe data type of a column is incorrect\n\nOur dataset has both of these problems. For example, while it isn‚Äôt per se a problem that region and sub_prefecture are in all capitals, it also isn‚Äôt particularly nice. To fix this, we can use another function from the {tidyverse}, this time from a package called {stringr} to make these columns title case:\n\nmutate(df_raw,\n       region = str_to_title(region),\n       sub_prefecture = str_to_title(sub_prefecture))\n\n\nUse mutate() to update the format of malaria_rdt and outcome to use title case. The head of these two columns should now look something like this:\n\n\n  malaria_rdt   outcome\n1    Negative Recovered\n2    Negative      &lt;NA&gt;\n3    Negative Recovered\n4    Negative Recovered\n5    Negative Recovered\n6    Negative Recovered\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that we didn‚Äôt need to load {stringr} to do the above exercise. That‚Äôs because, like {dplyr} this package is loaded when we load the {tidyverse}.\n\n\nThat‚Äôs nicer. Now let‚Äôs consider the second issue, having variables with the wrong type.\n\nTake a look at the data type of your columns, do any of them look strange?  Hint. str() may be useful here.\n\nMost of the columns look ok, but it seems theres something strange with the dates. Some of them are character type and others are something called POSIXct. We would much rather have all of these columns use the simple Date type.\nTo convert to dates, we are going to call on yet another package from the the tidyverse, {lubridate}. In particular, we are going to use the function ymd(). For example:\n\nmutate(df_raw,\n       date_outcome = ymd(date_outcome))\n\n\nUse mutate() and ymd() to modify date_onset and date_admission to be Date type. Use a temporary data frame tmp to check that your code is doing what you want it to.\n\n\n\nRemoving Duplicates\nOk great! We know how to select, rename, and modify our data. Another task we will often need to do is removing duplicate entries. Fortunately this one is easily done using the function distinct(), which has the following basic syntax:\n\n# DO NOT RUN (PSEUDO-CODE)\ndistinct(df)\n\nNotice that distinct only needs one argument by default, the dataset itself. This will look for and remove any duplicate observations in the data frame. There are some fancier ways of using distinct() that will look for duplication on certain variables only, but that‚Äôs outside of the scope of today‚Äôs session.\n\nUse distinct() to create a temporary data frame, tmp, that contains all the unique observations in df_raw. Compare the number of rows in tmp to that of df_raw. Did we have any duplicates?"
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#the-pipe-operator",
    "href": "sessions_core/03_data_verbs.html#the-pipe-operator",
    "title": "Data Manipulation, Basics",
    "section": "The Pipe Operator",
    "text": "The Pipe Operator\nSo it looks like we have actually done quite a bit of cleaning while learning the core {dplyr} verbs. We should probably try to put some of the above steps together to start building a basic data cleaning pipeline. So far we haven‚Äôt been saving any of our changes, except perhaps to a temporary data frame. It would be nice to keep them in a new clean df object.\nFor example, if we want to effect the column renaming we did above to a reusable object we might write something like this:\n\ndf &lt;- rename(df_raw, \n             prefecture = sub_prefecture,\n             village = village_commune,\n             facility = health_facility_name)\n\n\n\n\n\n\n\nTip\n\n\n\nIn general, it‚Äôs good practice to keep a raw version of your dataset, here df_raw, that remains unmodified in your code. This is so that you always have it available in your environment as a reference and is always available at the start of your cleaning pipeline to improve reproducibility.\n\n\nNow we have a new object, df that we can do more operations on. Brilliant. For example, if we now wanted to select everything except for full_name we could update the above code like this:\n\n# Step 1: Rename Variables\ndf &lt;- rename(df_raw, \n             prefecture = sub_prefecture,\n             village = village_commune,\n             facility = health_facility_name)\n\n# Step 2: Select Variables to Keep\ndf &lt;- select(df,\n             -full_name)\n\nNotice that in this second step we are using df as the input of select() rather than df_raw because we want to continue working on our modified version of the data. Let‚Äôs say now we want to add a column of age in years:\n\n# Step 1: Rename Variables\ndf &lt;- rename(df_raw, \n             prefecture = sub_prefecture,\n             village = village_commune,\n             facility = health_facility_name)\n\n# Step 2: Select Variables to Keep\ndf &lt;- select(df,\n             -full_name)\n\n# Step 3: Add Age in Years\ndf &lt;- mutate(df,\n             age_years = age / 12)\n\nHm, ok well this is working but it is starting to get repetitive. With each step we are reusing the output of the last step and then updating the same data frame, df. It would be better if these actions could be chained together in a simpler way.\nThis is exactly what the pipe operator, |&gt; is for! The pipe has the following basic syntax:\n\n# DO NOT RUN (PSEUDO-CODE)\ninput |&gt; action()\n\nHere the input on the left side (input) is ‚Äúpiped into‚Äù the action on the right side (action()). So for example instead of writing:\n\nselect(df_raw, id, sex)\n\nWe could instead write:\n\ndf_raw |&gt;\n  select(id, sex)\n\n\nTry out the above code to see if it works.\n\nThis can be used to chain multiple actions together and you will often see tidyverse style code that uses pipes in the following way:\n\n# DO NOT RUN (PSEUDO-CODE)\ndf &lt;- df_raw |&gt;\n  first_action() |&gt;\n  second_action() |&gt;\n  third_action()\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that here each action in the pipe is given its own line. This is considered good practice as it makes your code easier to read and understand.\n\n\nSo, if we wanted to chain the example actions we saw above into a single pipe, we might write something like this:\n\ndf &lt;- df_raw |&gt;\n  rename(prefecture = sub_prefecture,\n         village = village_commune,\n         facility = health_facility_name) |&gt;\n  select(-full_name) |&gt;\n  mutate(age_years = age / 12)\n\nThat‚Äôs a lot easier than reassigning df after each step!\n\nLet‚Äôs see if we can put together what we learned above into a single pipeline! Use the pipe operator |&gt;, select(), rename(), mutate(), str_to_title(), ymd(), and distinct() to perform the following actions on df_raw and assign the output to a new data frame called df:\n\nRemove the variables full_name and age_unit\nRename the following variables:\n\nage becomes age_months\nsub_prefecture becomes prefecture\nvillage_commune becomes village\nhealth_facility_name becomes facility\n\nAdd a variable age_years with patient age in years\nUpdate region and prefecture to use title case\nUpdate all date columns to use Date type\nRemove any duplicate observations\n\nThe head of your final data should look something like this:\n\n\n  id   sex age_months  region prefecture        village date_onset\n1  1 femme         36 Mandoul   Moissala Sangana Ko√Øtan 2022-08-13\n2  2     f          5 Mandoul   Moissala      Mousdan 1 2022-08-18\n3  3     f        156 Mandoul   Moissala     Djaroua Ii 2022-08-17\n4  6 homme          8 Mandoul   Moissala     Monakoumba 2022-08-22\n5  7     m          7 Mandoul   Moissala      T√©tindaya 2022-08-30\n6 10     m          4 Mandoul   Moissala      Danamadja 2022-08-30\n  date_consultation hospitalisation date_admission\n1        2022-08-14             yes     2022-08-14\n2        2022-08-25             yes     2022-08-25\n3        2022-08-20            &lt;NA&gt;           &lt;NA&gt;\n4        2022-08-25              no           &lt;NA&gt;\n5        2022-09-02              no           &lt;NA&gt;\n6        2022-09-02             yes     2022-09-02\n                         facility malaria_rdt fever rash cough red_eye\n1 H√¥pital du District de Moissala    negative    No &lt;NA&gt;   Yes      No\n2 H√¥pital du District de Moissala    negative    No   No   Yes      No\n3                      CS Silambi    negative   Yes &lt;NA&gt;    No      No\n4 H√¥pital du District de Moissala    negative    No   No    No    &lt;NA&gt;\n5                      CS Silambi    negative  &lt;NA&gt;   No   Yes     Yes\n6                    Moissala Est    negative   Yes   No    No    &lt;NA&gt;\n  pneumonia encephalitis muac vacc_status vacc_doses   outcome date_outcome\n1        No           No  244        &lt;NA&gt;       &lt;NA&gt; recovered   2022-08-18\n2      &lt;NA&gt;           No  232          No       &lt;NA&gt;      &lt;NA&gt;   2022-08-28\n3        No         &lt;NA&gt;  123  Yes - oral       &lt;NA&gt; recovered         &lt;NA&gt;\n4        No           No  210          No       &lt;NA&gt; recovered         &lt;NA&gt;\n5        No           No   80          No       &lt;NA&gt; recovered         &lt;NA&gt;\n6        No           No  220          No       &lt;NA&gt; recovered   2022-09-03\n   age_years\n1  3.0000000\n2  0.4166667\n3 13.0000000\n4  0.6666667\n5  0.5833333\n6  0.3333333\n\n\nHint. Be careful with your column names here! If you renamed something you will need to use the new names for any subsequent parts of the pipe.\n\nAmazing! That looks like a great start at a data cleaning pipeline. Keep this code handy, you will use it in the next session where we will look at another common part of data cleaning: recoding."
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#done",
    "href": "sessions_core/03_data_verbs.html#done",
    "title": "Data Manipulation, Basics",
    "section": "Done!",
    "text": "Done!\nWell done, you‚Äôve learned the fundamentals of data manipulation and how to string multiple commands together into a data manipulation pipe. Moving forward, solution files will focus less on being ‚Äúexercise by exercise‚Äù and rather provide an example of what a real script might look like in a real world context. In this case, the solutions will then focus only on the final pipe that is created at the end of the session.\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_core/03_data_verbs.html#going-further",
    "href": "sessions_core/03_data_verbs.html#going-further",
    "title": "Data Manipulation, Basics",
    "section": "Going Further",
    "text": "Going Further\n\nExtra Exercises\n\nA a line to your mutate() to update the hospitalisation variable so that its text would be in title case as well.\nPerhaps you would prefer to use lower case for the region column rather than the title case, update your code to do this instead. Hint: you might want to check out str_to_lower() from {stringr}.\nCreate a delay_consultation column, that contains the number of days between the onset of symptoms and the consultation."
  },
  {
    "objectID": "sessions_core/05_summary_table.html",
    "href": "sessions_core/05_summary_table.html",
    "title": "Summary Tables",
    "section": "",
    "text": "Create contingency tables with count()\nCompute summary statistics by group using summarize()\nReview how to subset rows using filter() and create/modify variables with mutate()\nCreate ordered categorical variables"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#objectives",
    "href": "sessions_core/05_summary_table.html#objectives",
    "title": "Summary Tables",
    "section": "",
    "text": "Create contingency tables with count()\nCompute summary statistics by group using summarize()\nReview how to subset rows using filter() and create/modify variables with mutate()\nCreate ordered categorical variables"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#setup",
    "href": "sessions_core/05_summary_table.html#setup",
    "title": "Summary Tables",
    "section": "Setup",
    "text": "Setup\nDependencies. This session assumes that you know how to use RStudio that you are able to import data and that you know th basic data handling verbs that we have seen in the core sessions so far. If you need a refresher on either of these topics, we encourage you to review the core sessions in the learning pathway.\n\nThis session will use the clean version of the Moissala measles dataset.\n\n\n\n  Course Folder\n\n\n\n Open your RStudio Project and create a new script in the R folder called tables.R with appropriate metadata and a ‚ÄúPackages‚Äù section that imports: {rio}, {here} and {tidyverse}. Add an ‚ÄúImport Data‚Äù section that loads the cleaned version of the measles linelist."
  },
  {
    "objectID": "sessions_core/05_summary_table.html#introduction-data-aggregation",
    "href": "sessions_core/05_summary_table.html#introduction-data-aggregation",
    "title": "Summary Tables",
    "section": "Introduction: Data aggregation",
    "text": "Introduction: Data aggregation\nOK so let‚Äôs recap, you have just performed one of the most important tasks of an epidemiologist: the data cleaning. Now that you have clean and standardized data, you can get into the real business and start analysing them. Analyses typically start with some tables and summaries that describe our data:\n\nUnivariate frequency tables to count occurrences of different values\nSummary statistics of numerical variables (mean, median, standard deviation)\nCross-tabulations to examine relationships between categorical variables\nGroup-wise summaries to compare statistics across different subsets of the data"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#counting-multiple-columns-contingency-tables",
    "href": "sessions_core/05_summary_table.html#counting-multiple-columns-contingency-tables",
    "title": "Summary Tables",
    "section": "Counting Multiple Columns (Contingency Tables)",
    "text": "Counting Multiple Columns (Contingency Tables)\nDuring the data exploration session, you have learned to create a frequency table for a single categorical variable using the count() function. This is nice, but we often want to count the number observations based on two (or more!) variables.\nThese tables are called contingency tables. For example, knowing the number of patients by sub_prefecture is great but we might want to stratify by both sub_prefecture and age_group to see if certain areas have unusually old patients. Doing this is easy, you just need to pass multiple column names to count():\n\ndf_linelist |&gt;\n  count(sub_prefecture, age_group)\n\n\nCreate a new summary table counting the number of patients stratified by sub_prefecture and hospitalisation. What happens if you change the order of the arguments given to count()?  Now, using count(), answer the following questions:\n\nHow many patients were female? What is the proportion?\nWhat are all the possible values of the outcome variable?\nHow many patients between 1 - 4 years have recovered?"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#filtering-out-nas",
    "href": "sessions_core/05_summary_table.html#filtering-out-nas",
    "title": "Summary Tables",
    "section": "Filtering out NAs",
    "text": "Filtering out NAs\nWhen looking at the categories of outcome, you should have spotted that some patients have missing values (NA):\n\ndf_linelist |&gt;\n  count(outcome) |&gt;\n  mutate(prop = n / sum(n))\n\n\nObserve the output of the code above. How can you also call the proportion of patients who died? Are you happy with this calculation?\n\nThe proportion of cases that died is also referred to as the Case Fatality Ratio (CFR). To precisely calculate the CFR we need to make sure that the denominator only includes patient for whom we are sure of their outcome (ie we need to remove all cases with NA or left aginst medical advice).\nRemember that we can do this using filter(). To filter for missing values (NA) in a variable we can use the small function is.na(outcome). Adding a ! in front will do the opposite: removing missing values from outcome:\n\ndf_linelist |&gt;\n  filter(\n    outcome != \"left against medical advice\", \n    !is.na(outcome)\n  ) |&gt;\n  count(outcome)\n\n\nWhich other conditionnal statement could you use in filter() to obtain the same results\n\nNow that we have removed the patients with unknown outcomes, we can add this before creating our frequency table to get the right CFR.\n\nUsing your filter, update your code to summarize the observed number of patients who survived and died as well as the CFR (proportion who died). Store this new dataframe into an object, cfr_df.\n\n\n\n\n\n\n\nTip\n\n\n\nBonus. A useful ‚Äúshortcut‚Äù function is drop_na() from the package {tidyr} that equates to filter(!is.na()).\n\ndf_linelist |&gt;\n  drop_na(outcome) |&gt;\n  count(outcome)\n\ndrop_na() is particularly useful as you can give it multiple column names to filter by. But be careful that doing so will remove all rows where one or more of those columns have a missing value."
  },
  {
    "objectID": "sessions_core/05_summary_table.html#sec-stratify",
    "href": "sessions_core/05_summary_table.html#sec-stratify",
    "title": "Summary Tables",
    "section": "Summary Table: Statistics by Sub Prefecture",
    "text": "Summary Table: Statistics by Sub Prefecture\nOk now that we have produced some simple frequency and contingency tables we may want to increase the complexity. A common task in epidemiology is to look at summary statistics within subsets of the data.\nFor example, we may be asked to produce patient statistics at the sub-prefecture level, ie: for each sub-prefecture in the data, we need to answer the following questions:\n\nHow many patients consulted?\nWhat is their average age?\nWhat was the earliest date of admission?\nHow many patients have been hospitalized?\nAmong children under 6 months, how many have died?\n\nThis is exactly what the function summarize() has been made for! It allows us to calculate summary statistics on a dataset, and the syntax is similar to that of mutate():\n\n# DO NOT RUN (PSEUDO-CODE)\ndf |&gt;\n  mutate(new_col = function_to_create(existing_col))\n\ndf |&gt;\n  summarize(\n    .by = grouping_variable,\n    new_col = summary_function(existing_col)\n  )\n\nConsider the following code, here we are summarizing the data to calculate the average age across all patients.\n\ndf_linelist |&gt;\n  summarize(mean_age = mean(age))\n\n  mean_age\n1 6.822047\n\n\nNotice that this code yields a single value for average age. No grouping variable was provided so summarize() returned one summary statistic for the whole dataset. To calculate the average age by a specific strata, we need to specify a grouping variable using the .by argument:\n\ndf_linelist |&gt;\n  summarize(\n    .by = sex,  # Make the summary (here, the mean) by sex\n    mean_age = mean(age)\n  )\n\n  sex mean_age\n1   f 6.773824\n2   m 6.869855\n\n\n\nTake a look at the above results. How would you interpret them?\n\nNow that we can use summarize() we can use it to calculate some proper summary statistics by sub-prefecture. Let‚Äôs start by calling an empty summarize() and grouping the data on sub_prefecture.\n\nRun the following code:\n\ndf_linelist |&gt;\n  summarize(\n    .by = sub_prefecture\n  )\n\nWhat happens when you run these lines?\n\n\nCounts\nWe first want to look at the number of cases in each sub_prefecture. This can be done using the helper function n():\n\ndf_linelist |&gt;\n  summarize(\n    .by = sub_prefecture,\n    n_patients = n() # Count stuffs\n  )\n\n\nOk now let‚Äôs build a summary table for each sub_prefecture. First start by replicating the above lines\n\n\n\nSummarizing Continuous Variables\nWe can then use the mean(), median(), min(), max() functions (and other) to produce summaries for continuous variables. For example the average age:\n\ndf_linelist |&gt;\n  summarize(\n    .by = sub_prefecture,\n    n_patients = n(),\n    mean_age = mean(age)\n  )\n\n\nAdd the minimum date of admission to your table for each of the sub_prefecture? Are you happy with the results?\n\n\n\n\n\n\n\nTip\n\n\n\nRemember that with the arithmetic functions such as mean(), median(), min(), max(), you need to explicitly tell R to remove NA.\n\n\n\n\nCounting with a Condition\nWe may also be interested in looking at the number of patients (rows) that fit a condition: the number of patients that were female. Counting by a logical condition can be done with the following syntax:\n\n# DO NOT RUN (PSEUDO-CODE)\nsummarize(\n  sum_category = sum(LOGIC_TEST, na.rm = TRUE)\n  )\n\nThis sum allows us to count all the lines where our condition was met (returns TRUE). For example:\n\ndf_linelist |&gt;\n  summarize(\n    .by = sub_prefecture,\n    n_female = sum(sex == \"f\", na.rm = TRUE)\n  )\n\n\nAdd a variable to your table that counts the number of patients that have been hospitalized. (ie: rows that have yes in variable hospitalisation)\n\n\n\nOther Statistics\nSometimes we want to produce a more complicated statistic, for example the mean age of all hospitalized patients. Here the syntax is a bit different:\n\n# DO NOT RUN (PSEUDO-CODE)\ndf |&gt;\n  summarize(\n    mean_category = mean(col_to_use[LOGIC_TEST], na.rm = TRUE)\n    )\n\nHere, we have:\n\nStated what summary statistic we want to use (mean())\nIndicated which column we want to calculate that statistic on (col_to_use)\nCreated a condition of which observations in that column to use in the calculation ([LOGIC_TEST])\n\nTo give a concrete example, if we wanted to compute the mean of the age variable but only for hospitalized patients (ie: in rows where hospitalisation == \"yes\") we would write:\n\ndf_linelist |&gt;\n  summarize(\n    .by = sub_prefecture,\n    n_patients = n(),\n    mean_age_hosp = mean(age[hospitalisation == \"yes\"], na.rm = TRUE)\n  )\n\nThe use of a logical test in the example above is called logical indexing, where a condition is essentially being used to filter which observations you want to consider when performing a calculation. Logical indexing is very powerful but can also take some getting used to, so don‚Äôt worry too much if it isn‚Äôt perfectly clear at this stage.\n\nCan you use this syntax to calculate the mean age of female patients in your table?\n\nThat is looking great! We are starting to get a pretty exhaustive grouped summary table with a lot of useful information by sub_prefecture! An extra challenge for you:\n\nCHALLENGE: Could you add a variable to your table that counts the number of patients that died among the ones that are &lt; 6 months old.\n Hint. You want to count rows (so use sum()) that fill a specific condition for outcome (outcome == \"dead\"), but only when age_group == \"&lt; 6 months\"\n\n\n\nUse the Output\nFinally, remember that summarize() returns a dataframe that we can then further manipulate (eg: with filter() and mutate()).\n\nAdd a mutate() after producing your summary table to calculate:\n\nThe proportion of hospitalized patients per sub-prefecture\nThe proportion of female patients per sub-prefecture\n\n\nThe head of your final table should look like this:\n\n\n  sub_prefecture n_patients mean_age min_admission n_female n_hosp\n1       Moissala       1808 6.842920    2022-08-14      923    612\n2          Bouna       1376 6.555959    2023-01-11      669    412\n3       Bedjondo        534 7.073034    2023-06-09      251    184\n4       Bekourou        496 6.836694    2023-06-17      251    164\n5         Bedaya        435 7.098851    2023-07-04      209    147\n6         Koumra        253 7.106719    2023-08-14      138     84\n  mean_age_hosp mean_age_female n_death_u6m prop_female prop_hosp\n1      5.485294        6.748646          71   0.5105088 0.3384956\n2      5.665049        6.633782          61   0.4861919 0.2994186\n3      5.211957        6.948207          22   0.4700375 0.3445693\n4      6.042683        6.840637          25   0.5060484 0.3306452\n5      6.156463        7.105263          17   0.4804598 0.3379310\n6      6.261905        6.456522           7   0.5454545 0.3320158"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#done",
    "href": "sessions_core/05_summary_table.html#done",
    "title": "Summary Tables",
    "section": "Done!",
    "text": "Done!\nYou should be proud of yourselves, making summary tables is an important skill to an epidemiologist, making it in R is very efficient! Don‚Äôt forget to save your code!\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_core/05_summary_table.html#going-further",
    "href": "sessions_core/05_summary_table.html#going-further",
    "title": "Summary Tables",
    "section": "Going Further",
    "text": "Going Further\n\nExtra Exercises\n\nMake a summary table that summarizes:\n\nThe number of patients\nThe proportion of male\nThe number of deaths\nThe CFR\nThe number of deaths among patients that had pneumonia\nin all the different age groups !\n\nMake a table that shows the proportion of patients by age with any measles vaccine (by oral recall or card) and those with 1 or 2 doses.\nMake a table that compares the proportion of hospitalised and non-hospitalised patients with positive malaria RDT, fever, rash, cough, red eye, pneumonia, encephalitis, and ‚Äúred‚Äù or ‚Äúyellow‚Äù MUAC (less than 125 mm).\nCalculate the mean days from first symptom onset to consultation by sub-prefecture.\nCalculate the mean time spent in hospital (i.e.¬†days from admission to outcome) by outcome (i.e.¬†in those who recovered and those who died).\n\n\n\nAdditional Resources\n\nThe EpiR Handbook chapter on grouping data\nOnce you have tables, you can extensively customize them for display/publication using {gt} package:\n\nWebsite of gt\nBook about gt"
  },
  {
    "objectID": "about.html#hey-there",
    "href": "about.html#hey-there",
    "title": "About",
    "section": "Hey There",
    "text": "Hey There\nWelcome to {repicentre}, an open source site developed by Epicentre to support folks learning R for humanitarian contexts. The site is composed of self paced tutorials and has two main options for learning:\n\nLinear. Designed for people with zero prior experience in R, the linear course will walk you through core R concepts using a case study about measles in Chad. The course covers the following concepts:\n\nData Structures and the RStudio Interface\nData Importation\nData Manipulation\nData Cleaning\nData Aggregation\nData Visualization\n\nChoose Your Own Adventure. If you have a bit more experience or if you are looking for a particular subject, feel free to explore the full range of tutorials. Tutorials are tagged with categories and designed to be self contained."
  },
  {
    "objectID": "about.html#recommendations-and-requests",
    "href": "about.html#recommendations-and-requests",
    "title": "About",
    "section": "Recommendations and Requests",
    "text": "Recommendations and Requests\nIs there a topic that you would like to see a tutorial on that isn‚Äôt currently available? Great! Feel free to let us know by opening an issue on the GitHub repository associated with this website. If you aren‚Äôt familiar with how to open an issue, please get in touch with Cat Eisenhauer instead."
  },
  {
    "objectID": "about.html#contributing",
    "href": "about.html#contributing",
    "title": "About",
    "section": "Contributing",
    "text": "Contributing\nWould you like to help write or maintain some tutorials? Increadible! Please get in touch with Cat."
  },
  {
    "objectID": "template.html",
    "href": "template.html",
    "title": "Session Title",
    "section": "",
    "text": "Objective 1\nObjective 2"
  },
  {
    "objectID": "template.html#objectives",
    "href": "template.html#objectives",
    "title": "Session Title",
    "section": "",
    "text": "Objective 1\nObjective 2"
  },
  {
    "objectID": "template.html#setup",
    "href": "template.html#setup",
    "title": "Session Title",
    "section": "Setup",
    "text": "Setup\n\nDescription of something participants need to setup, primarily used at the beginning of a section but can also be used for tasks like setting up an Rproject file, folder structure, etc."
  },
  {
    "objectID": "template.html#main-section-1",
    "href": "template.html#main-section-1",
    "title": "Session Title",
    "section": "Main Section 1",
    "text": "Main Section 1\nYour main section(s) can (and probably should) be boken down into subsections.\n\nSubsection 1\n\n\nSubsection 2"
  },
  {
    "objectID": "template.html#done",
    "href": "template.html#done",
    "title": "Session Title",
    "section": "Done!",
    "text": "Done!\nThis last header let‚Äôs students know that they are done with the main material for the day. It should also include a link to the solutions (hosted on github). For example:\n\n\n\n Solution File\n\n\n\nMake sure this link references the main."
  },
  {
    "objectID": "template.html#going-further",
    "href": "template.html#going-further",
    "title": "Session Title",
    "section": "Going Further",
    "text": "Going Further\nAfter your main content is done you should have a section called called ‚ÄúGoing Further‚Äù for students who finish the main content early. It should include: 1. A mention of one or two satellite sessions that would be relevant extensions of the current material 2. A section with ‚Äúextra exercise questions‚Äù (these don‚Äôt need to use the ‚Äúaction blocks‚Äù (see below) and can just be a number list as shown below.\n\nExtra Exercises\n\nDo this.\nThen do that."
  },
  {
    "objectID": "template.html#markdown-reminders",
    "href": "template.html#markdown-reminders",
    "title": "Session Title",
    "section": "Markdown Reminders",
    "text": "Markdown Reminders\nThe rest of this document is a reminder on qmd syntax and a basic style guide. Enjoy.\n\nText Formatting\n\nItalic and Bold will turn out like this\nBlock quotes will look like this:\n\n\nThis is a blockquote made using &gt;\n\n\nTooltips can be done using spans (please do not use asides or footnotes)\n\n\n\nCode\nInline coding will turn out like this\nCode blocks will appear like this:\n\n# comment\nprint('hello world')\n\nWarning: For these tutorials, code blocks are not evaluated by default. If you want to evaluate them, you must indicate it specifically.\n\n# comment\nprint('hello back!')\n\n[1] \"hello back!\"\n\ntest &lt;- function(x) {\n  if (x &gt; 1) {\n    return(x)\n  } else {\n    print('nothing to see here')\n  }\n}\n\nNote. We are no longer using solution blocks, instead a single code file will be available at the end of each session contiaining code that runs through all the exercises.\n\n\nCallouts\nIMPORTANT: please do not use callouts not explicitly defined here; they have not been included in the css and therefore will not render well in the final document.\n\n\n\n\n\n\nNote\n\n\n\nThis is a callout using {.callout-note}\n\n\n\n\n\n\n\n\nTip\n\n\n\nComment about a genral tip / trick or best practice.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWarning / comment on something really important.\n\n\n\n\nAction Boxes\nThese are used for things participants are expected to actually do, ie: exercises. They are split into three categories.\n\nDescription of something participants need to setup, primarily used at the beginning of a section but can also be used for tasks like setting up an Rproject file, folder structure, etc.\n\n\nDescription of something participants should observe, investigate, etc.\n\n\nDescription of a coding exercise that participants are expected to complete.\n\n\n\nTabsets\n\nOneTwoThree\n\n\nContent that will show under the first tab\n\n\nContent that will show under the second tab\n\n\nContent that will show under the third tab\n\n\n\n\n\nImages\nYou can insert images by referring to their relative path using markdown syntax or HTML. Note that the markdown syntax does not allow you to modify image size. In either case, make sure to add alt text for accessibility.\nMarkdown style syntax:\n\n\n\nexample image alt text\n\n\nHTML style syntax (with specification of desired size):\n\n\n\nLinking to Other Pages\nEasy, use relative paths within a standard href, ie: link to home page."
  },
  {
    "objectID": "sessions_extra/data_exploration.html",
    "href": "sessions_extra/data_exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Perform quick exploration of an imported dataset\nProduce frequency tables for variables"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#objectives",
    "href": "sessions_extra/data_exploration.html#objectives",
    "title": "Data Exploration",
    "section": "",
    "text": "Perform quick exploration of an imported dataset\nProduce frequency tables for variables"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#setup",
    "href": "sessions_extra/data_exploration.html#setup",
    "title": "Data Exploration",
    "section": "Setup",
    "text": "Setup\nDependencies. This extra session assumes that you have completed the sessions introduction to R and R studio, and data importation.\n\nFor this session we will work with our raw Moissala measles linelist which can be downloaded here:\n\n\n\n  Course Folder\n\n\n\n Make sure it is appropriately stored in data/raw of your project. Then open a new script called data-exploration.R, and make sure packages {here}, {rio} and {dplyr} are loaded. Finally, import the data into R as an object called df_linelist."
  },
  {
    "objectID": "sessions_extra/data_exploration.html#data-exploration",
    "href": "sessions_extra/data_exploration.html#data-exploration",
    "title": "Data Exploration",
    "section": "Data Exploration",
    "text": "Data Exploration\nRight after importing some data into R, we might want to take a look at it. When talking of data exploration we usually want to do a few things:\n\nExamine dimensions of the data (ie: how many rows and how many columns)\nLook at columns names\nVisualise the first or last few rows\nDetermine the type of the variables\nDetermine the range of values in continuous variables\nObserve the possible values in each categorical variable\n\nThis process is crucial and will allow us to familiarize ourselves with our data and identify issues that will be adressed during the data cleaning step."
  },
  {
    "objectID": "sessions_extra/data_exploration.html#basic-exploration",
    "href": "sessions_extra/data_exploration.html#basic-exploration",
    "title": "Data Exploration",
    "section": "Basic Exploration",
    "text": "Basic Exploration\nThe very first thing you want to know about your data is the dimensions, which refers to the number of rows and number of columns that make up your data. There are several ways to get this information in R:\n\nLook at your environment pane in RStudio and check for your data - the number next to it (5230x25) tells us it is a dataframe with 5230 rows and 25 columns.\nUse dim() on your data to return a vector with both the number of rows and number of columns\nAlternatively, use ncol() to get the number of columns and nrow() for the number of rows\n\nIt‚Äôs good to remember these numbers so you can quickly spot if there are unexpected changes to your data during your analysis (ie: more/fewer rows or columns than expected).\n\nUsing the method of your choice, get the dimensions of your dataframe df_linelist."
  },
  {
    "objectID": "sessions_extra/data_exploration.html#variable-names",
    "href": "sessions_extra/data_exploration.html#variable-names",
    "title": "Data Exploration",
    "section": "Variable Names",
    "text": "Variable Names\nBecause we are going to use the variable names very often during our analysis, we want to get familiar with them pretty early on. Also, we need to identify the ones that will need to be renamed during our data cleaning. The function names() returns a vector of all the variable names in our dataframe:\n\nnames(df_linelist)\n\n [1] \"id\"                   \"full_name\"            \"sex\"                 \n [4] \"age\"                  \"age_unit\"             \"region\"              \n [7] \"sub_prefecture\"       \"village_commune\"      \"date_onset\"          \n[10] \"date_consultation\"    \"hospitalisation\"      \"date_admission\"      \n[13] \"health_facility_name\" \"malaria_rdt\"          \"fever\"               \n[16] \"rash\"                 \"cough\"                \"red_eye\"             \n[19] \"pneumonia\"            \"encephalitis\"         \"muac\"                \n[22] \"vacc_status\"          \"vacc_doses\"           \"outcome\"             \n[25] \"date_outcome\"        \n\n\n\nWhat do you think of the names in your dataset? Can you already spot some variables names you would like to rename?"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#inspecting-your-data",
    "href": "sessions_extra/data_exploration.html#inspecting-your-data",
    "title": "Data Exploration",
    "section": "Inspecting Your Data",
    "text": "Inspecting Your Data\nIt is also nice to inspect your data, it may be easier for you to spot some inconsistencies, variables with a lot of missing values, and it will allow you to see what values to expect in each of them. You can print your data in the console by:\n\nRunning the df_linelist object alone (careful though, you may not want to do this if you have a large dataset)\nUse the head() function to see the top 6 rows (you can increase this number using the argument n)\nUse the tail() function to see the last 6 rows (again, you can increase this number using the argument n)\n\nThese methods will only print the first 40 rows of your data at most because that‚Äôs the limit of your console. Alternatively, you can use View() to see your data in a tabular form. This will open a new window with your data displayed like like an Excel spreadsheet. Note, this command only displays the data, it doesn‚Äôt allow you to modify it.\n\n\n\n\n\n\nTip\n\n\n\nBe very careful with View() on large dataset as this may crash your RStudio session. To avoid this, you can print the output in the console.\n\n\n\nCan you display the first 15 rows of your data? What happen when you change the width of your console pane and run the command again?"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#variable-type",
    "href": "sessions_extra/data_exploration.html#variable-type",
    "title": "Data Exploration",
    "section": "Variable Type",
    "text": "Variable Type\nWe now want to check the type of the different variables. This is important as part of data cleaning involves making sure that numerical variables are type numeric, dates Date, and categorical variables are factor or character. You have already seen the class() function, to check the type of a vector. In R, each variable of a dataframe is a vector. We can extract all the values of that vector using the $ sign, and pass it to the class() function:\n\nclass(df_linelist$age)\n\n\nTry extracting all the values from the sex variable. What is the type of this variable?\n\nYou can also use str() on your dataframe to check the class of all the variables at once:\n\nstr(df_linelist)\n\n\nUse str() to check the data type of each column. Does anything look odd? Remember that you can also use functions like is.character() and is.numeric() if you‚Äôd like to test the type of a particular column."
  },
  {
    "objectID": "sessions_extra/data_exploration.html#exploring-continuous-variables",
    "href": "sessions_extra/data_exploration.html#exploring-continuous-variables",
    "title": "Data Exploration",
    "section": "Exploring Continuous Variables",
    "text": "Exploring Continuous Variables\nNow that you know how to extract the values from a variable, you may want to explore some of these values from the numeric variables to check for inconsistencies. Let‚Äôs look for some summary statistics for these, and Base R provides many handy functions:\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\nReturns\n\n\n\n\nmin()\nMinimum value\nmin(x)\nSingle minimum value\n\n\nmax()\nMaximum value\nmax(x)\nSingle maximum value\n\n\nmean()\nArithmetic average\nmean(x)\nAverage value\n\n\nmedian()\nMiddle value\nmedian(x)\nMiddle value\n\n\nrange()\nMin and max\nrange(x)\nVector of (min, max)\n\n\nIQR()\nInterquartile range\nIQR(x)\nQ3 - Q1\n\n\nquantile()\nSpecified quantiles\nquantile(x, probs = c(0.25, 0.75))\nRequested quantiles\n\n\nsd()\nStandard deviation\nsd(x)\nStandard deviation\n\n\nvar()\nVariance\nvar(x)\nVariance\n\n\nsum()\nSum of values\nsum(x)\nSum\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThese functions require you to explicitly remove missing values (NA) using the argument na.rm = TRUE\n\n\nYou can extract the values of a variables using $, and pass them to any of those functions.\n\nUse the $ syntax to get:\n\nThe minimum value of age\nThe maximum of muac\n\nAny problems?"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#exploring-categorical-variables",
    "href": "sessions_extra/data_exploration.html#exploring-categorical-variables",
    "title": "Data Exploration",
    "section": "Exploring Categorical Variables",
    "text": "Exploring Categorical Variables\nFinally, let‚Äôs look at the values in our categorical variables. For this we can use frequency tables. This is handy as:\n\nIt allows us to quickly see the unique values in a categorical variable\nThe number of observations for each of those categories\n\nThis is done using the function count() from the package {dplyr}, which accepts the a dataframe and the name of one (or more!) column(s) as arguments. It will then count the number of observations of each unique element in that column. For example, let‚Äôs see the possible values of the variable sex:\n\ncount(df_linelist, sex)\n\nThe output is a new, smaller dataframe containing the number of patients observed stratified by sex. It seems like this variable requires some recoding‚Ä¶ We will do that in a later session.\n\nUsing your linelist data, look into the values for the outcome variable. How does it look?\nNow, try adding the argument sort = TRUE to the count() function. What did this argument do?"
  },
  {
    "objectID": "sessions_extra/data_exploration.html#done",
    "href": "sessions_extra/data_exploration.html#done",
    "title": "Data Exploration",
    "section": "Done!",
    "text": "Done!\nWell done taking a first look at your data!\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html",
    "href": "sessions_extra/weekly_epicurves.html",
    "title": "Weekly Epicurves",
    "section": "",
    "text": "In the epicurve session you learned how to plot an epicurve of the number of cases per day:\n\n\n\n\n\n\n\n\n\nThis graph is aggregated by day, which is a reasonable level of aggregation if the outbreak is short or if you wish to zoom on a short period. As epidemiologists we however often want to plot data by week.\nIn this tutorial we will learn two ways of aggregating data by week, plot the data and tweak the date labels."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#objectives",
    "href": "sessions_extra/weekly_epicurves.html#objectives",
    "title": "Weekly Epicurves",
    "section": "",
    "text": "In the epicurve session you learned how to plot an epicurve of the number of cases per day:\n\n\n\n\n\n\n\n\n\nThis graph is aggregated by day, which is a reasonable level of aggregation if the outbreak is short or if you wish to zoom on a short period. As epidemiologists we however often want to plot data by week.\nIn this tutorial we will learn two ways of aggregating data by week, plot the data and tweak the date labels."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#setup",
    "href": "sessions_extra/weekly_epicurves.html#setup",
    "title": "Weekly Epicurves",
    "section": "Setup",
    "text": "Setup\nWe will build on code from the epicurve session so you may either write your code in the script associated with that session or create a new script.\n\nCreate a new script for this tutorial or open the script from the epicurve lesson.\n Make sure the following packages are installed and loaded:\n\n{here} to write robust absolute paths,\n{rio} to import the data,\n{dplyr} to manipulate data,\n{ggplot2} to create the graphs,\n{lubridate} to manage dates and times\n{scales} to create prettier labels\n\nIf it is not already done, import the clean data (moissala_linelist_clean_EN.rds) into a df_linelist data frame and create a new section in your script called PREPARE DATA.\n\nAs we did in the core session, the examples in this lesson will be shown for outcomes and you will code the classic epicurve for date of onset in the exercises."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#aggregate-data-by-week",
    "href": "sessions_extra/weekly_epicurves.html#aggregate-data-by-week",
    "title": "Weekly Epicurves",
    "section": "Aggregate Data by Week",
    "text": "Aggregate Data by Week\nWe will to discuss two ways of aggregating data by weeks. You may be more familiar with the first one (using week numbers to identify weeks), but we will to focus more heavily on a more robust way (using the firs day of the week to identify weeks).\n\nUsing Week Numbers\nProbably the most intuitive way of thinking of weekly aggregated data is to think in terms of week numbers, as aggregated data from MoH are often in this format, and you probably created a lot of epicurves with week numbers yourselves.\nTheisoweek() from the {lubridate} packages takes a date (or a vector of dates) and returns the associated ISO week.\n\nexample_date &lt;- as.Date('2025-02-24')\n\nexample_date\n\n[1] \"2025-02-24\"\n\nisoweek(example_date)\n\n[1] 9\n\n\nWe can use this function to create a week_outcome_number in our data frame:\n\ndf_linelist &lt;- df_linelist |&gt; \n  mutate(week_outcome_number = isoweek(date_outcome))\n\nThe head of the date_outcome and week_outcome_number columns looks like this:\n\n\n  date_outcome week_outcome_number\n1   2022-08-18                  33\n2   2022-08-28                  34\n3   2022-09-03                  35\n4   2022-09-12                  37\n5   2022-09-10                  36\n6   2022-09-18                  37\n\n\n\nYour turn. Use the mutate() and isoweek() functions to create a new column in your data frame called week_onset_number that contains the ISO week associated with every onset date. The head of date_onset and week_onset_number columns should look like this:\n\n\n  date_onset week_onset_number\n1 2022-08-13                32\n2 2022-08-18                33\n3 2022-08-17                33\n4 2022-08-22                34\n5 2022-08-30                35\n6 2022-08-30                35\n\n\n\nNow, you could use this column to aggregate data by week using count() and then plot the weekly aggregated data using {ggplot2} with a code very similar to what we saw in the core epicurve session.\nThere is a problem, though. With isoweek() there is a first week in 2022, but also in 2023, 2024 and so on. With a short outbreak that would be only in 2022, this would be fine. However, our data frame gathers data at the whole region scale, and the dates range from 2022 to 2023. So if we were to just count the number of patients by week number, this table would be wrong:\n\n# WRONG\ndf_linelist |&gt; \n  count(week_onset_number) |&gt; \n  head(10)\n\n   week_onset_number  n\n1                  1 36\n2                  2 35\n3                  3 42\n4                  4 56\n5                  5 70\n6                  6 78\n7                  7 85\n8                  8 49\n9                  9 62\n10                10 81\n\n\nInstead, we could count by week stratified by years:\n\ndf_linelist |&gt; \n  mutate(year_onset = isoyear(date_onset)) |&gt; \n  count(year_onset, week_onset_number) |&gt; \n  head(10)\n\n   year_onset week_onset_number  n\n1        2022                32  1\n2        2022                33  2\n3        2022                34  1\n4        2022                35  8\n5        2022                36  8\n6        2022                37 10\n7        2022                38 17\n8        2022                39 17\n9        2022                40 19\n10       2022                41 16\n\n\nThese counts are perfectly correct. You could plot them using faceting by year, or just filter a given year and plot the weekly numbers with the ISO week number on the x-axis. For example:\n\ndf_linelist |&gt; \n  mutate(year_onset = isoyear(date_onset)) |&gt; \n  count(year_onset, week_onset_number) |&gt; \n  ggplot(aes(x = week_onset_number,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  theme_classic(base_size = 16) +\n  facet_wrap(vars(year_onset),  # Magic to make subplots very easily\n             ncol = 1)\n\n\n\n\n\n\n\n\nIf you have not read about facetting yet, do no worry, think of this plot as a teaser of how easily you can make subplots by a variable! But this is out of the scope of this tutorial. Instead, we will show you another way of aggregating data by week which is robust to multi-year data.\n\n\nUsing the First Day of the Week\nAn alternative way of aggregating by week is to use the function floor_date() (also from the {lubridate} package), which returns the first date of a given period. You can think of it as a sort of rounding to the smallest value, but for dates.\nThe function has a unit argument that allows you to choose the period of interest (week, month‚Ä¶) and a week_start argument where you can pass the first day of the week (Mondays are 1).\n\ndf_linelist &lt;- df_linelist |&gt; \n  mutate(\n    week_outcome_monday = floor_date(date_outcome,\n                                     unit = \"week\",\n                                     week_start = 1)\n  )\n\nLet‚Äôs look at all these different time variables to figure out what‚Äôs happening:\n\ndf_linelist |&gt; \n  select(id, date_outcome, week_outcome_number, week_outcome_monday) |&gt;\n  arrange(date_outcome) |&gt;     # Sort the data by date\n  head(n = 10)\n\n   id date_outcome week_outcome_number week_outcome_monday\n1   1   2022-08-18                  33          2022-08-15\n2   2   2022-08-28                  34          2022-08-22\n3  10   2022-09-03                  35          2022-08-29\n4  16   2022-09-10                  36          2022-09-05\n5  22   2022-09-12                  37          2022-09-12\n6  14   2022-09-12                  37          2022-09-12\n7  41   2022-09-16                  37          2022-09-12\n8  20   2022-09-17                  37          2022-09-12\n9  17   2022-09-18                  37          2022-09-12\n10 23   2022-09-19                  38          2022-09-19\n\n\nIt might be easier to visualize if we calculate the day of the week associated with each date using the function wday() (which also belong to the {lubridate} package, are you maybe seeing a pattern here üòâ):\n\ndf_linelist |&gt; \n  # Get the name of the day for several date variables, to understand a bit better\n  mutate(\n    day_outcome = wday(date_outcome, \n                       label = TRUE, \n                       abbr = FALSE),\n    they_are_mondays   = wday(week_outcome_monday, \n                           label = TRUE, \n                           abbr = FALSE)) |&gt; \n  arrange(date_outcome) |&gt;     # Sort the data by date\n  select(date_outcome,\n         day_outcome,\n         week_outcome_number,\n         week_outcome_monday,\n         they_are_mondays) |&gt; \n  head(n = 10)\n\n   date_outcome day_outcome week_outcome_number week_outcome_monday\n1    2022-08-18    Thursday                  33          2022-08-15\n2    2022-08-28      Sunday                  34          2022-08-22\n3    2022-09-03    Saturday                  35          2022-08-29\n4    2022-09-10    Saturday                  36          2022-09-05\n5    2022-09-12      Monday                  37          2022-09-12\n6    2022-09-12      Monday                  37          2022-09-12\n7    2022-09-16      Friday                  37          2022-09-12\n8    2022-09-17    Saturday                  37          2022-09-12\n9    2022-09-18      Sunday                  37          2022-09-12\n10   2022-09-19      Monday                  38          2022-09-19\n   they_are_mondays\n1            Monday\n2            Monday\n3            Monday\n4            Monday\n5            Monday\n6            Monday\n7            Monday\n8            Monday\n9            Monday\n10           Monday\n\n\nThis illustrates how week_outcome_number and week_outcome_monday are two ways to have only one value representing a week. While week numbers are not unique as discussed before, dates are!\n\nAdd a new command to your mutate() call and create the variable week_onset_monday that contains the first day of the week for patient date of onset. Choose your argument as if the first day of the week is a Monday.\n\n\n\n\n\n\n\nTip\n\n\n\nGo read the help page for floor_date() to check out the list of possible units.\n\n\n\n\nActually Count Things\nNow that we have variables that represent week, it‚Äôs time to do the actual aggregation, ie count things!\n\nCount the number of patients per week of of onset, using the week start (week_onset_monday).\nHere are the first ten lines of what it should look like:\n\n\n   week_onset_monday  n\n1         2022-08-08  1\n2         2022-08-15  2\n3         2022-08-22  1\n4         2022-08-29  8\n5         2022-09-05  8\n6         2022-09-12 10\n7         2022-09-19 17\n8         2022-09-26 17\n9         2022-10-03 19\n10        2022-10-10 16"
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#draw-the-epicurve",
    "href": "sessions_extra/weekly_epicurves.html#draw-the-epicurve",
    "title": "Weekly Epicurves",
    "section": "Draw the Epicurve",
    "text": "Draw the Epicurve\nSo far so good, now we can pipe that aggregated data frame into our plot commands, making a couple adjustments to make it work.\n\nCreate a ggplot with the same look at the epicurve from the epicurve core session, but with the first day of the week on the x-axis. Don‚Äôt forget to update axes names.\n\nIt should look like that:\n\n\n\n\n\n\n\n\n\nWe see dates on the x-axis, but a bar represent data for a week starting on Monday."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#improve-the-axis",
    "href": "sessions_extra/weekly_epicurves.html#improve-the-axis",
    "title": "Weekly Epicurves",
    "section": "Improve the Axis",
    "text": "Improve the Axis\nNow, let‚Äôs learn how to tweak the appearance of that date axis!\n{ggplot2} automatically provided labels for the x-axis, trying to adjust for the range of data. That default may not always please us so we may want to manually force the labels to be more or less frequent, or change their format.\nTo modify the appearance of the axis, we will use another {ggplot2} function, from the scale family: scale_x_date().\n\nModify Breaks\nThe breaks controls the frequency of ticks on the axis.\nThe scale_x_date() function has a date_breaks argument that accepts the interval between two labels in a string. The string can have the following format: \"1 week\", \"2 weeks\", \"4 months\", \"2 years\" etc.\n\ndf_linelist |&gt; \n  count(week_outcome_monday) |&gt; \n  ggplot(aes(x = week_outcome_monday,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of outcome\",\n       y = \"Measles cases\",\n       title = \"Measles outcomes in Mandoul region (Chad)\") +\n  scale_x_date(date_breaks = \"4 months\") +  # Define breaks\n  theme_classic(base_size = 16)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\nYour turn! Modify your code so that the x-axis displays labels at reasonable intervals on your screen.\n\n\n\nImprove Labels\nNow that we changed the interval between ticks, let‚Äôs improve the labels themselves (the way dates are displayed on the axis). By default the labels are in the form year-month-day. We are going to show you two ways to change that.\n\nWith the {scales} Package.With the strptime Syntax\n\n\nThe scale_x_date() function has a label argument, that accepts several entries, among which a vector containing the dates, but also a function that generates labels from the breaks. The {scales} package provides such a function, label_date_short(), that attempts to create efficient and short labels for dates.\n\ndf_linelist |&gt; \n  count(week_outcome_monday) |&gt; \n  ggplot(aes(x = week_outcome_monday,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of outcome\",\n       y = \"Measles outcomes\",\n       title = \"Measles outcomes in Mandoul region (Chad)\") +\n  scale_x_date(date_breaks = \"2 months\",\n               labels = scales::label_date_short()) + # Short labels\n  theme_classic(base_size = 16)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\nModify your code and use label_date_short() to generate labels.\n\n\n\nIf you prefer to have full control on how to format dates, R has a syntax to describe date and time formats. There is a long help page with all the syntax items accessible with the help(strptime) command, but here are a few of the most useful elements to format a date label:\nDay:\n\n%d: from 01 to 31\n%e: from 1 to 31\n\nMonth:\n\n%b: abbreviated month name (current locale on your computer)\n%B: full month name (current locale on your computer)\n%m: month as a decimal number\n\nYear:\n\n%y: Year without the century (two digits)\n%Y: year in four digits\n\nSpecial separators:\n\n%n: newline\n%t: tab\n\nYou can assemble these items in a string, that you pass to different functions that accept a format as argument. Here we will pass it to the format() function to quickly see what display it creates, but after that we will use them in our graph command.\n\n# Create a date vector to explore different formats\nsome_dates &lt;- as.Date(c(\"2024-10-06\", \"2024-12-15\", \"2025-01-20\"))\n\n# Let's try out different syntax\nformat(some_dates, \"%Y-%b-%d\")\n\n[1] \"2024-Oct-06\" \"2024-Dec-15\" \"2025-Jan-20\"\n\nformat(some_dates, \"%Y-%b\")\n\n[1] \"2024-Oct\" \"2024-Dec\" \"2025-Jan\"\n\nformat(some_dates, \"%Y %B %d\")\n\n[1] \"2024 October 06\"  \"2024 December 15\" \"2025 January 20\" \n\nformat(some_dates, \"%y/%m/%d\")\n\n[1] \"24/10/06\" \"24/12/15\" \"25/01/20\"\n\nformat(some_dates, \"%d/%m/%Y\")\n\n[1] \"06/10/2024\" \"15/12/2024\" \"20/01/2025\"\n\n\nBack to the graph! The scale_x_date() function has an argument date_labels that accepts a string of text in the above format for the date labels.\n\ndf_linelist |&gt; \n  count(week_outcome_monday) |&gt; \n  ggplot(aes(x = week_outcome_monday,\n             y = n)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of outcome\",\n       y = \"Measles cases\",\n       title = \"Measles outcomes in Mandoul region (Chad)\") +\n  scale_x_date(\n    date_breaks = \"2 months\",      # Define intervals betw. labels\n    date_labels = \"%Y%n%b%n%d\") +  # Define format of labels\n  theme_classic(base_size = 16)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\nModify the code of your graph to have labels look like this:"
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#done",
    "href": "sessions_extra/weekly_epicurves.html#done",
    "title": "Weekly Epicurves",
    "section": "Done!",
    "text": "Done!\nCongratulations! Dates are complicated, and their formatting is often scary, but we hope this little introduction showed you some nice tricks for your epicurves!\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#going-further",
    "href": "sessions_extra/weekly_epicurves.html#going-further",
    "title": "Weekly Epicurves",
    "section": "Going Further",
    "text": "Going Further\n\nExtra execices\n\nUse date format like this: ‚Äú2024-oct.‚Äù, ‚Äú2024-dec.‚Äù\nCreate an epicurve with the date of consultation, using the first day of the week on the x-axis (format dates the way you prefer)\nCreate an epicurve for 2023 data using the date of hospital admission and the ISO week number on the x-axis.\n\n\n\nChallenge\n\nDo the epicurve for the date of onset, but instead of aggregating by week, aggregate it by month. Find an appropriate format for the labels."
  },
  {
    "objectID": "sessions_extra/weekly_epicurves.html#resources",
    "href": "sessions_extra/weekly_epicurves.html#resources",
    "title": "Weekly Epicurves",
    "section": "Resources",
    "text": "Resources\n\nChapter of the Elegant graphics for data analyses book on date scales\nGet started with lubridate from the package homepage."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html",
    "href": "sessions_companion/surveillance_companion.html",
    "title": "Surveillance",
    "section": "",
    "text": "Reuse skills aquired in the FETCH-R modules (import, clean and visualize data)\nMore specifically, analyze surveillance data to detect alerts and to help decide which alerts to prioritize for further field investigation."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#objectives",
    "href": "sessions_companion/surveillance_companion.html#objectives",
    "title": "Surveillance",
    "section": "",
    "text": "Reuse skills aquired in the FETCH-R modules (import, clean and visualize data)\nMore specifically, analyze surveillance data to detect alerts and to help decide which alerts to prioritize for further field investigation."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#introduction",
    "href": "sessions_companion/surveillance_companion.html#introduction",
    "title": "Surveillance",
    "section": "Introduction",
    "text": "Introduction\nWarning. This session is a companion to the case study Measles emergency response in the Katanga region (DRC) from the FETCH Surveillance module and might not make sense as a standalone tutorial.\nFrom an R point of view, this tutorial builds on skills acquired throughout the FETCH-R modules, introduces a couple of useful generalist functions, and some more specialized ones.\n\n\n\n\n\n\nTip\n\n\n\nDo not hesitate to refer to past sessions and your own scripts to remind yourself of some functions!"
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#setup-question-2",
    "href": "sessions_companion/surveillance_companion.html#setup-question-2",
    "title": "Surveillance",
    "section": "Setup (Question 2)",
    "text": "Setup (Question 2)\nSince this is part of a specific module, you will create a new RStudio project. We refer you to the main session for help creating a project and importing your data.\n\nSetup a new project\n\n\nCreate a folder surveillance_case_study associated with the FETCH Surveillance module. Add the following subfolders in it:\n\n\nüìÅ data\n\nüìÅ clean\nüìÅ raw\n\nüìÅ R\nüìÅ outputs\n\n\nCreate an RStudio project at the root of the surveillance_case_study folder.\nIf you do not already have the data from the case study download them.\n\n\n\n\n Download raw data\n\n\n\n 4. Unzip the archive if you just downloaded. Save the two Excel files in the subfolder data/raw.  5. Create a new script called import_clean.R and save it in the R subdirectory. Add metadata and a section to load the following packages: {here}, {rio}, and {tidyverse}.\n\n\n\nImport data in R\nReminder from the case study: you requested access to the routine surveillance data and the laboratory data to the DRC MoH. The MoH agreed to share it with you on a weekly basis. The first dataset you received is of week 20 in 2022 (the data we are working on are simulated).\n\nIf you have not done it already, open the raw data files in Excel (or another equivalent application) to inspect them.\n\nThe surveillance dataset is pretty straightforward to import. The lab dataset is slightly trickier: the data headers do not start at line one. Fear not, the skip argument from the import() function is made for this situation:\n\n# DO NOT RUN (PSEUDO-CODE)\nimport(\n  here(\"data\", \"raw\", \"example_file.xlsx\"), \n  skip = 3  # Skip the first three lines and start importing from line four.\n) \n\n\n\nAdd a section to your script dedicated to data importation.\nImport the surveillance data and store it into a data_surv_raw data frame. Then, import the lab data and save it in a data_lab_raw data frame.\nVerify that the import went well for both data frames (Viewer, check the dimensions or start and tail of data frames)."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#cleaning-question-2-and-3",
    "href": "sessions_companion/surveillance_companion.html#cleaning-question-2-and-3",
    "title": "Surveillance",
    "section": "Cleaning (Question 2 and 3)",
    "text": "Cleaning (Question 2 and 3)\n\nSurveillance data (Q2)\nNow that the data is correctly imported, we are going to perform some more checks, as usual, before a bit of cleaning.\n\nQuick checks\nDuring the case study you won‚Äôt have time to inspect and clean all columns within the available time, so for now we will focus on key columns: health_zone, week, totalcases and totaldeaths.\n\n\n\n\n\n\nNote\n\n\n\nIf you work on this tutorial in your own time, inspect the quality of the other columns and cross-check information of several columns. We refer you to the discussion of the case study for more checks to perform.\n\n\n\nAdd a section for the exploration and cleaning of the surveillance data into your script.  Now, explore the surveillance data frame and answer the following questions:\n\nWhat are the column names?\nHow many provinces are in the dataset? Is this coherent with what you expect?\nHow many health zones are in the dataset?\nWhat is the range of weeks?\nWhat is the min of totalcases?\nWhat is the max of the totaldeaths?\nDo you notice missing data for these columns? Are the strings of text clean?\n\n\n\n\nClean strings\nNow that we have a better idea of what is the state of the data, let‚Äôs start cleaning. We are going to write a cleaning pipeline like we did in the main modules (check out your code for the end of the cleaning modules to see an example final pipeline).\n\n\n\n\n\n\nTip\n\n\n\nTo facilitate debugging your pipeline, add commands one by one, checking each new command before adding a new one.\n\n\nWe are going to perform a couple of actions on the columns containing text to remove potential problems:\n\ntransform them to lower case\nremove potential extra spaces\nreplace - and spaces by _.\n\nBecause you may not have the time to do all of text colums, work on the health_zone or the province column for the following instructions.\n\nStart a cleaning pipeline with a mutate() that turns the chosen column to lower case.\n\nNow, we are going to introduce two handy functions for more text cleaning. The first one is the str_squish() function from the {stringr} package (help page here), that removes spaces at the start or end of the strings, and replace multiple spaces in the middle of a string by a single space.\n\nexamples &lt;- c(\" Trailing spaces     \",\n              \"Multiple     spaces\",\n              \" Everything     here \")\n\nstr_squish(examples)\n\n[1] \"Trailing spaces\" \"Multiple spaces\" \"Everything here\"\n\n\nThe other function, str_replace (also from the {stringr} package) does what you expect from its name: replace something in a string by something else. It has a pattern argument that take the bit of text to be replaced, and a replacement arguments that takes the bit of text to use as replacement:\n\nstr_replace(\n  \"HAUT-KATANGA\",    # A string of text (or a column, if used in a mutate)\n  pattern = \"-\",     # The bit to replace\n  replacement = \"_\"  # The replacement\n)\n\n[1] \"HAUT_KATANGA\"\n\n\n\nAdd steps to your mutate to:\n\nRemove all unwanted spaces from your chosen column\nChange the - and to _ in the column (in two steps)\n\nThe head of these columns should now be:\n\n\n  country     province    health_zone disease\n1     drc haut_katanga mufunga_sampwe measles\n2     drc haut_katanga        sakania measles\n3     drc haut_katanga        mitwaba measles\n4     drc haut_katanga kilela_balanda measles\n5     drc haut_katanga         likasi measles\n6     drc haut_katanga         kikula measles\n\n\nStore the result data frame in a data_surv object.\n\n\n\nSave the clean data\n\nUse the {rio} package to export data_surv to a .rds file called data_ids_2022-w20_clean in the data/clean subfolder of your project.\n\n\n\n\nLaboratory data (Q2)\nWe are going to follow the same steps as before for the lab data, and focus for now on the columns health_zone, igm_measles and igm_rubella.\n\nQuick checks\n\nPerform data checks on the colums names and dimensions. What are the categories for igm_measles and igm_rubella? What do you need to do to clean these columns?\n\n\n\nClean and recode strings\n\n\nStart a new cleaning pipeline to clean the lab data. As before, for one of the text column, change it to lower case, remove the extra spaces and replace the or - by _.\nRecode at least one of igm_measles or igm_rubella columns so that the categories are negatif, positif and indetermine.\nStore the cleaner version in a data_lab data frame\n\nThe head of the cleaned columns should now be:\n\n\n   health_zone igm_measles igm_rubella\n1      kambove    negative    negative\n2      kambove    negative    negative\n3      kambove    negative    positive\n4      kambove    negative    negative\n5      kambove    negative    positive\n6      kambove    negative    negative\n7      kambove    negative    negative\n8      kambove    negative    positive\n9       manika    negative    negative\n10   kamalondo    negative    negative\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou can use the case_when() function to recode the IGM columns.\n\n\n\n\n\nSave the clean data\n\nExport the data_lab data frame to a .rds file called data_lab_2022-w20_clean in the data/clean subfolder of your project.\n\n\n\n\nGoing further\nThis is the end of the steps for question 2! If you finished in advance and there is still time, reuse the functions we just saw to clean the other text columns in both datasets and recode both IGM column in the lab dataset.\nIf you still have time, perform more checks on the data:\n\nDisplay the health zone for which the numbers by age group add up to a different number than the total (if any)\nAre there any health zone for which the number of deaths is higher than the total number of cases?\nAre there duplicated lines (fully duplicated, or several values for health zone and week)?\nAre there unrealistic case numbers?\n\n\n\nComplete surveillance dataset (Q3)\nDuring the case study and the data checks, you realized that some weeks are missing from the surveillance dataset. You discussed the possible reasons for it, and the associated problems. Here we are going providing code to create a dataset that contains all weeks (assuming that missing weeks had zero cases and deaths).\nWe will use the function complete() from the {tidyr} package to add the missing lines and fill the columns containing numbers (totalcases and totaldeaths) with zeros. Due to the constrained time, we will give you the code for now, but check out the details in the Going further section when you have time.\n\n\nStart a new pipeline that takes the data_surv data frame and keeps only the columns province, health_zone, week and total cas.\nAdd a new step to your pipeline and paste the following code to complete the data frame:\n\n\ncomplete(\n  # Use all the existing combinaitions of province and health zone:\n  nesting(province, health_zone),\n  \n  # All the combinations whould have all weeks from the minimum (1) to the maximum (20) of the week column.\n  week = seq(min(week, na.rm = TRUE), \n             max(week, na.rm = TRUE)),\n  \n  # Fill these two columns with zeros for the newly created weeks:\n  fill = list(totalcases  = 0,  \n              totaldeaths = 0 \n  )\n) \n\n\nStore the result of the pipeline in a data frame called data_surv_weeks. The head of that data frame looks like:\n\n\n\n# A tibble: 10 √ó 5\n   province     health_zone  week totalcases totaldeaths\n   &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 haut_katanga kafubu          1          0           0\n 2 haut_katanga kafubu          2          0           0\n 3 haut_katanga kafubu          3          0           0\n 4 haut_katanga kafubu          4          0           0\n 5 haut_katanga kafubu          5          0           0\n 6 haut_katanga kafubu          6          0           0\n 7 haut_katanga kafubu          7          0           0\n 8 haut_katanga kafubu          8          0           0\n 9 haut_katanga kafubu          9          0           0\n10 haut_katanga kafubu         10          0           0\n\n\n\nWhen you are done, export that data frame to a .rds file called data_ids_2022-w20__weeks_clean in the data/clean subfolder of your project.\n\n\n\n\nGoing further\nThis is the end of question 3; if you are finished in advance, do not hesitate to carry on checking the data and listing potential problems and cleaning the columns. And go read the explanations for the complete() function, or its help page`."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#defining-alerts-question-4",
    "href": "sessions_companion/surveillance_companion.html#defining-alerts-question-4",
    "title": "Surveillance",
    "section": "Defining alerts (Question 4)",
    "text": "Defining alerts (Question 4)\n\nPreparing the dataset\nWe are going to carry on preparing the datasets for the analyses.\n\n\nIf you have not had the time to clean both health zone and province in both datasets, as well as both igm columns in the lab dataset you can import cleaner versions of the data:\n\n\n\n\n Download clean data\n\n\n\n Unzip the archive in your data/clean subfolder\n\nCreate a new script analysis_surv.R in the R subfolder. Add the metadata of the script, a package import section to import the packages {here}, {rio}, {tidyverse}, {lubridate} and {zoo}.\nAdd an import data section and import the clean .rds files in R using the import() function as usual (your cleaned version or the one you just downloaded). Assign the cleaned data to data_surv, data_lab and data_surv_weeks and carry on.\n\n\n\nSubset health zone\nTo simplify the work, we are going to focus on four health zones: Dilolo, Kampemba, Kowe, and Lwamba.\n\nStart a new pipeline from data_surv_weeks. Its first step is to only retain data for the the Dilolo, Kampemba, Kowe, and Lwamba health zones.\n\n\n\nWeekly indicator\nThe first indicator we want to caclulate is whether a health zone has 20 or more suspected cases in one week. This indicator is binary and only considers data in a given health zone and week, which corresponds to individual rows of our data frame.\n\nAdd a mutate() to your pipeline, to create a cases20 column that contains 1 if a given health zone has 20 cases or more in that week, and 0 otherwise.\n The top of the data frame created by the pipe thus far looks like this:\n\n\n# A tibble: 10 √ó 6\n   province     health_zone  week totalcases totaldeaths cases20\n   &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1 haut_katanga kampemba        1         75           0       1\n 2 haut_katanga kampemba        2         42           0       1\n 3 haut_katanga kampemba        3         46           0       1\n 4 haut_katanga kampemba        4         50           0       1\n 5 haut_katanga kampemba        5         43           0       1\n 6 haut_katanga kampemba        6         33           0       1\n 7 haut_katanga kampemba        7         45           0       1\n 8 haut_katanga kampemba        8         52           0       1\n 9 haut_katanga kampemba        9         38           0       1\n10 haut_katanga kampemba       10         46           0       1\n\n\n\n\n\nCumulative indicator\nThe second indicator you want to calculate is whether a health zone has more than 35 cumulated suspected cases within three weeks. This is a bit more complicated than the previous case: within health zone you need to calculate the sum of cases by groups of three weeks, but the groups are not fixed, they are rolling across time. We are getting in the territory of moving averages/sums/etc.\n\nCumulative sum\nWe are going to use the rollapply() function from the {zoo} package, as it is versatile and powerful. As its name suggests, the rollapply() function applies a function in a rolling way to a vector or a column of a data frame.\nSince we are constrained in time, we are going to provide the code of the rollapply() function to calculate the cumulative sum over three weeks, but check out the details in the Going further section when you have time.\nThis is how to do it for one health zone:\n\n# Create mini example data frame\nexample_df = data.frame(\n  province    = \"Haut Katanga\",\n  health_zone = \"Dilolo\",\n  week        = 1:10,\n  totalcases  = rep(1, times = 10))\n\nexample_df \n\n       province health_zone week totalcases\n1  Haut Katanga      Dilolo    1          1\n2  Haut Katanga      Dilolo    2          1\n3  Haut Katanga      Dilolo    3          1\n4  Haut Katanga      Dilolo    4          1\n5  Haut Katanga      Dilolo    5          1\n6  Haut Katanga      Dilolo    6          1\n7  Haut Katanga      Dilolo    7          1\n8  Haut Katanga      Dilolo    8          1\n9  Haut Katanga      Dilolo    9          1\n10 Haut Katanga      Dilolo   10          1\n\nexample_df |&gt; \n  mutate(cumcas = rollapply(\n    data  = totalcases, # The column to work on\n    width = 3,          # Width of the window  \n    FUN   = sum,        # Function to apply, here the sum   \n    align = \"right\",    # We are counting backward in time\n    partial = TRUE,     # Allows sum to be made even if window is less than three\n    na.rm = TRUE        # Extra unamed argument to be passed to the sum function\n  )\n  )\n\n       province health_zone week totalcases cumcas\n1  Haut Katanga      Dilolo    1          1      1\n2  Haut Katanga      Dilolo    2          1      2\n3  Haut Katanga      Dilolo    3          1      3\n4  Haut Katanga      Dilolo    4          1      3\n5  Haut Katanga      Dilolo    5          1      3\n6  Haut Katanga      Dilolo    6          1      3\n7  Haut Katanga      Dilolo    7          1      3\n8  Haut Katanga      Dilolo    8          1      3\n9  Haut Katanga      Dilolo    9          1      3\n10 Haut Katanga      Dilolo   10          1      3\n\n\n\n\nBy health zone\nNow, we want to do this cumulative sum by health zone. This is not that complicated: we are going to sort our data frame properly by health zone and week, and use the .by argument to tell the mutate() function to perform the action by health zone.\n\n\n\n\n\n\nNote\n\n\n\nYou may remember from the aggregation session how we summarized by groups using the .by argument in the summarize() function. This is exactly the same idea, except that instead of returning one value by group (as summarize() does), we want to return one value per row (as mutate() does).\nAs a little reminder of how summarize() + .by work, here is how we would calculate the total number of patients and deceased by province over the whole dataset:\n\ndata_surv_weeks |&gt; \n  summarize(\n    .by = province,  # Do things by province\n    cases_tot = sum(totalcases, na.rm = TRUE),\n    dead_tot  = sum(totaldeaths, na.rm = TRUE)\n  )\n\n# A tibble: 4 √ó 3\n  province     cases_tot dead_tot\n  &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n1 haut_katanga      5948       34\n2 haut_lomami       6928       70\n3 lualaba           1485        3\n4 tanganyika        7836      137\n\n\n\n\n\n\nAdd a step to your previous pipeline to sort the data frame by province, health zone and week with the arrange() function, which is a sorting function from {dplyr}\n\n\ndata_surv_weeks |&gt; \n  arrange(province, health_zone, week)\n\n\nThen add the following code to calculate the cumulative sum:\n\n\nmutate(\n  .by = c(province, health_zone),\n  cumcas = rollapply(\n    data  = totalcases,\n    width = 3,          # Width of the window  \n    FUN   = sum,        # Function to apply, here the sum   \n    align = \"right\",    # Windows are aligned to the right\n    partial = TRUE,     # Allows sum to be made even if window is less than three\n    na.rm = TRUE        # Extra unamed argument to be passed to the sum function\n  )\n)\n\n\nNow that the complicated part is over (the computing of the cumulative sum) we are left to summarize the information with a binary indicator, for heach week and health zone. Then we can create a second indicator, that aggregates the result of both the weekly and cumulative indicators, to say if an alert is to be raised.\n\n\nAdd a new step to your pipeline to calculate a binary indicator, cumcases35 that is 1 if the cumulative sum of cases for that week is equal or above 35 and 0 if not.\nAdd a new column alert, that is 1 if either the cases20 indicator or the cumcases35 indicator is 1 and 0 otherwise. You can use the | operator, which is R logical OR (the test will output TRUE if at least one of the condition is TRUE)..\nWhen the pipe is working, assign the result to a data_alert data frame.\n\nThe main columns of data_alert should look like this (other hidden for display):\n\n\n# A tibble: 10 √ó 7\n   health_zone  week totalcases cases20 cumcas cumcases35 alert\n   &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 kampemba        1         75       1     75          1     1\n 2 kampemba        2         42       1    117          1     1\n 3 kampemba        3         46       1    163          1     1\n 4 kampemba        4         50       1    138          1     1\n 5 kampemba        5         43       1    139          1     1\n 6 kampemba        6         33       1    126          1     1\n 7 kampemba        7         45       1    121          1     1\n 8 kampemba        8         52       1    130          1     1\n 9 kampemba        9         38       1    135          1     1\n10 kampemba       10         46       1    136          1     1\n\n\n\n\n\n\n\nHealth zones in alert\nAfter all this work we can finally investigate which health zones are in alert in the last week of our dataset (the now of the case study, week 20)!\n\nFilter your data frame to only keep the 20th week. Which health zones are in alert?\nCreate a vector hz_alert that contains the name of the health zones in alert, so that we can use it to filter data from these health zones later."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#draw-the-epicurve-question-4",
    "href": "sessions_companion/surveillance_companion.html#draw-the-epicurve-question-4",
    "title": "Surveillance",
    "section": "Draw the epicurve (Question 4)",
    "text": "Draw the epicurve (Question 4)\nLet us draw the epicurves of health zones currently in alert (in alert during week 20).\nWe have drawn very similar curves in the epicurve session. Here again we will use the ggplot() function with the geom_col() geom to create a barplot showing the distribution of cases. Since we already have the number of cases per week we do not need to count it ourselved like we did in the past.\n\nDraw an epicurve for one of the health zones in alert.\n The graph should look like this (but maybe for another health zone):\n\n\n\n\n\n\n\n\n\n\nThe facet_wrap() function allows us to plot several subplots in the same graph (see the faceting satellite for more information on faceting):\n\ndata_alert |&gt;\n  filter(health_zone %in% hz_alert) |&gt;\n  ggplot(aes(x = week, \n             y = totalcases)) + \n  geom_col(fill = \"#2E4573\") + \n  theme_bw(base_size = 15) + \n  labs(x = \"Week\",\n       y = \"N cases\",\n       title = \"Health zones in alert\") +\n  facet_wrap(vars(health_zone))   # One graph by health_zone"
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#key-indicators-question-6",
    "href": "sessions_companion/surveillance_companion.html#key-indicators-question-6",
    "title": "Surveillance",
    "section": "Key indicators (Question 6)",
    "text": "Key indicators (Question 6)\nLet‚Äôs gather more data on both alerts to help you decide which one to investigate.\n\n\n\n\n\n\nTip\n\n\n\nThis session builds on summarizing skills seen in the summary session. Do not hesitate to check it or your code if you forgot something.\n\n\n\nWeek of the first alert\n\nUse the summarize() function to display the first week the alert was raised for each health zone in alert. Which health zone started first?\n\n\n\nSurveillance data indicators\nLet us go back to the full surveillance dataset that contains more columns of interest.\n\n\nAdd a column cunder_5 to data_surv that contains the the number of cases less than five years.\nDerive, for each health zone in alert, the following indicators (organized in a single table):\n\n\nThe number of cases\nThe number of deaths\nThe number of less than five year olds\nThe CFR in percentage\nThe percentage of reported cases under five\n\nThe result should look like this:\n\n\n  health_zone n_cases n_deaths n_under_5 p_under_5     cfr\n1    kampemba     730        0       544 0.7452055 0.00000\n2      lwamba     256        2       233 0.9101562 0.78125\n\n\n\n\n\nLab data indicators\nNow we are going to use the laboratory data to derive a couple more indicators.\n\nFor each health zone in alert, derive the following indicators within one table:\n\nThe number of patients tested for measles\nThe number of positives for measles\nThe percentage of positives for measles\nThe number of patients tested for rubella\nThe number of positive for rubella\nThe percentage of positive for rubella\n\nThe result should look like this:\n\n\n  health_zone n_test_meas n_test_meas_pos positivity_measles n_test_rub\n1      lwamba          10               5          0.5000000         10\n2    kampemba          14               4          0.2857143         14\n  n_test_rub_pos positivity_rubella\n1              0         0.00000000\n2              1         0.07142857\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the section on summaries with conditions to remind you of the more advanced summaries."
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#done",
    "href": "sessions_companion/surveillance_companion.html#done",
    "title": "Surveillance",
    "section": "Done!",
    "text": "Done!\nCongratulation, you are done!\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_companion/surveillance_companion.html#sec-going-further",
    "href": "sessions_companion/surveillance_companion.html#sec-going-further",
    "title": "Surveillance",
    "section": "Going Further",
    "text": "Going Further\n\nExploring the complete() function\nLook at the simplified example below: the Kitenge health zone has no row for week 2:\n\n# Create simplified data frame for the example, with three weeks\nexample_df = data.frame(\n  province    = c(\"haut_katanga\", \"haut_katanga\", \"haut_katanga\", \"haut_lomami\", \"haut_lomami\"),\n  health_zone = c(\"likasi\", \"likasi\", \"likasi\", \"kitenge\", \"kitenge\"),\n  week        = c(1, 2, 3, 1, 3),\n  totalcases  = c(2, 1, 3, 1, 2))\n\nexample_df\n\n      province health_zone week totalcases\n1 haut_katanga      likasi    1          2\n2 haut_katanga      likasi    2          1\n3 haut_katanga      likasi    3          3\n4  haut_lomami     kitenge    1          1\n5  haut_lomami     kitenge    3          2\n\n\nWe use the following code to make sure that all the health zones have all the possible week values. Since the weeks range from one to three in that toy example, we pass a vector with weeks ranging from one to three:\n\n# Complete the missing week in Kitenge\nexample_df |&gt; \n  complete(\n    nesting(province, health_zone), \n    week = seq(1, 3),             # vector from 1 to 3\n    fill = list(totalcases = 0)   # fill new lines with zero (default is NA)\n  ) \n\n# A tibble: 6 √ó 4\n  province     health_zone  week totalcases\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 haut_katanga likasi          1          2\n2 haut_katanga likasi          2          1\n3 haut_katanga likasi          3          3\n4 haut_lomami  kitenge         1          1\n5 haut_lomami  kitenge         2          0\n6 haut_lomami  kitenge         3          2\n\n\nNow both health zones within provinces have values for all three weeks.\nYou may be wondering why we used nesting(province, health_zone) and not just health_zone. The reason is that there could be two health zones in different provinces with the same name. So we need to keep the province column into account. The nesting() argument tells the function to only use the existing combinations of the two columns in the data frame.\n\n\n\n\n\n\nNote\n\n\n\nIf we were passing both column names to the complete() function, it would try to cross all levels of province to all levels of health_zone, which does not make sense in this case:\n\n# Complete the missing week in kikula\nexample_df |&gt; \n  complete(\n    province, health_zone, \n    week = seq(1, 3),  # vector from 1 to 3\n    fill = list(totalcases = 0)\n  ) \n\n# A tibble: 12 √ó 4\n   province     health_zone  week totalcases\n   &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1 haut_katanga kitenge         1          0\n 2 haut_katanga kitenge         2          0\n 3 haut_katanga kitenge         3          0\n 4 haut_katanga likasi          1          2\n 5 haut_katanga likasi          2          1\n 6 haut_katanga likasi          3          3\n 7 haut_lomami  kitenge         1          1\n 8 haut_lomami  kitenge         2          0\n 9 haut_lomami  kitenge         3          2\n10 haut_lomami  likasi          1          0\n11 haut_lomami  likasi          2          0\n12 haut_lomami  likasi          3          0\n\n\n\n\nIt would be good to automatically pick the week series, since the data frame is going to change every week. To do that, we can remplace hardcoded values by the smallest and largest week number in the week column to get the range of weeks in the dataset:\n\n# Complete the missing week in kikula\nexample_df |&gt; \n  complete(\n    nesting(province, health_zone),\n    week = seq(min(week, na.rm = TRUE),   # vector ranging from smallest to largest week numbers in dataset\n               max(week, na.rm = TRUE)),\n    fill = list(totalcases = 0)\n  ) \n\n# A tibble: 6 √ó 4\n  province     health_zone  week totalcases\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 haut_katanga likasi          1          2\n2 haut_katanga likasi          2          1\n3 haut_katanga likasi          3          3\n4 haut_lomami  kitenge         1          1\n5 haut_lomami  kitenge         2          0\n6 haut_lomami  kitenge         3          2\n\n\n\n\nExploring the rollaply() function\nIf we want to do a cumulative sum of cases over three weeks, we want to apply the sum() function over windows of three weeks.\n\nexample_vect &lt;- rep(1, time = 10)\nexample_vect\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\nrollapply(\n  data  = example_vect,\n  width = 3,       # Width of the window  \n  FUN   = sum,     # Function to apply, here the sum   \n  align = \"right\"  # Value at row i is the sum of i, i-1 and i-2.\n)\n\n[1] 3 3 3 3 3 3 3 3\n\n\nWe inputed a vector of ten values and obtained a vector of lenght height, containing the sums. Obviously the function has a way of dealing with the extremities, and the size of the output is smaller than the size of the input. This would be a problem in a mutate() that creates new columns in a data frame, that need to be the same length as the existing columns.\nYou can control the behavior at the extremities:\n\nFill with NA when there is not enough values to calculate a window of three\nAllow partial sums (some values represent less than three weeks)\n\nThe argument fill = NA pads the extremities with NA (on the left in our case, since we aligned right):\n\nrollapply(\n  data  = example_vect,\n  width = 3,       # Width of the window  \n  FUN   = sum,     # Function to apply, here the sum   \n  align = \"right\", # Windows are aligned to the right\n  fill  = NA\n)\n\n [1] NA NA  3  3  3  3  3  3  3  3\n\n\nIt is a reasonnable way of dealing with incomplete windows. In our case however, we can do better: if there were 40 cases in week 1 it would be a cause for alert! We thus want the cumulative sum to be calculated from week one to be able to detect early alerts. The partial = TRUE argument allows this:\n\nrollapply(\n  data    = example_vect,\n  width   = 3,       # Width of the window  \n  FUN     = sum,     # Function to apply, here the sum   \n  align   = \"right\", # Windows are aligned to the right\n  partial = TRUE)\n\n [1] 1 2 3 3 3 3 3 3 3 3\n\n\nThis is close to what we need.\n\n\n\n\n\n\nNote\n\n\n\nKeeping in mind that since the first two weeks have only partial data compared to later weeks, a lack of alert in these weeks does not necessarily means there is no alert, just that we do not have the data to detect it.\n\n\nYou may remember that arithmetic operations in R return NA if some of the values are NA and we usually need to pass the argument na.rm = TRUE to the functions for them to ignore missing values.\nIf we had a slightly less complete vector we would have a problem:\n\nexample_vect_missing &lt;- c(1, 1, 1, NA, 1, 1)\n\nrollapply(\n  data  = example_vect_missing,\n  width = 3,       # Width of the window  \n  FUN   = sum,     # Function to apply, here the sum   \n  align = \"right\", # Windows are aligned to the right\n  partial = TRUE   # Allows sum to be made even if window is less than three\n)\n\n[1]  1  2  3 NA NA NA\n\n\nFortunately we can pass the na.rm = TRUE argument to rollapply() so that it passes it to sum().\n\nrollapply(\n  data  = example_vect_missing,\n  width = 3,       # Width of the window  \n  FUN   = sum,     # Function to apply, here the sum   \n  align = \"right\", # Windows are aligned to the right\n  partial = TRUE,  # Allows sum to be made even if window is less than three\n  na.rm = TRUE     # Extra unamed argument to be passed to the sum function\n)\n\n[1] 1 2 3 2 2 2\n\n\n\n\n\n\n\n\nTip\n\n\n\nHere we applied the sum() function to create a cumulative sum over 3 weeks. But you could, with minimal modifications, apply the mean() function to caclulate a moving average!\n\n\nA last point on the align argument. It defines the position of the rolling windows compared to the value being calculated. The default is that the window is centered: the value i is the sum of values i, i-1 and i+1.\nExample of the three alignements (pading with NA to better see what‚Äôs happening):\n\nrollapply(data  = c(5, 10, 1, 2, 5, 10),\n          width = 3, \n          FUN   = sum,\n          align = \"left\",\n          fill = NA)\n\n[1] 16 13  8 17 NA NA\n\nrollapply(data  = c(5, 10, 1, 2, 5, 10),\n          width = 3, \n          FUN   = sum,\n          align = \"center\",\n          fill = NA)  # The default\n\n[1] NA 16 13  8 17 NA\n\nrollapply(data  = c(5, 10, 1, 2, 5, 10),\n          width = 3, \n          FUN   = sum,\n          align = \"right\",\n          fill = NA)\n\n[1] NA NA 16 13  8 17\n\n\nIn our case we want the value for a given week to reflect this week and the week past, so we align the window right, to calculate backward in time (by opposition, if we aligned left we would calculate forwards in time).\n\n\nNicely formatted percentages\nThe percent function from the {scales} packages can add percentage formatting to a value.\n\nscales::percent(0.8556)\n\nIt takes an accuracy arguments that controls the number of decimals:\n\nscales::percent(0.8556,\n                accuracy = 0.1)\n\nYou can wrap it around the values that you calulate in the summary tables to change the proportions into nicely formatted percentages.\n\n\n\n\n\n\nImportant\n\n\n\nOnce you aplied that function the column is treated as text (since we added a % sign) and you will not be able to do further arithmetical operations on it."
  },
  {
    "objectID": "explore.html",
    "href": "explore.html",
    "title": "Explore",
    "section": "",
    "text": "Choose your own adventure by browsing all available sessions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Data Visualization\n\n\n\nCore\n\nVisualization\n\n\n\nLearn the basics of buidling plots with ggplot2, and create your first epicurve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Exploration\n\n\n\nSatellite\n\nData Exploration\n\n\n\nExplore your data after importation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Importation\n\n\n\nCore\n\nRStudio\n\nData Import\n\n\n\nCreate an Rstudio project, install useful packages and start importing data to work in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Basics\n\n\n\nCore\n\nData Manipulation\n\nData Cleaning\n\n\n\nAn introduction to data manipulation and cleaning using {dplyr}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Filtering and Recoding\n\n\n\nCore\n\nData Manipulation\n\nData Cleaning\n\nLogic\n\n\n\nUsing {dplyr} and conditional logic to filter and recode data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFaceting\n\n\n\nSatellite\n\nVisualization\n\n\n\nCreate a plot with multiple subplots (facets)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntoduction to Shiny\n\n\n\nSatellite\n\nVisualization\n\nDashboard\n\n\n\nLearn how to work and customise simple {shiny} apps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R\n\n\n\nCore\n\nR Basics\n\nData Types\n\n\n\nYour first steps in R. Learn your way around Rstudio, and meet some common R objects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Mortality Survey\n\n\n\nCompanion\n\nAnalysis\n\n\n\nCompanion session to the survey FETCH module\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Tables\n\n\n\nCore\n\nSummary Tables\n\n\n\nCreate summary tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurveillance\n\n\n\nCompanion\n\nAnalysis\n\n\n\nCompanion session to the surveillance Fetch-R module\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Epicurves\n\n\n\nSatellite\n\nVisualization\n\n\n\nPlot weekly epicurves and improve date labels on the x-axis\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sessions_companion/survey_basic.html",
    "href": "sessions_companion/survey_basic.html",
    "title": "Standard Mortality Survey",
    "section": "",
    "text": "Calculate person time at risk\nUse {srvyr} to estimate mortality rates"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#objectives",
    "href": "sessions_companion/survey_basic.html#objectives",
    "title": "Standard Mortality Survey",
    "section": "",
    "text": "Calculate person time at risk\nUse {srvyr} to estimate mortality rates"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#introduction",
    "href": "sessions_companion/survey_basic.html#introduction",
    "title": "Standard Mortality Survey",
    "section": "Introduction",
    "text": "Introduction\nThis session focuses on how to do a basic analysis of data from a retrospective mortality survey using the MSF standard mortality survey protocol. We will be using a case study wherein a survey was conducted following a cholera epidemic in Haiti in 2010.\nThis session assumes you have completed the basic learning pathway for R and are able to:\n\nImport data\nPerform basic cleaning using case_when()\nAggregate data using count() and summarize()\n\nIf you need to revisit or learn any of these topics, please refer to the core sessions of the learning pathway."
  },
  {
    "objectID": "sessions_companion/survey_basic.html#setup",
    "href": "sessions_companion/survey_basic.html#setup",
    "title": "Standard Mortality Survey",
    "section": "Setup",
    "text": "Setup\n\nThis session uses a specific case study. Download and unzip the associated folder then open the main.R script from the R folder:\n\n\n\n Download\n\n\n\n\nThe folder you have downloaded contains a (mostly) empty R script as well as Excel files for the Kobo form used in the survey and the data collected with it.\n\nTake a minute to open and investigate both the Kobo form and the raw data. What is contained in the different tabs of the dataset?"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#import",
    "href": "sessions_companion/survey_basic.html#import",
    "title": "Standard Mortality Survey",
    "section": "Import",
    "text": "Import\nOur dataset has two tabs, the first contains household level data and the second contains individual data. For now, we are most concerned with data about the individuals but we will ultimately need both. Let‚Äôs load it all into R (as well as the packages we will be using in today‚Äôs session).\n\nIn your script (main.R), add an appropriate header for the file and create a section that loads the following packages:\n\nhere\nrio\ndplyr\nlubridate\nsrvyr\n\nThen create a new section called Import and use rio to import the second sheet of your dataset into an object called df_raw. We don‚Äôt need all of the columns of this data, use select() to select only the following:\n\nsex\nage\nborn\nborn_date\njoined\njoined_date\nleft\ndied\ndied_date\ndied_cause\n_parent_index renamed as hh\n\nThen create a second object called df_hh containing the first sheet of your dataset keeping only the following columns:\n\ninterview_date\nclst_id\n_index renamed as hh\npresent\nconsent\n\nHint. Remember that when using select() you can quickly rename something by using an =, for example: hh = '_parent_index'."
  },
  {
    "objectID": "sessions_companion/survey_basic.html#first-look-and-recoding",
    "href": "sessions_companion/survey_basic.html#first-look-and-recoding",
    "title": "Standard Mortality Survey",
    "section": "First Look (and Recoding)",
    "text": "First Look (and Recoding)\nGreat! Now that we‚Äôve loaded our data let‚Äôs take a first look at our data. One of the first things we can do is check the structure of our data:\n\ndf_raw |&gt;\n  str()\n\nWe might also want to quickly check how many individuals in the dataset had died as our survey focuses on mortality:\n\ndf_raw |&gt;\n  count(died)\n\n\nUse count() to determine how many participants you had by sex.\n\nHm, 1 and 2 for sex are a bit ambiguous. It might be helpful to recode our categorical data to use more meaningful labels. For example:\n\ndf &lt;- df_raw |&gt;\n  # recoding\n  mutate(sex = case_when(sex == 1 ~ 'Male',\n                         sex == 2 ~ 'Female`,\n                         .default = NA))\n\n\nCreate a new section in your script called Cleaning. This section will have a ‚Äúcleaning pipe‚Äù that will take df_raw, perform several cleaning steps, and store the resulting dataframe into an object called df.  Using case_when(), create a new step in your cleaning pipe that recodes the categorical variables in your dateset. You can use the above recoding for sex. For the variables born, joined, left, died use the recoding:\n\n0 = No\n1 = Yes\n99 = Unknown\n\nFor died_cause use the recoding:\n\n1 = Diarrhoea\n2 = Fever\n3 = Respiratory Disease\n4 = Accident\n5 = During Delivery\n6 = Other\n99 = Unknown\nNA = Did Not Die\n\nThe head of df should look like this:\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh\n1 Did Not Die  1\n2 Did Not Die  1\n3 Did Not Die  1\n4 Did Not Die  1\n5 Did Not Die  1\n6 Did Not Die  1\n\n\nNow that we have nicer labels, let‚Äôs explore our data a bit more. For example:\n\nHow many people died of each potential cause?\nLook at the combinations of died, left, joined, and born. Which combinations are the most common? Does this make sense?\nWho died more, males or females? Who was more at risk?\n\n Hint. Remember that you can give multiple column names to count() in order to create contingency tables."
  },
  {
    "objectID": "sessions_companion/survey_basic.html#cleaning",
    "href": "sessions_companion/survey_basic.html#cleaning",
    "title": "Standard Mortality Survey",
    "section": "Cleaning",
    "text": "Cleaning\n\nDates as (Simple) Dates\nLet‚Äôs finish tidying up our data for analysis. One ting we need to do is make sure our data is all of the right type. We have already recoded everything for the categorical variables but we haven‚Äôt yet looked at dates.\n\nclass(df_raw$born_date)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nIt looks like our dates are of the type POSITct, let‚Äôs convert them to a more simple date format using ymd() from {lubridate}.\n\nAdd a step to your cleaning pipleing that uses ymd() from {lubridate} to convert the dates for born_date, joined_date, left_date, and died_date to simple dates. \nThe class() of df$born_date should now be Date:\n\nclass(df$born_date)\n\n[1] \"Date\"\n\n\n\n\n\nFixing Logical Issues\nA well designed Kobo form can go a long way to ensure we are collecting good quality data right from the start. For instance, we can make certain questions required to avoid missingness. We can also create ‚Äúconstraints‚Äù that will produce error messages when entered data violates preset rules; for example we might create a constraint that does not let the surveyor enter a date of death that falls outside the recall period.\n\nOpen the Kobo survey Excel file retrospective-mortality_kobo.xlsx and take a look at the column ‚Äúconstraint‚Äù. What were the constraints created in this file? Can you think of anything that wasn‚Äôt accounted for?\n\nDespite all of the protections we set up in the collection process there will always be a bit of cleaning needed. For example, while we have made sure that all dates fall within the recall period we didn‚Äôt create checks for other illogical relationships between dates. Let‚Äôs take a look, for instance, to see if anyone was born after they had died:\n\ndf_raw |&gt; \n  filter(died_date &lt; born_date)\n\n\nUse filter() to check for instances of people who joined the family after they had died. How many times did this happen?\nThe head of your output should look like this:\n\n\n  sex age born_date joined_date left_date  died_date born joined left died\n1   1  23      &lt;NA&gt;  2010-12-18      &lt;NA&gt; 2010-12-08    0      1    0    1\n  died_cause   hh\n1          1 1524\n\n\n\nWell that‚Äôs not great. How should we fix this? The exact best practice here is subject to a bit of debate but we recommend that you retain the date for death and remove the date for birth / joined (ie: reassign it to NA). Alternatively, if you catch this error during data collection you can ask the interviewer about the error. It may be the case that it was a typo and they remember the appropriate dates. Where possible, try to do this type of oversight on a daily basis so that issues can be corrected in real-time.\nIf we can only keep one date, why do we give preference to the date of death? The date of death is the more important variable for the purpose of this particular survey and is also a rare event, meaning that any lost dates might have a disproportionate impact on results. Additionally, we might expect that the date of death is more likely to be reliable compared to other dates (such as the exact date of birth or when someone joined / left the household).\n\n\n\n\n\n\nImportant\n\n\n\nIn this instance, there were only a couple cases of ‚Äúbirth or joining the household after death‚Äù out of a dataset of over 18,000 people so removing their birth / join dates isn‚Äôt a huge deal. If errors like this are more common, however, it may signal an important problem with the form design and / or the training of the surveyors. Quickly looking for problems like this (even in Excel) after the pilot and during the data collection phase can help bring your attention to any issues while you still have time to fix them.\n\n\n\nUsing mutate() and case_when(), add a step to your cleaning pipe to replace the problematic birth / joined dates with NA. How can you check if this worked correctly?\n\nCan you see any other issues in the data? I see two:\n\nThere are a few people who were born within the recall period but have an age greater than 0\nOne person died after having left the household\n\n\nHow would you handle these two issues? Think about the types of problems that might have produced them and the consequences of different cleaning strategies on your final results.  Bonus. Do you think either of these could have been prevented through a better Kobo design?\n\nLet‚Äôs consider the issue of being born in recall with an age above 0 first. How to handle ages below 1 can be tricky and surveyors should be explicitly trained on whether they ‚Äúround up‚Äù or ‚Äúround down‚Äù. Alternatively, modern surveys will tend to ask for age in months for individuals under a certain limit (typically 12, 23, or 59 months). Recording age in months for young children is particularly important in surveys that focus on issues like vaccination, malnutrition, or mortality where the health issues of interest are (potentially) associated with infants or children &lt; 5. A constraint could also have been added to the Kobo form to avoid this issue.\nFor the purpose of this survey, we don‚Äôt have any information on months so the best we can do is impose a consistent rule that anything &lt; 12 months should be recorded as 0. This means that if a child was born during recall (which is a period of &lt; 12 months) then their age must be 0.\n\nAdd a step to your pipeline that ensures that anyone born in recall has an age of 0. If you have done this correctly, you should be able to filter df to look only at individuals born in recall and verify that their age is 0:\n\ndf |&gt;\n  filter(born == 'Yes') |&gt;\n  pull(age) |&gt;\n  unique()\n\n[1] 0\n\n\n\nThe second issue is a bit more complex. Let‚Äôs take a look at the individual(s) in question:\n\ndf |&gt;\n  filter(left_date &lt; died_date)\n\n     sex age born_date joined_date  left_date  died_date born joined left died\n1   Male  25      &lt;NA&gt;        &lt;NA&gt; 2010-11-02 2011-03-08   No     No  Yes  Yes\n2 Female   3      &lt;NA&gt;  2011-01-05 2010-11-20 2011-03-28   No    Yes  Yes  Yes\n3 Female  60      &lt;NA&gt;  2011-03-08 2011-02-15 2011-03-15   No    Yes  Yes  Yes\n  died_cause   hh\n1  Diarrhoea   88\n2  Diarrhoea  236\n3  Diarrhoea 2861\n\n\n\nExamine the three individuals in the above output. Are all of them problematic? Why or why not?\n\nThe 3 year old and 60 year old don‚Äôt pose a problem, they simply left and then rejoined the household. The 25 year old, on the other hand seems to have left and then died without ever coming back in between. What should we do? Let‚Äôs think about why something like this might have appeared in our data. There are two main options:\n\nMaybe the person did rejoin the household but the participant forgot to mention it\nPerhaps the participant did not fully understand that the survey would only consider deaths when someone was still a member of the household when they died\n\nIf possible, we might discuss with the surveyor who conducted this interview to determine which option was more likely. In the absence of any additional information, however, we will probably need to go with option two. If we do that then we will need to recode this person as having lived rather than died as they were stil alive at the time that they left the household.\n\nAdd another step to your pipeline that recodes this individual as having lived, ie: their died value should be reset to 'No' and their date of death should be removed. If you check again for people who left the household prior to dying you should now see only two people:\n\n\n     sex age born_date joined_date  left_date  died_date born joined left died\n1 Female   3      &lt;NA&gt;  2011-01-05 2010-11-20 2011-03-28   No    Yes  Yes  Yes\n2 Female  60      &lt;NA&gt;  2011-03-08 2011-02-15 2011-03-15   No    Yes  Yes  Yes\n  died_cause   hh\n1  Diarrhoea  236\n2  Diarrhoea 2861\n\n\nNote. The decision to remove a death from the dataset is debatable. Remember that because deaths are rare events the addition / removal of one can have a disproportionate impact on mortality calculations. To minimize issues like this one, make sure to spend sufficient time when training surveyors to make sure they fully understand core concepts like the recall period and the idea of a ‚Äúcontinuous household‚Äù. Giving specific examples like this one during training can help surveyors to navigate these issues appropriately when they come up during data collection."
  },
  {
    "objectID": "sessions_companion/survey_basic.html#joining-household-level-data",
    "href": "sessions_companion/survey_basic.html#joining-household-level-data",
    "title": "Standard Mortality Survey",
    "section": "Joining Household Level Data",
    "text": "Joining Household Level Data\nOur individual level data is looking nice but they are completely detached from our household level data (remember df_hh from the start of the tutorial?). For example, we might like to know the interview date associated with each individual as well as the cluster they were in. To do this, we need to perform a join.\nAn in depth look at joins is beyond the scope of this session, but in essence joins are used to take the data from one dataframe and add it (row-wise) to another dataframe based on a variable that is shared by both datasets (such as an id). For example, here we want to go row by row in our individual level data (df) and add columns with the related household level information for each person (from df_hh). To do this, we will use the function left_join() from {dplyr}:\n\ndf |&gt;\n  left_join(df_hh) |&gt;\n  head()\n\nJoining with `by = join_by(hh)`\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh interview_date clst_id present consent\n1 Did Not Die  1     2011-03-29       1       1       1\n2 Did Not Die  1     2011-03-29       1       1       1\n3 Did Not Die  1     2011-03-29       1       1       1\n4 Did Not Die  1     2011-03-29       1       1       1\n5 Did Not Die  1     2011-03-29       1       1       1\n6 Did Not Die  1     2011-03-29       1       1       1\n\n\nNotice that here R has used the column hh (household id) as the common variable between the datasets; you can see a message indicating this right above the output of head().\n\n\n\n\n\n\nNote\n\n\n\nWhat does the ‚Äúleft‚Äù in left_join() mean? In a simple sense, left joins involve one dataset that data is added to (Dataset A) and another that data is taken from (Dataset B). Dataset A is the ‚Äúcore dataset‚Äù and the output will always include all of it‚Äôs rows. Rows from Dataset B will be kept if and only if left_join() finds an appropriate row in Dataset A to which they can be added. In our data, for example, rows of data on households which had no members (and thus do not appear in df) will not be included in the output of the above join.  In a left_join() R will always consider the first argument to be the core dataset (Dataset A); ie:\n\n# PSEUDO-CODE\nleft_join(dataset_a, dataset_b)\n\n\n\n\nAdd a final step in your cleaning pipe that uses left_join() to add the household level data to each of the rows in df and then converts interview_date to use a basic date format (as you did with born_date etc). Your final pipe should now do the following:\n\nUse df_raw as an input\nRecode categorical variables\nConvert dates to simple y-m-d format\nFix illogical data issues\nJoin household indicators\n\nIf everything went well, the head of df should look like this:\n\nhead(df)\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh interview_date clst_id present consent\n1 Did Not Die  1     2011-03-29       1       1       1\n2 Did Not Die  1     2011-03-29       1       1       1\n3 Did Not Die  1     2011-03-29       1       1       1\n4 Did Not Die  1     2011-03-29       1       1       1\n5 Did Not Die  1     2011-03-29       1       1       1\n6 Did Not Die  1     2011-03-29       1       1       1"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#mortality-calculations",
    "href": "sessions_companion/survey_basic.html#mortality-calculations",
    "title": "Standard Mortality Survey",
    "section": "Mortality Calculations",
    "text": "Mortality Calculations\nWith cleaning and joining out of the way we can finally move on to the fun part, analysis. We want to calculate the following:\n\nCrude Mortality Rate\nUnder Five Mortality Rate\nDiarrhoea Specific Mortality Rate\n\n\n(On paper) Write out the formula for each of these indicators. Do we already have all the necessary variables in our dataset for the calculations?\n\nThese indicators are rates, meaning they require a denominator in person-time at risk. Our dataset doesn‚Äôt have a column for this yet. Let‚Äôs fix that.\n\nPerson Time at Risk\nFor our survey, each individual‚Äôs person-time at risk is the time when they were:\n\nAlive and\nPart of the household\n\nMost people were alive and part of the household for the full recall period. For these people their time at risk is the full recall period. There are, however, a number of other options. For example:\n\n\n\n\n\n\nMost of these cases can all be handled the same way, in fact all but the last one. Take a minute and try to work out on paper a forumla for person time that we could use. Bonus points if you are able to convert this into code.\n\nComing up with a good formula here is not trivial, so let‚Äôs go through it together. Let‚Äôs imagine a person who joined the household in late 2010 and then died in February of 2011. If we put this person‚Äôs data into a dataframe we might have something like this:\n\nexample &lt;- data.frame(\n  date_interview = as.Date('2011-04-07'),\n  born = 'No',\n  date_born = NA,\n  joined = 'Yes',\n  date_joined = as.Date('2010-12-08'),\n  left = 'No',\n  date_left = NA,\n  died = 'Yes',\n  date_died = as.Date('2011-02-13')\n)\n\nexample\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07   No        NA    Yes  2010-12-08   No        NA  Yes\n   date_died\n1 2011-02-13\n\n\nTo calculate this person‚Äôs time at risk, we need to get ‚Äúwhen their person time started‚Äù and ‚Äúwhen their person time ended‚Äù. Then we take the difference between those two dates. For the start of someone‚Äôs person time, we need to pull the date when they where born / joined the household or (if they were present for the full preiod) the start date for the recall period. We can do this using case_when():\n\nrecall_start &lt;- as.Date('2010-10-17')\n\nexample |&gt;\n  mutate(pt_start = case_when(born == 'Yes' ~ date_born,\n                              joined == 'Yes' ~ date_joined,\n                              .default = recall_start))\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07   No        NA    Yes  2010-12-08   No        NA  Yes\n   date_died   pt_start\n1 2011-02-13 2010-12-08\n\n\nSimilarly, their time at risk ends when they die / leave or at the end of recall (when they were interviewed):\n\nexample |&gt;\n  mutate(pt_end = case_when(left == 'Yes' ~ date_left,\n                            died == 'Yes' ~ date_died,\n                            .default = date_interview))\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07   No        NA    Yes  2010-12-08   No        NA  Yes\n   date_died     pt_end\n1 2011-02-13 2011-02-13\n\n\nPutting it together, we can then calculate the total person time at risk as the difference between when the person time ended and when it started:\n\nexample |&gt;\n  mutate(pt_start = case_when(born == 'Yes' ~ date_born,\n                              joined == 'Yes' ~ date_joined,\n                              .default = recall_start),\n         pt_end = case_when(left == 'Yes' ~ date_left,\n                            died == 'Yes' ~ date_died,\n                            .default = date_interview),\n         pt = pt_end - pt_start)\n\n  date_interview born date_born joined date_joined left date_left died\n1     2011-04-07   No        NA    Yes  2010-12-08   No        NA  Yes\n   date_died   pt_start     pt_end      pt\n1 2011-02-13 2010-12-08 2011-02-13 67 days\n\n\n\nCreate a new section in your code called Calculate Person Time and initialize an object called recall_start with the date 2010-10-17. Add a code block adapting the above to create a pt column in df that calculates person time at risk. The head of df should now look like this:\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh interview_date clst_id present consent   pt_start     pt_end\n1 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n2 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n3 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n4 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n5 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n6 Did Not Die  1     2011-03-29       1       1       1 2010-10-17 2011-03-29\n        pt\n1 163 days\n2 163 days\n3 163 days\n4 163 days\n5 163 days\n6 163 days\n\n\n\nLet‚Äôs use range() to look at the maximum and minimum values of pt:\n\nrange(df$pt)\n\nTime differences in days\n[1] NA NA\n\n\nLooks like the value for person time at risk is sometimes missing. This happens when someone, for example, was born / joined / left / died but where the date information for that event is missing. How should we handle this? One option is to leave the value missing, meaning that person doesn‚Äôt contribute any person time at risk to the subsequent calculations of mortality. Alternatively, we can take the first availble value for which we have a date. So, for example, if we don‚Äôt know when someone was born we will use the start of the recall period as the begining of their person time at risk.\n\nWhat are the pros and cons of these two options? How would you adjust your above code to acheive option two?\n\nOption one artificially reduces the denominator of our mortality calculations, thus resulting in an overestimate of mortality. The second option will do the opposite. For today‚Äôs analysis we will go with option one and leave our code as is (missing values and all). Let‚Äôs look at our range again, this time ignoring the missing values:\n\nrange(df$pt, na.rm = TRUE)\n\nTime differences in days\n[1] -141  172\n\n\nNow we get numbers, but it looks like we have some negative values. What‚Äôs going on? Think back to the figure at the begining of this section. While most cases can be managed with our current calculation, it doesn‚Äôt account for individuals who left and then rejoined the household because this individuals will have joined_date &gt; left_date.\n\nThink about these individuals who leave and rejoin a household and the dates involved. Can you think of an equation for their person time at risk? How do you think this might be coded?\n\nFor these individuals, instead of taking a difference (between the end and start of time at risk), we instead need to calculate two chunks of time (before they left and after they returned) and then add them together. Here‚Äôs how we can do it:\n\ntmp &lt;- df |&gt;\n  mutate(\n    pt = case_when(\n      joined_date &gt; left_date & born == 'Yes' ~ (left_date - born_date) + (interview_date - joined_date),\n      joined_date &gt; left_date ~ (left_date - recall_start) + (interview_date - joined_date),\n      .default = pt\n    )\n  )\n  \nrange(tmp$pt, na.rm = TRUE)\n\nTime differences in days\n[1]   0 172\n\n\n\nAdapt your person-time code pipe to include this correction for individuals who left and rejoined the household. Then add a line remove the columns pt_start and pt_end as we won‚Äôt be using them anymore (and they won‚Äôt be accurate for individuals who left and returned). The head of df should now look like this:\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh interview_date clst_id present consent       pt\n1 Did Not Die  1     2011-03-29       1       1       1 163 days\n2 Did Not Die  1     2011-03-29       1       1       1 163 days\n3 Did Not Die  1     2011-03-29       1       1       1 163 days\n4 Did Not Die  1     2011-03-29       1       1       1 163 days\n5 Did Not Die  1     2011-03-29       1       1       1 163 days\n6 Did Not Die  1     2011-03-29       1       1       1 163 days\n\n\nBonus. Why did our above case_when need to have a separate case for individuals born during recall?\n\nNice, we are just about ready to calculate mortality! Notice that right now our values for person time are represented as a time difference (difftime class) in number of days. For our onward calculations it would be better if this were a simple numeric type.\n\nAdd a final step in your person-time pipe that convers pt to a numeric type using as.numeric().\n\n\n\nMortality Calculations\nNow we are (finally) ready to calculate mortality rates. We could do a basic calculation of this directly using the totals of number of deaths and cumulative person-time at risk:\n\nsum(df$died == 'Yes') / sum(df$pt, na.rm = TRUE) * 10000\n\n[1] 0.5405712\n\n\n\nHow would you interpret this mortality rate? Is it high?  In 2010, the baseline mortality rate in Haiti was 9 deaths per 1,000 person-years. Knowing this, calculate the excess mortality observed during this epidemic (expressed in excess deaths per 10,000 person-days)?  Hint. Start by converting the baseline rate to be represented in deaths per 10,000 person-days.\n\nSo far so good, but we haven‚Äôt included any confidence intervals in our calculation nor have we taken our survey design into account. To do this, we will make use of the {srvyr} package. This package was built for complex analysis of survey data and provides statistical methods to adjust for design effect and finite population size. An in depth discussion of design effect and how to adjust for it is beyond the scope of this lesson but, in essence, design effects are introduced when we use a sampling process that is not fully random. For example, the use of cluster sampling in this survey creates a design effect as we might expect people within a cluster to be more similar to each other than they are to other randomly selected people in the population. When we adjust for design effect we widen our confidence intervals (reduce our precision) to account for this non-random similarity.\nTo perform these adjustments {srvyr} needs to know a few things:\n\nThe id of the sample units requiring adjustment (in this case cluster ids)\nThe size of the population (needed to resolve both design affect and account for finite population size)\nThe weight of each cluster\n\nThe weight of the cluster is the product of two fractions:\n\nTotal population size / sample size and\nExpected cluster / true size of the given cluster\n\nIn principle, each of our clusters should have had 32 households. In practice, true cluster size may have deviated in some cases. We can use the function n_distinct() inside summarize() to add a column with the actual cluster size associated with each individual:\n\ndf |&gt;\n  summarize(.by = clst_id,\n    hh_count = n_distinct(hh)\n  ) |&gt;\n  head()\n\n  clst_id hh_count\n1       1       32\n2       2       32\n3       3       32\n4       4       32\n5       5       32\n6       6       32\n\n\n\nCreate a new section of your code called Calculate Mortality. Write a pipe that uses the above summarize statement to calculate number of households observed per cluster and then uses mutate() to add create columns weight and pop respectively containing the weights and total population size (in 2010, this was 228,425 people). Store the output of this pipe into an object called df_wt. The head of df_wt should look like this:\n\n\n  clst_id hh_count   weight    pop\n1       1       32 12.38546 228425\n2       2       32 12.38546 228425\n3       3       32 12.38546 228425\n4       4       32 12.38546 228425\n5       5       32 12.38546 228425\n6       6       32 12.38546 228425\n\n\nHint. The formula for weight is (population_size / sample_size) * (32 / hh_count).  Now, use left_join() to join the newly created weight and population data onto df. The head of df should now look like this:\n\n\nJoining with `by = join_by(clst_id)`\n\n\n\n\n     sex age born_date joined_date left_date died_date born joined left died\n1 Female  23      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n2   Male  30      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n3 Female  11      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n4 Female   5      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n5   Male   1      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n6 Female  19      &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;   No     No   No   No\n   died_cause hh interview_date clst_id present consent  pt hh_count   weight\n1 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n2 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n3 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n4 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n5 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n6 Did Not Die  1     2011-03-29       1       1       1 163       32 12.38546\n     pop\n1 228425\n2 228425\n3 228425\n4 228425\n5 228425\n6 228425\n\n\n\nFor {srvyr} to use our newly added variables and perform calculations we need to create a ‚Äúsurvey object‚Äù. This is a special class of dataframe that is specific to {srvyr} and is created using the function as_survey_design():\n\ntmp &lt;- df |&gt;\n  as_survey_design(\n    ids = clst_id,\n    wt = weight,\n    fpc = pop\n  )\n\n\nTry running the above code. What is the class of tmp? Does this object behave like a normal dataframe? Try doing some basic manipulations; for example:\n\nPull the data from the age column\nFilter to see only individuals who died\n\n\nAs you can see, once we apply as_survey_design(), we don‚Äôt have a normal dataframe anymore. So, we should store its output in a separate object, for example tmp or df_srvy. This ensures that df itself remains a standard dataframe available for other calculations, visualizations, etc.\n{srvyr} offers a number of functions to calculate indicators on survey data, most of the time used inside a summarize(). In our case, we will use the function survey_ratio() to calculate crude and specific mortality rates. The basic syntax of of survey_ratio() is pretty simple, for example we can use the following to calculate crude mortality rate:\n\ndf |&gt;\n  as_survey_design(\n    ids = clst_id,\n    wt = weight,\n    fpc = pop\n  ) |&gt;\n  summarize(\n    cmr = survey_ratio(\n      numerator = (died == 'Yes') * 10000,\n      denominator = pt,\n      vartype = 'ci',\n      deff = TRUE,\n      na.rm = TRUE\n    )\n  )\n\nSimple enough but let‚Äôs break down the arguments:\n\nnumerator is the numerator of our ratio, in this case the number of people who died (individuals for whom died was 'Yes') times 10,000 (to get a final result in 10,000 person-days)\ndenominator is the denominator of our ratio, in this case the amount of person time at risk (pt)\nvartype any variable(s) we want included to estimate error, here we chose confidence interval ('ci') but we could also ask for standard error ('se')\ndeff indicates whether we want an estimate of design effect to be included\nna.rm indicates whether {srvyr} should ignore missing values when performing the calculation\n\nThe output of this code is a new dataframe with our point estimate (cmr), confidence interval (cmr_low and cmr_upp), and the associated design effect.\n\nIn the above example we calculated crude mortality. Because this survey is associated with a particular outbreak (of cholera), we might also be interested in the disease specific mortality attributable to diarrhoea. Using the crude mortality code as a model, write code to calculate the diarrhoea specific mortality rate. You should find the following:\n\n\n# A tibble: 1 √ó 4\n   dsmr dsmr_low dsmr_upp dsmr_deff\n  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 0.369    0.257    0.482      2.64\n\n\nWe would also like to calculate under 5 mortality rate. In this case, we calculate crude mortality on the subset of our population that is under 5, ie: we need to filter our dataframe down to children under 5. Write some code to calculate under 5 mortality, remembering that you‚Äôll need to filter prior to creating your survey design object. You should find the following:\n\n\n# A tibble: 1 √ó 4\n   u5mr u5mr_low u5mr_upp u5mr_deff\n  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 0.677    0.363    0.990      1.14\n\n\nHow would you interpret all of the above mortality rates? Take a minute to outline how you might present these findings. Are there any other indicators you might want to calculate for a more complete investigation?"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#done",
    "href": "sessions_companion/survey_basic.html#done",
    "title": "Standard Mortality Survey",
    "section": "Done!",
    "text": "Done!\nWell done, you have now worked through how to import, clean, and calculate mortality rates from basic mortality survey data.\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_companion/survey_basic.html#going-further",
    "href": "sessions_companion/survey_basic.html#going-further",
    "title": "Standard Mortality Survey",
    "section": "Going Further",
    "text": "Going Further\n\nExtra Exercises\n\nEarlier, we calculated excess mortality rate, ie: how much higher our observed mortality was compared to baseline. Another indicator that we often present is the number of excess deaths observed during the recall period. How would you calculate this?\nUse {ggplot2} to create a bar plot of deaths over time.\nTake a look at the documentation for {srvyr} and see if you can use survey_mean() to calculate proportional mortality by cause of death.\nUse {gt} to create an attractive output table of the proportional mortality data you generated above.\nIn the cleaning section we corrected for cases where someone was born in the recall period but had an age &gt; 0. How could this have been prevented with a Kobo constraint?"
  },
  {
    "objectID": "sessions_extra/faceting.html",
    "href": "sessions_extra/faceting.html",
    "title": "Faceting",
    "section": "",
    "text": "Learn the syntax to make subplots real quick in {ggplot2}\nLearn arguments to modify the appearance of the subplots"
  },
  {
    "objectID": "sessions_extra/faceting.html#objectives",
    "href": "sessions_extra/faceting.html#objectives",
    "title": "Faceting",
    "section": "",
    "text": "Learn the syntax to make subplots real quick in {ggplot2}\nLearn arguments to modify the appearance of the subplots"
  },
  {
    "objectID": "sessions_extra/faceting.html#introduction",
    "href": "sessions_extra/faceting.html#introduction",
    "title": "Faceting",
    "section": "Introduction",
    "text": "Introduction\nThis satellite builds on the core epicurve session, which is a prerequisite. In that session, we learned how to create an epicurve of measles cases across time:\n\n\n\n\n\n\n\n\n\nNow, this plot is cool, but in your sitrep you would like to show the data by age group. There are several ways to do that:\n\nYou could, for each age group, filter your data frame and copy and paste the plotting command to create specific plots\nYou could learn to use for loops or apply() or map() family functions, which are very useful ways to automatize actions, and involve less copy and pasting\nOr you could trust {ggplot2} to have a solution‚Ä¶\n\nThe first option is tedious and it is error prone, and we advise against it; learning the tools of the second option will be a good investment of you time at some point as they are really powerful, but they are way out of the scope of this tutorial because a much simpler option already exist in {ggplot2}."
  },
  {
    "objectID": "sessions_extra/faceting.html#setup",
    "href": "sessions_extra/faceting.html#setup",
    "title": "Faceting",
    "section": "Setup",
    "text": "Setup\n\nWe will use the same clean linelist that we used in the past sessions, which you can download here:\n\n\n\n Download clean data\n\n\n\n Make sure this dataset is saved into the appropriate subdirectory of your R project and create a new script called faceting.R in your R directory. Add an appropriate header and load the following packages: {here}, {rio}, and {tidyverse}.  Finally, add an import section where you use {here} and {rio} to load your data into an object called df_linelist."
  },
  {
    "objectID": "sessions_extra/faceting.html#faceting",
    "href": "sessions_extra/faceting.html#faceting",
    "title": "Faceting",
    "section": "Faceting",
    "text": "Faceting\nThe function facet_wrap() allows you to replicate a graph based on the categories of a variable. For example, you could make the epicurve graph by sex, or by site. As other layers of a ggplot graph, you add it to your existing graph with a +. It creates a a figure with multiple small graphs, that {ggplot2} calls facets or small multiples.\n\nGet the Data Ready\nIn the following session, we will explain the code by creating subplots by sub-prefecture, and you will be plotting the epicurve by age group.\nIf we want to to plot anything by sub-prefecture, the sub_prefecture variable must be present in the aggregated data frame that we use to plot.\nLet‚Äôs create a new summarized dataset that has the number of patients by day and by sub-prefecture!\n\ndf_pref &lt;- df_linelist %&gt;%\n  count(date_onset, sub_prefecture,\n        name = \"patients\")\n\n\n\n  date_onset sub_prefecture patients\n1 2022-08-13       Moissala        1\n2 2022-08-17       Moissala        1\n3 2022-08-18       Moissala        1\n4 2022-08-22       Moissala        1\n5 2022-08-30       Moissala        2\n6 2022-09-01       Moissala        1\n\n\n\nYou will draw a plot of the number of admissions by age group, so you need a new data frame summarized by day and age group. Create this data frame, and call it df_age. It should have this format:\n\n\n  date_onset    age_group n\n1 2022-08-13  1 - 4 years 1\n2 2022-08-17 5 - 14 years 1\n3 2022-08-18   &lt; 6 months 1\n4 2022-08-22 6 - 8 months 1\n5 2022-08-30   &lt; 6 months 1\n6 2022-08-30 6 - 8 months 1\n\n\n\n\n\nAdd the Facet Layer to the Plot\nNow, let‚Äôs plot this data. Look at the code bellow: it is exactly the same as before but for the last line, which creates the facets:\n\ndf_pref %&gt;%\n  ggplot(aes(x = date_onset,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of onset\",\n       y = \"Measles cases\",\n       title = \"Measles cases in Mandoul (Chad)\") +\n  theme_classic(base_size = 15) +\n  facet_wrap(vars(sub_prefecture))   # Make the plot by sub-prefecture!\n\n\n\n\n\n\n\n\nIsn‚Äôt that incredible? As you can see, the function facer_wrap() takes as argument a variable name wrapped in the vars() function.\n\nNow is your turn, draw the epicurve by age group (still keeping all the plots improvement: labels, themes etc.)\nIt should look like this:"
  },
  {
    "objectID": "sessions_extra/faceting.html#customize-facets",
    "href": "sessions_extra/faceting.html#customize-facets",
    "title": "Faceting",
    "section": "Customize Facets",
    "text": "Customize Facets\nCheck out the function help page to learn about the arguments that facet_wrap() accepts. We will cover a couple here.\n\nNumber of Rows or Columns\nThe arguments nrow and ncol allow you to decide how many facets there should be on one row, respectively one column.\nFor exemple, we could have all plots on two rows, for a wide figure:\n\ndf_pref %&gt;%\n  ggplot(aes(x = date_onset,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of onset\",\n       y = \"Measles cases\",\n        title = \"Measles cases in Mandoul (Chad)\") +\n  theme_classic(base_size = 15) +\n  \n  facet_wrap(vars(sub_prefecture),\n             nrow = 2)  \n\n\n\n\n\n\n\n\nOr force the number of rows to four, which forces a taller figure:\n\ndf_pref %&gt;%\n  ggplot(aes(x = date_onset,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date of onset\",\n       y = \"Measles cases\",\n       title = \"Measles cases in Mandoul (Chad)\") +\n  theme_classic(base_size = 15) +\n  \n  facet_wrap(vars(sub_prefecture),\n             nrow = 4)  \n\n\n\n\n\n\n\n\n\nUsing one of the mentioned argument, create a graph with three columns.\n\n\n\nAxis Ranges\nDid you notice that in the graph we produced, all facets share the same axis in x and y? This is often a desired feature, as playing with axes is one of the best ways to mislead readers.\nThat being said, if you are more interesting in seeing the shape of the epicurve by category and less by comparing categories to each other, zooming on the available data can be appropriate (alert your reader to the scale variation though!)\nThe scales argument accepts the following strings:\n\n\"fixed\": the default, same limits on x and y for all facets\n\"free_x\": the x axis may have different limits in different facets\n\"free_y\": the y axis may have different limits in different facets\n\"free\": both axis may vary in different facets\n\nLook at this graph:\n\n\n\n\n\n\n\n\n\nWe kept time window on the x axis fixed but allowed the y axis to vary to better read the number of cases by sub-prefecture.\n\nYour turn! Draw you graph with age group as facets with a free y axis, and a fixed x axis."
  },
  {
    "objectID": "sessions_extra/faceting.html#done",
    "href": "sessions_extra/faceting.html#done",
    "title": "Faceting",
    "section": "Done!",
    "text": "Done!\nVery well done team! You have learned how to facet plots! This will work not only on bar plots such as epicurves, but also on other types of plots made by {ggplot2}.\nDepending on the size of your graph, the date labels on the x-axis may be a bit messed up, the ones in my examples definitely are. Fear not, this can be controlled and is the object of another satellite!\n\n\n\n Solutions file"
  },
  {
    "objectID": "sessions_extra/shiny_intro.html",
    "href": "sessions_extra/shiny_intro.html",
    "title": "Intoduction to Shiny",
    "section": "",
    "text": "Understand the basic structure of a Shiny App\nCustomise the UI by adding new input\nUnderstand the app server and reactivity\nAdd a new output to an existing shiny app"
  },
  {
    "objectID": "sessions_extra/shiny_intro.html#objectives",
    "href": "sessions_extra/shiny_intro.html#objectives",
    "title": "Intoduction to Shiny",
    "section": "",
    "text": "Understand the basic structure of a Shiny App\nCustomise the UI by adding new input\nUnderstand the app server and reactivity\nAdd a new output to an existing shiny app"
  },
  {
    "objectID": "sessions_extra/shiny_intro.html#libraries",
    "href": "sessions_extra/shiny_intro.html#libraries",
    "title": "Intoduction to Shiny",
    "section": "Libraries",
    "text": "Libraries\nDuring this session we will be using some specific R packages, please make sure they are installed and loaded\n\n# install.packages(\"shiny\")\n# install.packages(\"bslib\")\nlibrary(shiny)\nlibrary(bslib)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "sessions_extra/shiny_intro.html#setting-up-your-project",
    "href": "sessions_extra/shiny_intro.html#setting-up-your-project",
    "title": "Intoduction to Shiny",
    "section": "Setting up your project",
    "text": "Setting up your project\n\nProject structure\n\nIf not done already, download and unzip the course folder. Save the uncompressed folder to a location that is not connected to OneDrive and navigate into it.\n\n\n\n  Course Folder\n\n\n\n\nThis folder gives an example of a typical, single-file shiny app structure:\n\nüìÅ data\n\nüìÅ clean\n\nüìÑ moissala_data.rds\n\n\nüìÑ app.R\n\nIt creates a small epidemiological dashboard that analyses simulated data from a measles outbreak in Mo√Øssala, Southern Chad (moissala_data.rds). This folder will be your working directory for the entire session.\n\n\nDefinitions\nIn this session we will use interchangeably dashboard and app as both refer to the same interactive web application built with the {shiny} package. App is the technical term for any application build with the package, while dashboard emphasizes the visual presentation of data and metrics within that app."
  },
  {
    "objectID": "sessions_extra/shiny_intro.html#exploring-the-project",
    "href": "sessions_extra/shiny_intro.html#exploring-the-project",
    "title": "Intoduction to Shiny",
    "section": "Exploring the project",
    "text": "Exploring the project\nThis project is a very simple shiny app, and we are first going to understand how it is organised.\n\nHave a look at your project:\n\ncan you see where the data are stored? under which format?\nopen the file app.R and explore it\n\n\nAs any other project, our app has some data stored internally in data/clean folders, nothing fancy about that. Now the core of it lies in the app.R file.\n\nAnatomy of a Shiny App\nEvery Shiny app has four essential components:\n\nUser Interface (UI): defines the layout and appearance of your app\nServer: contains the logic that creates outputs and handles user interactions\nReactivity: through reactive expressions, this links the user (inputs) with the server logic and the visual outputs\nApp call: launches the application\n\nIn simple apps, these are all bundled into a single file app.R, but they are can be organised in a two-file structure with server.R and ui.R for more complex apps.\n\n\n\nKey structure and components of a simple shiny app\n\n\n\nLooking at app.R, can you identify these three components?\n\nWhere does the UI start and end?\nWhere does the server logic live?\nCan you spot where reactivity may lie ?\nWhat line of code actually runs the app?\n\n\nThe basic structure looks like this:\n\nlibrary(shiny)\nlibrary(bslib)\n\nui &lt;- page_fluid(\n  # UI elements go here\n)\n\nserver &lt;- function(input, output, session) {\n  # Server logic goes here\n}\n\nshinyApp(ui = ui, server = server)\n\nWhere two objects: ui and server (a function) are defined, and then passed to the shinyApp() function.\n\n\nRunning Your First App\n\nLet‚Äôs run the app to see it in action:\n\nOpen app.R\nClick the Run App button at the top of the script (or use Cmd/Ctrl + A + Enter)\nThe app should open in a new window or in the Viewer pane\n\nOnce the app is running, try interacting with any input controls:\n\nDo the outputs update when you change inputs?\nClick the Stop button (red square) to close the app when done\n\nIf the app doesn‚Äôt run, check the R console for error messages\n\nNow let‚Äôs dig into it and customise it"
  },
  {
    "objectID": "sessions_extra/shiny_intro.html#understanding-the-ui",
    "href": "sessions_extra/shiny_intro.html#understanding-the-ui",
    "title": "Intoduction to Shiny",
    "section": "Understanding the UI",
    "text": "Understanding the UI\nThe User Interface (UI) is where we design the appearance and layout of the app, we will define which inputs can user interact with, choose where to put the outputs (plot, tables) of the analyses and organise everything on the screen. It is built using layout, input and ouput functions that nest inside each other.\nWhy {bslib}?\nModern Shiny apps use the {bslib} package for layouts instead of the older base Shiny functions. {bslib} provides:\n\nModern Bootstrap CSS: Uses the latest Bootstrap framework for better styling and responsive design\nTheming capabilities: Easy customization of app appearance with custom themes\nBetter layouts: More intuitive and flexible layout functions\nRecommended by Posit: The Shiny development team recommends {bslib} for all new apps\n\nWhile you may see older Shiny code using functions like fluidPage() and sidebarLayout(), we‚Äôll focus on the modern {bslib} equivalents throughout this tutorial.\n\nLayout Functions\nLayout functions control how your app is structured visually. The most common {bslib} pattern is:\n\npage_sidebar(): Creates a page with a built-in sidebar layout:\n\nTakes a title argument for the app title\nContains a sidebar() function that defines the sidebar panel (typically for input controls)\nMain content (outputs) goes directly in the page, no wrapper needed\n\npage_fluid(): Creates a responsive page without a sidebar\npage_fillable(): Creates a page that fills the browser height - great for full-screen dashboards\n\nOther useful layout functions include layout_columns() and layout_column_wrap() for creating custom grid layouts, and navset_tab() for organizing content into tabs.\n\n\nInput Functions\nInput functions create interactive controls that users can manipulate. Each input has two key arguments:\n\ninputId: A unique identifier (string) used to access the value in the server\nlabel: Text displayed to the user describing the input\nany other arguments: depending on which input function is called\n\nCommon input functions include:\n\ndateRangeInput(): Date range picker with start and end dates\nselectInput(): Dropdown menu with multiple options\nnumericInput(): Input box for numeric values\nradioButtons(): Radio buttons for mutually exclusive choices\n\n\n\n\n\n\n\nTip\n\n\n\nThe inputId must be unique across your entire app. You‚Äôll reference it in the server function as input$date_range, input$variable, etc.\n\n\nThere are a loads of input possibilities, and they can be explored here: Shiny Widget Gallery\n\n\nOutput Functions\nOutput functions create placeholders in the UI where results will be displayed. Like inputs, each output has a unique ID, and the main ones are:\n\nplotOutput(): Placeholder for plots\ntableOutput(): Placeholder for data tables\n\nBut these are just placeholders - the actual content is generated in the server function using matching render*() functions. We will deal with this later\n\nIn the app.R file:\n\nWhat is the main layout function being used? (look for a function with layout in the name)\nCan you identify the title of the app?\nHow many input controls are there? What are their inputIds?\nHow many output placeholders are there? What are their outputIds and types?\nTry to trace the connection: for each output in the UI, can you find where it‚Äôs rendered in the server?\n\n\nLooking at the app.R file, we can identify the key input and output elements that make this app interactive.\n\nThere is one input control: a dateRangeInput() with inputId = \"date_range\" that allows users to select a start and end date for filtering the data. This input appears in the sidebar panel and is initialized with the minimum and maximum dates from the dataset.\nThere is one output placeholder: a plotOutput() with outputId = \"epicurve\" that displays the epidemic curve in the main panel.\nThe connection between the UI and server is established through these IDs in the server function: you‚Äôll see input$date_range being used to access the selected dates, and output$epicurve being assigned with renderPlot() to generate the actual plot. This pairing of dateRangeInput() with renderPlot() demonstrates the fundamental pattern of Shiny: inputs capture user choices, outputs display results, and the server connects them through reactive expressions.\n\n\n\nAdding a New Input\nNow that we understand how inputs and outputs work together, let‚Äôs add a new input to customize our epicurve.\n\nCan you think of a new input that would improve the epicurve? What additional control might users want?\n\nAn epicurve showing daily cases is detailed, but it can be visually overwhelming, especially over long time periods. What if we added an input that allows users to select the level of time aggregation (day/week/month/year)? This would let them zoom in for detailed daily trends or zoom out to see broader patterns.\nThere are several ways to do this, we will stick to an easy one, and achieve it by creating a select input with selectInput(). This function creates a dropdown menu where users can choose from predefined options. The basic syntax is:\n\nselectInput(\n  inputId = \"time_unit\",      # Unique ID to reference in server\n  label = \"Time Unit:\",        # Label shown to user\n  choices = c(\"Day\", \"Week\", \"Month\", \"Year\"),  # Options in dropdown\n  selected = \"Day\"             # Default selection\n)\n\n\nAdd this selectInput() to the sidebar panel in your UI, right below the dateRangeInput().\nOnce this is done, run the app again, what happens when you interact with your new input ?\n\nNice you managed to add a new input, but this is not yet active as it is not linked to our data. Now is a good occasion to dive into the server side of things !"
  },
  {
    "objectID": "sessions_extra/shiny_intro.html#understanding-the-server",
    "href": "sessions_extra/shiny_intro.html#understanding-the-server",
    "title": "Intoduction to Shiny",
    "section": "Understanding the Server",
    "text": "Understanding the Server\nThe server function is where the reactive magic happens. It takes user inputs and creates outputs dynamically based on user interactions.\n\nServer Structure\nThe server is always defined as a function with three arguments and the following pattern:\n\nserver &lt;- function(input, output, session) {\n  # Server logic goes here\n\n  # Create outputs\n  output$plot1 &lt;- renderPlot({\n    # Code to create a plot\n  })\n  \n  output$table1 &lt;- renderTable({\n    # Code to create a table\n  })\n\n}\n\n\ninput: A list-like object containing all input values from the UI (accessed as input$inputId)\noutput: A list-like object where you assign rendered outputs (accessed as output$outputId)\nsession: Contains information about the current Shiny session (advanced usage)\n\n\nData management in the server\nThe core of the server function is where we manipulate data based on user inputs. This is where we:\n\nFilter data based on user selections\nTransform variables\nCalculate statistics\nPrepare data for visualization\n\nThis all happens before we create the visual outputs. Think of it as a pipeline:\nuser inputs ‚Üí data manipulation ‚Üí outputs\nFor example, if a user selects a date range, we need to filter our dataset to only include cases within that range. If they want to change the time aggregation, we need to transform the dates accordingly. The server handles all this logic.\n\nIn the server function of app.R:\n\nCan you find where the data is been loaded ? Where is it compared to the server and ui ?\nCan you find where data is being filtered?\nWhich input values are being used in the filtering? (look for input$...)\nWhere does the filtering happen relative to creating the plot?\n\n\nNotice that in app.R, the data is loaded outside the server function (and outside the UI as well). This way, it‚Äôs loaded once when the app starts, not every time a user interacts with it. This is much more efficient than loading data inside the server, which would reload it repeatedly.\nData are then used in the server where they are filtered based on the date range (input$date_range) defined by the user, thus creating a new, filtered dataset: filtered_data.\n\n# filter the data based on user input\n  filtered_data &lt;- reactive({\n    linelist |&gt;\n      filter(\n        date_onset &gt;= input$date_range[1], # the lower bound of the date range is accessed here\n        date_onset &lt;= input$date_range[2] # the upper bound of the date range is accessed here\n      )\n  })\n\nNow meticulous observer will notice that our filtering step is wrapped into a reactive({}) call - here is a very important concept, this is what makes this all process interactive !\n\n\n\nReactivity: The Heart of Shiny\nReactivity is what makes Shiny apps interactive. When a user changes an input, Shiny automatically knows which outputs depend on that input and re-runs only the necessary code.\nThe basic reactive flow looks like this:\nUser changes input ‚Üí Reactive expression updates ‚Üí Output re-renders\n\nReactive Expressions\nIn our app.R, we use reactive() to create a filtered version of the data based on the date range selected by the user:\n\nfiltered_data &lt;- reactive({\n  linelist |&gt;\n    filter(\n      date_onset &gt;= input$date_range[1], # lower bound of date range\n      date_onset &lt;= input$date_range[2]  # upper bound of date range\n    )\n})\n\nThis creates a reactive expression that:\n\nAutomatically re-runs when input$date_range changes\nCaches its result so it doesn‚Äôt re-compute unnecessarily\nCan be used by multiple outputs efficiently\nIs called with parentheses: filtered_data() (like a function)\n\nWith this reactive expression our data management step is automatically linked to all of our user inputs, and can be automatically updated when they change !\n\n\n\n\n\n\nTip\n\n\n\nReactive expressions are called with () because they are special functions. Always use filtered_data(), not filtered_data, when you want to access the filtered data.\n\n\n\nIn the server function:\n\nCan you identify the reactive() expressions? How many do you see ?\nWhat input value triggers them to update?\nWhere is filtered_data() being used? (remember the parentheses!)\nWhat would happen if the user changes the date range?\n\n\nThe beauty of reactivity is that you don‚Äôt need to manually tell Shiny when to update outputs. Shiny automatically tracks dependencies:\nwhen input$date_range changes ‚Üí filtered_data() updates ‚Üí any output using filtered_data() re-renders.\n\n\nImplement our new input\nRemember the selectInput() we added for time aggregation? Now we need to make it functional in the server by modifying our reactive expression. You noticed earlier that we are indeed working with two reactive expression\n\nfiltered_date() which then feeds in\nplot_df() to create a dataframe used for plotting the epicurve.\n\n\n\nHow are data been processed for plotting ?\nDo we need to change anything if we want to aggregate ?\n\n\nCurrently, our epicurve counts cases by exact date using the reactive expression filtered_data():\n\nfiltered_data() |&gt;\n  count(date_onset)\n\nBut we want to aggregate by the time unit selected by the user (input$time_unit). So we need to transform date_onset based on the selected time unit before counting, and this can be implemented in our plot_df() reactive expression.\n\n\n\n\n\n\nTip\n\n\n\nYou can use floor_date() from the {lubridate} package to round dates to different units. The syntax is:\n\nfloor_date(date_column, unit = \"week\")  # unit can be \"day\", \"week\", \"month\", \"year\"\n\nNote that floor_date() expects lowercase units (‚Äúweek‚Äù, ‚Äúmonth‚Äù), but our selectInput() choices are capitalized (‚ÄúWeek‚Äù, ‚ÄúMonth‚Äù). You‚Äôll need to convert them using tolower().\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNotice that we use two separate reactive expressions for data manipulation:\n\nfiltered_data(): Returns the filtered linelist with all individual cases\nplot_df(): Returns an aggregated count dataframe\n\nThis separation is intentional. By keeping filtered_data() in its original linelist format (one row per case), we maintain flexibility to add other outputs later (like tables, summary statistics, or additional plots) that might need access to individual case data. The plot_df() reactive then handles the specific aggregation needed for the epicurve visualization.\n\n\n\nModify the plot_df() reactive expression to:\n\nCreate a new variable that floors date_onset to the selected time unit (use mutate() and floor_date)\nCount by this new aggregated date variable instead of date_onset\n\nThen make necessary changes so that the plotting function use newly aggregated date on the x-axis\n\n\nOnce you‚Äôve implemented this, test your app:\n\nChange the time unit dropdown - does the epicurve update automatically?\nTry ‚ÄúWeek‚Äù - do you see weekly aggregated bars?\nTry ‚ÄúMonth‚Äù - does it show monthly counts?\nCombine it with the date range filter - does everything work together?\n\n\nThis demonstrates the power of reactivity: you modified the render function to use input$time_unit, and Shiny automatically knows to re-run it when either input$time_unit OR input$date_range changes because they are linked through reactive expressions !\n\n\n\nRender Functions\nNow that we understand how data flows through reactive expressions, we need to display it to the user (even though you just made it work !). This is where render functions come in.\nRender functions are the final step in our reactive pipeline. They take the processed data and create visual outputs (plots, tables, text) that appear in the UI.\n\nThe Render Pattern\nEach type of output placeholder in the UI (see Output Functions) has a corresponding render function in the server:\n\n\n\nUI Function\nServer Function\nWhat it creates\n\n\n\n\nplotOutput()\nrenderPlot()\nPlots and visualizations\n\n\ntableOutput()\nrenderTable()\nData tables\n\n\ntextOutput()\nrenderText()\nPlain text\n\n\nverbatimTextOutput()\nrenderPrint()\nConsole-style output\n\n\n\nThese are the base Shiny render functions, but many visualization and table packages provide their own specialized render functions. For example, {leaflet} has renderLeaflet() for interactive maps, and {highcharter} has renderHighchart() for interactive charts. These package-specific render functions follow the same pattern but are optimized for their respective output types.\n\n\nHow Render Functions Work\nRender functions are reactive endpoints. They:\n\nAutomatically re-execute when their reactive dependencies change\nSend the updated output to the UI\nAre always assigned to output$ with a name that matches an outputId in the UI\n\nFor example, if we have plotOutput(\"epicurve\") in the UI, we need output$epicurve &lt;- renderPlot({...}) in the server.\n\nIn the server function:\n\nHow many output$... assignments are there?\nDo the output names match the outputIds in the UI?\nWhat type of render function is being used?\nCan you identify the reactive dependencies? (What inputs or reactive expressions does it use?)\n\n\nIn our app, we have one render function that creates the epicurve:\n\noutput$epicurve &lt;- renderPlot({\n  plot_df() |&gt; \n    ggplot(aes(x = date_onset, y = n)) +\n    geom_col(fill = \"steelblue\") +\n    labs(\n      title = \"Cases by Date of Onset\",\n      x = \"Date of Onset\",\n      y = \"Number of Cases\"\n    ) +\n    theme_minimal()\n})\n\nNotice how this render function depends on plot_df(), which itself depends on filtered_data(). When the user changes the date range OR the time units, filtered_data() updates, which updates plot_df(), which automatically triggers renderPlot() to re-run and update the plot in the UI.\nInside a render function, you write normal R code to create your output. For renderPlot(), this is typically {ggplot2} code. For renderTable(), you‚Äôd prepare a dataframe. The key is that the last line of the render function should produce the output you want to display. This\n\nInside the render function for our epicurve, can you make the following changes to improve the visualization:\n\nChange the fill color of the bars to something more appropriate for disease surveillance (hint: try a red tone like \"#E74C3C\" or \"coral\")\nAdd a border to the bars using the color argument in geom_col() (try color = \"white\" to separate bars clearly)\n\nBonus:\n\nUpdate the plot title to be more informative - include the time unit being displayed (hint: you can use paste() or glue() to combine text with input$time_unit)\nImprove the axis labels:\n\nMake the x-axis label dynamic based on the selected time unit\nConsider adding units to the y-axis (e.g., ‚ÄúNumber of Cases (n)‚Äù)\n\nAdd a subtitle that shows the date range being displayed using subtitle in labs()\nAdd a caption that says how many cases with valid dates are been displayed\n\n\nThe customization possibilities within render functions are as infinite as the customization of plots themselves: there‚Äôs no right or wrong way to style your outputs, only what best communicates your data (except pie charts‚Äîbecause nothing says ‚ÄúI understand data visualization‚Äù quite like a circle divided into 9 differently colored slices that all look identical).\nHere is a suggestion:\n\nrenderPlot({\n    n_valid &lt;- nrow(filtered_data() |&gt; filter(!is.na(date_onset)))\n\n    plot_df() |&gt;\n      ggplot(aes(x = agg_date, y = n)) +\n      geom_col(fill = \"#E74C3C\", color = \"white\", linewidth = 0.3) +\n      labs(\n        title = paste(\"Cases by\", input$time_unit),\n        subtitle = paste(\n          \"Date range:\",\n          format(input$date_range[1], \"%b %d, %Y\"),\n          \"to\",\n          format(input$date_range[2], \"%b %d, %Y\")\n        ),\n        x = paste(\"Date of Onset (\", input$time_unit, \")\", sep = \"\"),\n        y = \"Number of Cases (n)\",\n        caption = paste(\n          \"Displaying\",\n          n_valid,\n          \"cases with valid dates\"\n        )\n      ) +\n      theme_minimal()\n  })\n}\n\nNow that we‚Äôve explored all three components of a Shiny app: the UI (defining inputs and output placeholders), reactive expressions (managing and filtering data based on user inputs), and render functions (creating the actual visualizations), you have all the building blocks needed to add new features.\nLet‚Äôs put this knowledge into practice by adding a completely new output to our dashboard!"
  },
  {
    "objectID": "sessions_extra/shiny_intro.html#adding-a-new-output",
    "href": "sessions_extra/shiny_intro.html#adding-a-new-output",
    "title": "Intoduction to Shiny",
    "section": "Adding a new output",
    "text": "Adding a new output\nNow that you‚Äôve built and customized your first Shiny app, let‚Äôs pause and think about dashboard design more broadly. Before we add another output, we should ask ourselves: what makes a dashboard useful?\nA good dashboard isn‚Äôt just about cramming in every possible visualization‚Äîit‚Äôs about communicating insights effectively and enabling decision-making. Before adding any new element, ask yourself:\n\nPurpose: What question does this output answer? Who is the intended user?\nClarity: Does this visualization communicate information clearly, or does it add noise?\nActionability: Can users make decisions or take actions based on what they see?\nContext: Does this output complement existing visualizations or duplicate information?\n\n\n\n\n\n\n\nImportant\n\n\n\nEvery element should serve a purpose. If you can‚Äôt articulate why something is there, it probably shouldn‚Äôt be.\n\n\n\nThink about our current dashboard:\n\nWho is it designed for?\nWhat questions can it answer?\nWhat questions can‚Äôt it answer that might be important?\nIf you were responding to this outbreak, what would you want to see next?\n\n\n\nPlanning Your Next Output\nRather than randomly adding features, let‚Äôs be intentional about what we add. Any epidemiologist in the room will tell you that our dashboard currently lacks Person and Place components to complement the Time analysis from the epicurve (which could benefit from much improvement let‚Äôs be honest, but we are short on time). Maps are another level in R, so let‚Äôs stick with something simpler and focus on the Person analysis.\nFor a person analysis in outbreak investigation, we typically want to understand:\n\nAge distribution: Who is being affected? Are certain age groups more vulnerable?\nGender distribution: Are there gender-specific patterns?\nCase demographics: What are the key characteristics of cases?\n\nWe‚Äôre going to add a comprehensive person analysis section that includes:\n\nAn age pyramid showing the distribution of cases by age group and gender\nA summary table displaying key demographic statistics\n\nOk in order to implement these new outputs, we need to deal with two things which you know now:\n\nthe UI: to keep our dashboard organized we‚Äôll use tabs to separate the _Summary Statistics from the Age Pyramid plot.\nthe server: We need to code the logic to generate a summary table and an age pyramid.\n\n\n\nThe UI\n\nUnderstanding Tabsets\nBefore we start coding, let‚Äôs understand how to organize multiple outputs using tabs. Tabsets allow you to place related visualizations in separate panels that users can switch between, keeping the interface clean and organized.\nIn {bslib}, you create tabs using:\n\nnavset_tab(): Creates the container for tabs\nnav_panel(): Defines each individual tab with a title and content\n\nThe basic structure looks like:\n\nnavset_tab(\n  nav_panel(\"Tab 1 Title\", \n            plotOutput(\"plot1\")),\n  nav_panel(\"Tab 2 Title\", \n            tableOutput(\"table1\"))\n)\n\nEach nav_panel() can contain multiple outputs (plots, tables, text).\n\nStep 1: Restructure the UI with tabsets\nModify your page_sidebar() in the UI to use a tabset structure:\n\nBelow the epicurve, add a new section with a navset_tab() for the Person Analysis\nCreate two tabs for the Person Analysis:\n\nFirst tab: ‚ÄúAge Pyramid‚Äù containing a placeholder for the age pyramid plotOutput(\"age_pyramid\")\nSecond tab: ‚ÄúSummary Statistics‚Äù containing a placeholder for the table tableOutput(\"summary_table\")\n\n\nRun your app to see the tabbed interface.\n\nDoes your epicurve still appear at the top?\nDo you see the two tabs for Person Analysis below it?\n\n\nOk now your page_sidebar() should look like:\n\n# Inside page_sidebar(), after the epicurve\nplotOutput(\"epicurve\"),\n\nh3(\"Person Analysis\"), \n\nnavset_tab(\n  nav_panel(\"Age Pyramid\",\n            plotOutput(\"age_pyramid\")),\n  nav_panel(\"Summary Statistics\",\n            tableOutput(\"summary_table\"))\n)\n\nLet‚Äôs not complicate things further for now and not add anymore inputs - no that our UI is set up we can implement the server logic.\n\n\n\nThe server\n\nAdding the Age Pyramid\nAn age pyramid is a powerful visualization for understanding the demographic structure of cases. It shows the distribution of cases by age group, split by gender, with males on one side and females on the other. To create an age pyramid, we need to:\n\nCount cases by age group and gender (age_group adn `` in the linelist)\nCreate a horizontal bar chart with males and females on opposite sides\n\nTo simplify this process, and focuss on the learning of {shiny} concepts, we will use the package {apyramid} to build the Age pyramid plot\n\napyramid::age_pyramid(\n  data = our_data,\n  age_group = \"age_group\",\n  split_by = \"sex\"\n)\n\n\nStep 2: Conceptualise the age pyramid\nIn the server function, thing carefully:\n\nWhat dataset do you want to use for the age pyramid ?\nDo you want the dataset to by a reactive expression that responds to user inputs ?\nDo you want / need to create a new reactive expression for this plot ?\n\n\nWe want our age pyramid to be influenced by our time_unit and date_range inputs the same way that our epicurve is to make sure they display the same information. Thus, we want to work with the reactive expression filtered_data() to maintain consistency.\nNow we need to perform some data manipulation prior to plotting (remove NAs), but these are not dependent on any user inputs, and could be implemented right before plotting. However, to maintain flexibility in our dashboard, and for a future input that could allow the user to interact with the age pyramid (also to practice !), we are going to create another reactive expression called pyramid_df() prior to the plot\n\nStep 3: Create the age pyramid data\nIn the server function, after the epicurve render function:\n\nCreate a reactive expression called pyramid_df that removes NAs for age_group and sex\nHow are you going to access this reactive expression ?\n\nStep 4: Call the age pyramid render function\nRight after defining pyramid_df(),\n\ncreate a render function to plot the age pyramid\n\n\n\n\n\n\n\nTip\n\n\n\nRemember, you need to assign the renderPlot() function to an output$ of a corresponding ID that you have defined in a placeholder in the UI\n\n\n\nIn this render function add the call to apyramid to create the function but be careful with the data you are using !\n\n\n# this will egenrate an epicurve\napyramid::age_pyramid(\n  data = ______,\n  age_group = \"age_group\",\n  split_by = \"sex\"\n)\n\nwhen you are confident that all is in order, run your app and navigate to the age pyramid tab. Do you see the age pyramid?\nBonus You will notice that the sex variable has very simple labels,\n\nHow and where in the server would you recode these ?\n\n\nNice, great to see some more plot up and running, let‚Äôs deal with our final task: adding a demography table to this other tab !\n\n\nAdding a summary table\nHere are task is fairly straightforward, we will use our filtered data (via the reactive expression filtered_data()) and summarise() it to display some summary statistics by age group - we will do this directly in the render function renderTable()\n\nStep 5: Create the summary table\nIn the server function, after the age pyramid render function:\n\nadd a render function to display the summary table\n\n\n\n\n\n\n\nTip\n\n\n\nRemember, you need to assign the render function to an output of an ID that you defined in a placeholder in the UI\n\n\n\nin this render function make a summary table of filtered_data() wich shows by age_group:\n\nTotal Cases\nMale Cases = sum(gender == ‚Äúm‚Äù, na.rm = TRUE),\nFemale Cases = sum(gender == ‚Äúf‚Äù, na.rm = TRUE),\n\n\nBonus - Consider adding more useful statistics: - Case fatality ratio (using the outcome data) - Proportion by age group\n\nThis is start to look professional ! Now that you have multiple outputs, you could spend hours making sure that everything looks nice, consistent and cohesive - unfortunately, we will not do this now as we do not have infinity ahead of use."
  },
  {
    "objectID": "sessions_extra/shiny_intro.html#done",
    "href": "sessions_extra/shiny_intro.html#done",
    "title": "Intoduction to Shiny",
    "section": "Done!",
    "text": "Done!\nAnd this is a clap, you are done ! Congrats on building and tweaking your first shiny app (if it was) ! I hope you have enjoyed this small practical, and wish you the best of luck in the depth of dashboarding in R. You can find below a link to a solution script for the final App (hosted on github).\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_extra/shiny_intro.html#going-further",
    "href": "sessions_extra/shiny_intro.html#going-further",
    "title": "Intoduction to Shiny",
    "section": "Going Further",
    "text": "Going Further\nShiny is a huge field within the R community and there are a lots of amazing resources available online - here is a non-exhaustive list\n\nApplied Epi R Handbook Chapter on Reporting & Dahsboards and Shiny\nMastering Shiny - the free online reference for anyone working with Shiny\nThe Shiny website - full of useful tips and their Gallery for demos and templates\n\nR packages\n\n{epishiny} - provides simple functions that produce engaging, feature-rich interactive visualisations and dashboards from epidemiological data using R‚Äôs shiny. (developped by Epicentre Data Science Team)\n{shinyWidgets} - gives you many many more widgets that can be used in your app\n{highcharter} - R wrapper for the Highcharts (javascript) library for interactive visualisations (license required, potentially paid)"
  },
  {
    "objectID": "pathway.html",
    "href": "pathway.html",
    "title": "Pathway",
    "section": "",
    "text": "These sessions can be followed in order to get a baseline level in R. The series assumes no prior experience in R and is suitable for beginners.\nLooking for more? Want more flexibility? Consider browsing the full session catalog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R\n\n\nYour first steps in R. Learn your way around Rstudio, and meet some common R objects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Importation\n\n\nCreate an Rstudio project, install useful packages and start importing data to work in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Basics\n\n\nAn introduction to data manipulation and cleaning using {dplyr}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation, Filtering and Recoding\n\n\nUsing {dplyr} and conditional logic to filter and recode data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Tables\n\n\nCreate summary tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Data Visualization\n\n\nLearn the basics of buidling plots with ggplot2, and create your first epicurve\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html",
    "href": "sessions_core/04_data_verbs_conditional.html",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "",
    "text": "Understand basic conditional logic statements\nLearn how to filter a data frame using filter()\nLearn how to recode variables using case_when()"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#objectives",
    "href": "sessions_core/04_data_verbs_conditional.html#objectives",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "",
    "text": "Understand basic conditional logic statements\nLearn how to filter a data frame using filter()\nLearn how to recode variables using case_when()"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#setup",
    "href": "sessions_core/04_data_verbs_conditional.html#setup",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "Setup",
    "text": "Setup\nDependencies. This session assumes that you know the basics of data manipulation with {dplyr}. If you need a refresher on this, please review the third session in the learning pathway.\n\nThis session will work with the raw Moissala linelist data, which can be downloaded here:\n\n\n\n  Download Data\n\n\n\n Make sure this dataset is saved into the appropriate subdirectory of your R project and create a new script called filtering_and_recoding_practice.R in your R directory. Add an appropriate header and load the following packages: {here}, {rio}, and {tidyverse}.  Finally, add an import section where you use {here} and {rio} to load your data into an object called df_raw."
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#using-conditional-logic-to-filter-data",
    "href": "sessions_core/04_data_verbs_conditional.html#using-conditional-logic-to-filter-data",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "Using Conditional Logic to Filter Data",
    "text": "Using Conditional Logic to Filter Data\nIn the last session we learned a lot of the core data verbs in {dplyr} for basic manipulation tasks like selecting particular variables of interest and modifying them to better suit our needs. Beyond selecting variables of interest, another common task we have as epidemiologists is selecting observations of interest; ie: filtering our data to look at particular observations that meet a certain criteria.\nFortunately, {dplyr} has our back with the conveniently named function, filter(). To understand how to use it, however, we will need to learn a bit about how to construct conditional logic statements in R. This will be the focus of our session today.\n\nThis Equals That\nThe basic syntax of filter() is pretty simple:\n\n# DO NOT RUN (PSEUDO-CODE)\ndf_raw |&gt;\n  filter([conditional logic statement]) # Keep lines where statement is TRUE\n\nBut what is a conditional logic statement? These are statements that ask ‚ÄúIs this thing true?‚Äù. The simplest conditional logic statement asks ‚Äúdoes this variable equal this value?‚Äù. For example, ‚Äúwas this patient hospitalized?‚Äù. In R, we can ask if one value equals another using ==.\nTo create a filter asking, for each observation, whether the value of hospitalization is equal to yes we can then use the following syntax:\n\ndf_raw |&gt;\n  filter(hospitalisation == 'yes')\n\nWhat filter() is doing here is going down each row of our dataset and asking: ‚Äúfor this row, is the value of hospitalisation equal to \"yes\"?‚Äù. It then returns only the rows where the answer to this question is TRUE.\n\nCreate a filter that selects all of the patients who had a fever, ie: where the value of fever was \"Yes\". The head of fever should look like this:\n\n\n  fever\n1   Yes\n2   Yes\n3   Yes\n4   Yes\n5   Yes\n6   Yes\n\n\nTake a look at your output and then take a look at the head of df_raw. Why does df_raw still contain patients who didn‚Äôt present with fever?\n\n\n\nThis Does Not Equal That\nChecking if something is the same is great, but a lot of the time we might have another question in mind. For example, we might want to know how many patients didn‚Äôt recover, whether this was because they died or because they left against medical advice.\nIn this case, instead of writing == we will instead use !=. So, for example if we want to select all observations where patients didn‚Äôt recover we would write:\n\ndf |&gt;\n  filter(outcome != 'recovered')\n\n\nCreate a filter that selects patients who did not have a card confirmed vaccination status. The head of vacc_status should look like this:\n\n\n  vacc_status\n1          No\n2  Yes - oral\n3          No\n4          No\n5          No\n6          No\n\n\nHint. Remember that you can use count() to check what the options were for vacc_status.\n\n\n\nGreater Than / Less Than\nThe other common question we have is whether a value was greater or less than a particular threshold. For example, how many patients were under 5 years old? Here we will use &lt; and &gt; to evaluate whether a variable is less than or greater than a particular value, respectively.\nFor example, to ask how many patients were less than 60 months old we can write:\n\ndf_raw |&gt;\n  filter(age &lt; 60)\n\n\nCreate a filter that selects all patients with severe accute malnutrition (ie: patients with a MUAC less than 110). The head of muac should look like this:\n\n\n  muac\n1   80\n2   88\n3   60\n4   85\n5   86\n6   68\n\n\nNow create another filter that selects patients who are over 15 years old. The head of your age column should look like this:\n\n\n  age\n1 348\n2 348\n3 312\n4 432\n5 444\n6 324\n\n\n\nSometimes, instead of asking if something is less or greater than a particular value, we want to ask if it is less than or equal to that value. Easy, we just need to add an equal sign! We write &lt;= for ‚Äúless than or equal to‚Äù and &gt;= for ‚Äúgreater than or equal to‚Äù. Careful here, the = must come after &lt; or &gt;, not before.\nSo if we want to ask for how many patients were 10 years of age or younger, we can write:\n\ndf_raw |&gt;\n  filter(age &lt;= 120)\n\n\nCreate a filter that selects all patients with a normal nutrition status, ie: patients with a MUAC greater than or equal to 125. The head of muac should look like this:\n\n\n  muac\n1  244\n2  232\n3  210\n4  220\n5  152\n6  155\n\n\n\n\n\nFilters with Multiple Conditions\nWant to combine several logic statements in a single filter? Easy. We can create a filter with multiple conditions by simply separating each condition with a comma:\n\n# DO NOT RUN (PSEUDO-CODE)\ndf |&gt;\n  filter([condition 1],\n         [condition 2],\n         [condition 3])\n\nSo for example, let‚Äôs say we want to select all patients under five who were hospitalized. In this case we can write:\n\ndf_raw |&gt;\n  filter(age &lt; 5,\n         hospitalised = \"true\")\n\n\nCreate a filter that selects all patients with severe accute malnutrition who were hospitalized in the Koumra health facility. The head of id, sub_prefecture, hospitalisation, and muac should look like this:\n\n\n    id sub_prefecture hospitalisation muac\n1 8624         KOUMRA             yes  103\n2 8939         KOUMRA             yes   67\n3 9957         KOUMRA             yes   71\n\n\nHint. This filter has a condition on both hospitalisation status, sub_prefecture, and muac.\n\n\n\nSummary of Basic Logic Statements\nGood job working through a quick tour of logic statements in R! Here is a handy table to help you remember the main logic statements we have learned so far:\n\n\n\nStatement\nR\n\n\n\n\nIs A the same as B?\nA == B\n\n\nIs A not the same as B\nA != B\n\n\nIs A greater than B?\nA &gt; B\n\n\nIs A greater than or equal to B?\nA &gt;= B\n\n\nIs A less than B?\nA &lt; B\n\n\nIs A less than or equal to B?\nA &lt;= B"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#recoding-with-case_when",
    "href": "sessions_core/04_data_verbs_conditional.html#recoding-with-case_when",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "Recoding with case_when()",
    "text": "Recoding with case_when()\nAs we have seen, conditional logic statements are incredibly useful when trying to filter our data, but you will find that they have many other uses as well. One of their other major use cases for us as epidemiologists is when we need to recode our data. This is where the {dplyr} function case_when() is here to help us.\nThe syntax of case_when() is a little more advanced than what we have seen so far, but we will go slowly and break it down. Once you get the hang of it, case_when() will become a very powerful part of your R toolbelt.\nWe will almost always use case_when() inside of a mutate(), because we will use it either to recode an existing variable or to create a new one. The basic syntax works like this:\n\n# DO NOT RUN (PSEUDO-CODE)\ndf |&gt;\n  mutate(column_name = case_when([first condition] ~ [value when condition is TRUE],\n                                 [second condition] ~ [value when second condition is TRUE],\n                                 .default = [default value])\n\nOk, that‚Äôs a lot. Let‚Äôs break it down.\nSo the first thing to notice is that, with the exception of the last line, each line inside of case_when() has the following format:\n\n[condition] ~ [value when condition is TRUE]\n\nSo for example, if we want our case_when() to say that anytime a patient had a MUAC less than 110 we want to have a value of \"SAM\", we would have something like this:\n\nmuac &lt; 110 ~ 'SAM'\n\nWe can add multiple possible outcomes by adding additional lines. In this case, our next condition might check if the patient is moderately but not severly malnourished using the statement muac &lt; 125 ~ 'MAM'.\nThe last line, with the argument .default gives the value we want case_when() to use when none of the above conditions have been met. In this case, we might give the value 'Normal'.\nTo put this together, if we wanted to create a variable that classifies the malnutrition status of patients using their MUAC, we would write:\n\ndf_raw |&gt;\n  mutate(malnut = case_when(muac &lt; 110 ~ 'SAM',\n                            muac &lt; 125 ~ 'MAM',\n                            .default = 'Normal'))\n\n\nTry running the above code to see if it successfully creates a new column malnut with the malnutrition status of each case. You should get something like this:\n\n\n  muac malnut\n1  244 Normal\n2  232 Normal\n3  123    MAM\n4  210 Normal\n5   80    SAM\n6  220 Normal\n\n\n\nBe careful. The order of your statements is important here. What case_when() will do is go through each statement from top to bottom and assign the first value that is TRUE. So in our above example, case_when() will ask the following questions in sequence:\n\nDoes this patient have SAM (is muac &lt; 110)? If so, assign the value \"SAM\"\nIf the patient didn‚Äôt have SAM, do they have MAM (is muac &lt; 125)? If so, assign the value `‚ÄúMAM‚Äù\nIf none of the above conditions were true, assign the default value \"Normal\"\n\n\nTry reordering the first and second conditions in the above case_when() so that you first check if muac &lt; 125. The head of your new data frame should now look like this:\n\n\n  muac malnut\n1  244 Normal\n2  232 Normal\n3  123    MAM\n4  210 Normal\n5   80    MAM\n6  220 Normal\n\n\nNotice anything different? Save this new data frame to a tmp object and inspect it to see if we still have any patients classified as \"SAM\". Can you figure out why this no longer gives the correct classification?\n\n\n\n\n\n\n\nNote\n\n\n\nThe .default argument in case_when() is not obligatory. If you don‚Äôt include it then case_when() will use NA by default.\n\n\nAs we saw in our above example, case_when() is an easy way of creating new variables based on the values of an existing column. This can be used to classify status (as we saw with malnutrition) or to regroup variables into categories (like age groups).\n\nUse case_when() to create a new variable age_group with three categories: \"&lt; 5 Years\", \"5 - 15 Years\", and \"&gt; 15 Years\". Patients missing age data should be assigned a default value of \"Unknown\". Be careful with your ordering! The head of your new column should look like this:\n\n\n  age     age_group\n1  36     &lt; 5 Years\n2   5     &lt; 5 Years\n3 156 5 - 15  Years\n4   8     &lt; 5 Years\n5   7     &lt; 5 Years\n6   4     &lt; 5 Years\n\n\n\n\nThe %in% operator\nSo now we can regroup variables into categories, great. But we can also use case_when() to standardize the values we see in a variable.\n\nUsing count() inspect the categorical variables in df_raw to check if any have inconsistencies in their coding.\n\nIn our dataset, we see that there are some issues in the way sex was coded. For example, female patients are coded as f, female and femme. That simply won‚Äôt do. Thankfully, we can use case_when() to recode this variable. This time, instead of creating a new variable we will directly update sex:\n\ndf_raw |&gt;\n  mutate(sex = case_when(sex == \"f\" ~ \"Female\",\n                         sex == \"female\" ~ \"Female\",\n                         sex == \"femme\" ~ \"Female\",\n                         sex == \"m\" ~ \"Male\",\n                         sex == \"male\" ~ \"Male\",\n                         sex == \"homme\" ~ \"Male\",\n                         .default = \"Unknown\"))\n\nWell, that works, but it seems awfully repetitive. It would be easier if we could just list all the options that we want to reassign to ‚ÄúFemale‚Äù and ‚ÄúMale‚Äù respectively. This is where the %in% operator is here to help. The %in% operator will check if a value is in a vector of options using the following basic syntax:\n\n# DO NOT RUN (PSEUDO-CODE)\n[value] %in% [vector_of_options]\n\nSo, for example, we could check if the value \"f\" is in the options \"f\", \"female\" using the following:\n\n\"f\" %in% c(\"f\", \"female\")\n\n\nTry running the above statement. What is the data type of your outcome?\n\nSee how the outcome of the above statement is a boolean, ie: a logic outcome? That means we can use it as a condition in case_when()! This means that our verbose code above can now be written as:\n\ndf_raw |&gt;\n  mutate(sex = case_when(sex %in% c(\"f\", \"female\", \"femme\") ~ \"Female\",\n                         sex %in% c(\"m\", \"male\", \"homme\") ~ \"Male\",\n                         .default = \"Unknown\"))\n\nMuch nicer.\n\nUse case_when() and the %in% operator to create a new column vacc_status_strict that has the value \"Yes\" for cases with card confirmed vaccination status, \"No\" for cases who said they were unvaccinated, and \"Unverified\" otherwise. The head of your new column should look like this:\n\n\n  vacc_status_strict\n1         Unverified\n2                 No\n3         Unverified\n4                 No\n5                 No\n6                 No"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#a-last-bit-of-cleanup",
    "href": "sessions_core/04_data_verbs_conditional.html#a-last-bit-of-cleanup",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "A Last Bit of Cleanup",
    "text": "A Last Bit of Cleanup\nNow that we know how to leverage case_when() and conditional logic (in addition to what we learned in the last session, we can actually put together a decent cleaning pipeline. I hope you kept your code from last time handy‚Ä¶\n\nUsing what you have learned above and what you practiced in the last session, create a basic data cleaning pipe that creates a new data frame, df, after doing the following:\n\nRemove the variables full_name and age_unit\nRename the following variables:\n\nage becomes age_months\nsub_prefecture becomes prefecture\nvillage_commune becomes village\nhealth_facility_name becomes facility\n\nAdd a variable age_years with patient age in years\nUpdate region and prefecture to use title case\nUpdate all date columns to use Date type\nCreate a new variable age_group age to include the groups: &lt; 6 months, 6 - 11 months, 12 - 59 months, 5 - 15 years, and &gt; 15 years (patients with unknown age should have a value ‚ÄúUnknown‚Äù)\nRecode sex to have only the values: Female, Male, and Unknown\nRemove any duplicate observations\n\nThe head of your final data should look something like this:\n\n\n  id    sex age_months  region prefecture        village date_onset\n1  1 Female         36 Mandoul   Moissala Sangana Ko√Øtan 2022-08-13\n2  2 Female          5 Mandoul   Moissala      Mousdan 1 2022-08-18\n3  3 Female        156 Mandoul   Moissala     Djaroua Ii 2022-08-17\n4  6   Male          8 Mandoul   Moissala     Monakoumba 2022-08-22\n5  7   Male          7 Mandoul   Moissala      T√©tindaya 2022-08-30\n6 10   Male          4 Mandoul   Moissala      Danamadja 2022-08-30\n  date_consultation hospitalisation date_admission\n1        2022-08-14             yes     2022-08-14\n2        2022-08-25             yes     2022-08-25\n3        2022-08-20            &lt;NA&gt;           &lt;NA&gt;\n4        2022-08-25              no           &lt;NA&gt;\n5        2022-09-02              no           &lt;NA&gt;\n6        2022-09-02             yes     2022-09-02\n                         facility malaria_rdt fever rash cough red_eye\n1 H√¥pital du District de Moissala    negative    No &lt;NA&gt;   Yes      No\n2 H√¥pital du District de Moissala    negative    No   No   Yes      No\n3                      CS Silambi    negative   Yes &lt;NA&gt;    No      No\n4 H√¥pital du District de Moissala    negative    No   No    No    &lt;NA&gt;\n5                      CS Silambi    negative  &lt;NA&gt;   No   Yes     Yes\n6                    Moissala Est    negative   Yes   No    No    &lt;NA&gt;\n  pneumonia encephalitis muac vacc_status vacc_doses   outcome date_outcome\n1        No           No  244        &lt;NA&gt;       &lt;NA&gt; recovered   2022-08-18\n2      &lt;NA&gt;           No  232          No       &lt;NA&gt;      &lt;NA&gt;   2022-08-28\n3        No         &lt;NA&gt;  123  Yes - oral       &lt;NA&gt; recovered         &lt;NA&gt;\n4        No           No  210          No       &lt;NA&gt; recovered         &lt;NA&gt;\n5        No           No   80          No       &lt;NA&gt; recovered         &lt;NA&gt;\n6        No           No  220          No       &lt;NA&gt; recovered   2022-09-03\n   age_years      age_group\n1  3.0000000 12 - 59 months\n2  0.4166667     &lt; 6 months\n3 13.0000000   5 - 15 years\n4  0.6666667  6 - 11 months\n5  0.5833333  6 - 11 months\n6  0.3333333     &lt; 6 months\n\n\n\nAmazing! Let‚Äôs look at how to save this (mostly) clean dataset. Here, we will use the function export() from {rio} and here() from {here} to specify where to save our output:\n\ndf |&gt;\n  export(here('data', 'clean', 'measles_linelist_clean.xlsx'))\n\nNotice here that we are putting our data in the appropriate clean subfolder of data.\n\n\n\n\n\n\nTip\n\n\n\nIn the above example we save our data as an xlsx, which is helpful if you want to be able to open the clean data in Excel. Often, however, we might prefer to use a file with the extension .rds instead. This is a file type specific to R and is more robust to issues related to encoding or date formatting than files like xlsx or csv. To save your above file as an rds all you need to do is change the extension:\n\ndf |&gt;\n  export(here('data', 'clean', 'measles_linelist_clean.rds'))"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#done",
    "href": "sessions_core/04_data_verbs_conditional.html#done",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "Done!",
    "text": "Done!\nVery well done. You‚Äôve learned how to use basic data verbs, conditional logic, and create a basic data cleaning pipeline.\n\n\n\n Solution File"
  },
  {
    "objectID": "sessions_core/04_data_verbs_conditional.html#going-further",
    "href": "sessions_core/04_data_verbs_conditional.html#going-further",
    "title": "Data Manipulation, Filtering and Recoding",
    "section": "Going Further",
    "text": "Going Further\n\nExtra Exercises"
  },
  {
    "objectID": "sessions_core/06_epicurves.html",
    "href": "sessions_core/06_epicurves.html",
    "title": "Basic Data Visualization",
    "section": "",
    "text": "Grasp the very basics of data visualization in R using {ggplot2}\nBuild a basic epicurve"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#objectives",
    "href": "sessions_core/06_epicurves.html#objectives",
    "title": "Basic Data Visualization",
    "section": "",
    "text": "Grasp the very basics of data visualization in R using {ggplot2}\nBuild a basic epicurve"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#introduction",
    "href": "sessions_core/06_epicurves.html#introduction",
    "title": "Basic Data Visualization",
    "section": "Introduction",
    "text": "Introduction\nThis session is a short introduction to data visualization using the popular {ggplot2} package. Keep in mind that visualization in general and even {ggplot2} in particular are huge subjects that we can‚Äôt cover in a single core session. This tutorial is intended as a taster to give you a feel for how plotting is typically done. To do that, we will come back to one of our most beloved epidemiological plots: the epicurve.\nOur final plot will look like this:"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#setup",
    "href": "sessions_core/06_epicurves.html#setup",
    "title": "Basic Data Visualization",
    "section": "Setup",
    "text": "Setup\nDependencies. This session assumes that you know how to use RStudio that you are able to import data and that you know th basic data handling verbs that we have seen in the core sessions so far. If you need a refresher on either of these topics, we encourage you to review the core sessions in the learning pathway.\n\nThis session will use the clean version of the Moissala measles linelist data.\n\n\n\n  Course Folder\n\n\n\n Open your RStudio project and create a new script called epicurves.R with appropriate metadata. Load the following packages: {here}, {rio}, {dplyr}, {lubridate}, and {ggplot2}. Add a section to your script called # IMPORT DATA where you import the clean course dataset (moissala_linelist_clean_EN.rds)."
  },
  {
    "objectID": "sessions_core/06_epicurves.html#paradigms-of-plotting",
    "href": "sessions_core/06_epicurves.html#paradigms-of-plotting",
    "title": "Basic Data Visualization",
    "section": "Paradigms of Plotting",
    "text": "Paradigms of Plotting\nIn R, and indeed in everything, there are a lot of ways to approach data visualization. Two of the biggest paradigms are :\n\nThe All-In-One: this approach is characterized by having a single, typically somewhat complex, function that handles all aspects of building a plot. Base R as well as a variety of specialized packages tend to use this approach.\nLayered (or modular): here, instead of creating a plot with a single function, we will use separate functions to add (or modify) different features of a plot (such as the primary shapes, labels, error bars, themes, etc). This is the strategy used by packages like {ggplot2}, {highcharter}, or {echarts4r}.\n\nAn in depth discussion of why one might use one approach versus another is beyond the scope of this course, though we will note that most modern visualization packages tend to use a layered model. With that in mind, let‚Äôs take a look at the types of layers we are talking about in our ‚Äúlayered‚Äù approach.\n\nBreaking it Down: A Visualization and its Parts\nFor the purpose of this tutorial we will talk about only four visualization components (layers):\n\nCanvas / Data\nPrimary Shapes\nLabels\nTheme\n\nTo illustrate these components, let‚Äôs look at a basic schematic of an epicurve:\n\n\n\n\n\nThe most conceptually complex of the above layers is probably the canvas itself. Much as an artist needs to buy a canvas and conceptualize what they want to paint before they start painting, so too does a user of {ggplot2}. Creating the canvas is where we tell R that we want to start making a plot and what parts of the data that plot will use. Here, for example, we will tell R ‚ÄúI want to make a plot where the x axis represents dates (or weeks sometimes) and the y axis represents cases‚Äù. Once that canvas is set up we can start adding other layers in the same way that an artist would begin adding paint, their signature, or a frame.\nNow, let‚Äôs look at the syntax for these layers in {ggplot2} and how to put them together.\n\n\nGetting Started with {ggplot2}\nThe method of building a ggplot is relatively simple and takes the form:\n\nCreate a canvas using a duo of functions ggplot(aes(...))\nAdd things to the canvas\n\n{ggplot2} takes the idea of ‚Äúadding something to the canvas‚Äù very literally: each new layer will be introduced to your plot using the + sign.\nThe general syntax of a ggplot is then:\n\n# DO NOT RUN (PSEUD-CODE)\ndf |&gt;                      # pipe in your data \n  ggplot(aes(x = ...,      # step 1: create canvas\n             y = ...)) +\n  layer_one(...) +         # step 2: add a first layer\n  layer_two(...) +         # step 3: add another layer\n  ...                      # continue adding layers...\n\nThe number of layers you add depends on how complex you want your plot to be. In our case, we will be adding three layers to our canvas with the following functions:\n\n# DO NOT RUN (PSEUD-CODE)\ndf |&gt;                    # pipe in your data\n  ggplot(aes(x = ...,     # step 1: create canvas\n             y = ...)) +\n  geom_col(...) +         # step 2: add shapes (bars)        \n  labs(...) +             # step 3: add titles\n  theme_classic(...)      # step 4: add a nicer theme\n\nWe can update our above schematic of an epicurve with these functions as follows:\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that in the above example, our very first line is actually our dataset being piped into the ggplot() function. This makes sense since {ggplot2} needs to know what data you‚Äôd like to visualize. But be careful, make sure that this line ends in a pipe (|&gt;) and not in a + sign like t |&gt; he other ones.\n\n\nIn the next part of the tutorial we will go through each of these steps (layers) individually using our course dataset to make your first epicurve."
  },
  {
    "objectID": "sessions_core/06_epicurves.html#sec-epicurve-steps",
    "href": "sessions_core/06_epicurves.html#sec-epicurve-steps",
    "title": "Basic Data Visualization",
    "section": "Building Your First ggplot",
    "text": "Building Your First ggplot\n\nPreparing Your Data: Aggregate by Day\nUltimately we would like to plot an epicurve of daily cases. You may have noticed, our current data is daily, but of course several cases may occur on some days. To plot an epicurve we will need to aggregate data by day. Fortunately, you already learned how to summarize data in previous sessions.\n\nUsing count(), create a new dataframe called df_cases that summarizes the total number of cases observed per day. The head of this data frame should look like this:\n\n\n  date_onset n\n1 2022-08-13 1\n2 2022-08-17 1\n3 2022-08-18 1\n4 2022-08-22 1\n5 2022-08-30 2\n6 2022-09-01 1\n\n\n\nGreat! Now we are ready to make our epicurve. In the following steps, you‚Äôll be asked to use df_cases to plot a classic epicurve of the number of daily admissions. To demonstrate the functions you‚Äôll be using, I will plot the curve of the number of daily hospitalizations as an example. To do that, I‚Äôve built myself another dataframe, df_outcome, which looks like this:\n\n\n  date_admission patients\n1     2022-08-14        1\n2     2022-08-25        1\n3     2022-09-02        1\n4     2022-09-06        1\n5     2022-09-09        1\n6     2022-09-10        1\n\n\n\n\nSet up a Canvas: Initialize a Plot\nThe first step is creating your canvas by specifying your dataset and the names of the columns you‚Äôd like to visualize. This is done using ggplot(aes(...)) with the following syntax:\n\n# DO NOT RUN (PSEUD-CODE)\ndf_data |&gt;\n  ggplot(aes(x = x_axis_variable_name,\n             y = y_axis_variable_name))\n\nFor an epicurve of hospitalizations, I‚Äôd like to plot the days (date_admission) on the x-axis and the number of patients hospitalized (patients) on the y-axis. Let‚Äôs update our pseudo-code to do that:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients))\n\n\n\n\n\n\n\n\nFabulous, take a look at that big beautiful box of potential. This is our empty canvas. In RStudio this plot should show up in the panel on the bottom right of the screen.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nJust like with {dplyr}, we write our column names without quotation marks. This is unsurprising as {ggplot2}, like {dplyr}, is a member of the {tidyverse} and therefore uses similar syntax.\n\n\nNow, you may be wondering what is this aes() function that we‚Äôve nested inside of ggplot()? The short answer is that aes() creates an AESthetic mapping that tells {ggplot2} which columns of our data should be represented by which visual elements of our plot (like the axes, for example).\nAesthetic mappings create a map that defines how data elements (variables) are to be represented by visual elements (like axes, colors, and sizes). For example, here we are mapping the days to the x-axis and the number of patients to the y-axis. We could also imagine, for example, an epicurve where bars are colored based on whether patients lived or died. This would be an example where the variable outcome is being mapped to the visual element of color.\nFor now it is enough to know that aes() is the place where you will define your x-and y-axis.\n\nCreate a new section in your script called # PLOT EPICURVE. Then create an empty canvas for your epicurve using df_cases.\n\nAt this point, your plot should look like this:\n\n\n\n\n\n\n\n\n\nExcellent! Now let‚Äôs add some bars.\n\n\nPlot the Bars\nNow that we have our canvas, it‚Äôs time to add some shapes. In {ggplot2}, the shapes plotted on a figure are called geometries. Geometries are the primary visual representation of your data and should feel pretty familiar. A few common types of geometries include:\n\nBar Plots (geom_col() or geom_bar())\nHistograms (geom_hist())\nScatterplots (geom_point())\nLine Plots (geom_line())\nBoxplots (geom_boxplot())\n\nToday, we‚Äôre doing epicurves so we are most interested in learning how to make a bar plot. In our case, we will be using geom_col(). Remember that adding a new layer (in this case a geometry) to our ggplot is as simple as using a +, so we can add bars to the epicurve of hospitalized cases in the following way:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col()\n\n\n\n\n\n\n\n\nBrilliant! That sure looks like an epicurve to me. Though it does look a bit‚Ä¶grey. If we‚Äôd like to update the color of our bars (called the fill), we simply need to add the fill argument to geom_col().\nLet‚Äôs give it a try:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\")\n\n\n\n\n\n\n\n\n\nUpdate your epicurve plot to add bars with the color #E4573.\n\nYour plot should now look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the {ggplot2} framework, layers must be added to an existing canvas. This means that running geom_col() by itself will not produce any visual output. This, however, makes sense. Continuing with our analogy of ggplots being like paintings, running geom_col() by itself would be like having paint with no canvas to put it on.\n\n\nLooking good. Now it‚Äôs time to make our plot just a bit more informative and just a bit more attractive by adding labels and a nicer theme.\n\n\nAdd Some Labels\nA good plot needs some good labeling; n is hardly an informative axis title. Fortunately, {ggplot2} makes adding labels easy with the function labs(). This function will accept a variety of arguments allowing you to add a variety of label/title elements to your plot, for example:\n\nAxis Titles (x = and y =)\nPlot Title (title =)\nCaption\n\nAs for other layers, we can include a label layer by adding labs() to our current plot with the + sign:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date\",\n       y = \"Daily Patients\",\n       title = \"Measles Hospitalizations in Mandoul Region (Chad)\")\n\n\n\n\n\n\n\n\n\nUpdate your epicurve plot to add some reasonable axis labels and a nice title.  Extra Credit! Try adding a data source using caption.\n\nYour plot might now look like (for example):\n\n\n\n\n\n\n\n\n\n\n\nAdd a Theme\nIf we wanted to, we could stop here if our goal is to produce an informal plot. Ideally, however, it would be nice to use a somewhat more attractive theme and to increase the text size. To do this, we will add one last layer to our plot: a theme layer. Much like how geometries in {ggplot2} all start with geom_, all themes start with theme_. There are several themes available to you and you can check out what they look like on the {ggplot2} website.\nToday, we will use theme_classic(), which offers a simple but elegant output:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date\",\n       y = \"Daily Patients\",\n       title = \"Measles Hospitalizations in Mandoul Region (Chad)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nOk, nice. But we‚Äôd also like to increase the size of that tiny font. To do that we can adjust the base_size argument:\n\ndf_hospital |&gt;\n  ggplot(aes(x = date_admission,\n             y = patients)) +\n  geom_col(fill = \"#2E4573\") +\n  labs(x = \"Date\",\n       y = \"Daily Patients\",\n       title = \"Measles Hospitalizations in Mandoul Region (Chad)\") +\n  theme_classic(base_size = 17)\n\n\n\n\n\n\n\n\nThat looks better! Keep in mind that the font size needed will depend on what the plot is going to be used for (i.e.¬†a presentation, an informal review, or a final report). Similarly, the exact theme you will want to use is ultimately a subjective choice. While there are guidelines, data visualization is as much an art as a science.\n\nAdd one final layer to your plot that adds a theme of your choice with an appropriate base_size.\n\n\n\nSave your plot\nIf you would like to save your epicurve, you can click on the ‚ÄúExport‚Äù button in the plot panel of RStudio:"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#done",
    "href": "sessions_core/06_epicurves.html#done",
    "title": "Basic Data Visualization",
    "section": "Done!",
    "text": "Done!\nVery well done team! You have build your first epicurve!\n\n\n\n Solutions file"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#go-further",
    "href": "sessions_core/06_epicurves.html#go-further",
    "title": "Basic Data Visualization",
    "section": "Go Further",
    "text": "Go Further\n\nExtra Exercises\n\nUse the theme_minimal() on one of your graph, with a base size font of 18.\nGo to this site, pick a color and update the color of your bars.\n\n\n\nChallenge Exercises\n\nInstead of aggregating by date, count the number of patients by sub-prefecture. Try to adapt your epicurve code to create a barplot of the number of patients by sub-prefecture.\n\n\n\nSatellites\n\nWeekly Epicurves\nFaceting"
  },
  {
    "objectID": "sessions_core/06_epicurves.html#resources",
    "href": "sessions_core/06_epicurves.html#resources",
    "title": "Basic Data Visualization",
    "section": "Resources",
    "text": "Resources\n\nA full book on using {ggplot2}\n\nA whole chapter on epicurves"
  },
  {
    "objectID": "sessions_core/01_introduction.html",
    "href": "sessions_core/01_introduction.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Familiarize yourself with RStudio\nLearn how to work with the console\nCreate and execute a script\nCreate basic R objects, including vectors and data frames"
  },
  {
    "objectID": "sessions_core/01_introduction.html#objectives",
    "href": "sessions_core/01_introduction.html#objectives",
    "title": "Introduction to R",
    "section": "",
    "text": "Familiarize yourself with RStudio\nLearn how to work with the console\nCreate and execute a script\nCreate basic R objects, including vectors and data frames"
  },
  {
    "objectID": "sessions_core/01_introduction.html#exercise-format",
    "href": "sessions_core/01_introduction.html#exercise-format",
    "title": "Introduction to R",
    "section": "Exercise Format",
    "text": "Exercise Format\nThese exercises are in the format of a self-paced tutorial containing short explanations of key concepts, examples, and exercises for you to follow. The course uses a ‚Äúlearning by doing‚Äù approach, and while this first session will start with a lot of time exploring the RStudio interface, future sessions will focus heavily on having you write your own code.\nInstructions for exercises will be given in the following formats:\n\nThis is a general action block. You will typically see it at the beginning of a session with instructions about the setup for that lesson.\n Example: Open a blank new script and name it my_first_script.R.\n\n\nThis is a code block, it indicates a coding exercise where you will actually write your own code.\n Example: Create an object called region that contains the value \"Mandoul\".\n\n\nThis is an observation block, it will have instructions about something that you are expected to look at or investigate.\n Example: Inspect the RStudio interface.\n\nAs you move through these exercises, you may run into some errors, which occur when R is unable to complete a command. This can happen for many reasons: maybe you misspelled the name of an object, asked R to look for a file that doesn‚Äôt exist, or provided the wrong type of data to a function. Whenever an error occurs, R will stop any ongoing calculation and give you a message explaining what went wrong. Having errors is completely normal and happens to all programmers, novice and expert. Much like a natural language, R is something you will get better at the more you practice and work through your mistakes."
  },
  {
    "objectID": "sessions_core/01_introduction.html#rstudio-and-r",
    "href": "sessions_core/01_introduction.html#rstudio-and-r",
    "title": "Introduction to R",
    "section": "RStudio and R",
    "text": "RStudio and R\nR is a functional programming language that can be used to clean and manipulate data, run analyses (especially statistical ones), visualize results, and much more.\nRStudio is a piece of software that provides a user-friendly interface for R (also called an IDE, for Integrated Development Environment). While using a graphical interface isn‚Äôt required, it is strongly recommended for beginners.\n\nGetting Started with RStudio\nLet‚Äôs get started!\n\nOpen RStudio using the start menu or desktop shortcut; if RStudio is already open, please close it and open it again.\n\nYou should see an interface that looks something like this:\n\n\n\nView of the Rstudio IDE interface at opening\n\n\n\nInspect the RStudio interface.\n\nYou will have either three or four panels, including:\n\nUpper Right Corner\nTo the upper right there will be a panel with several tabs. Many of these are beyond the scope of this course, but we will use the following two tabs during the course:\n\nEnvironment. A list of the objects saved by the user in the current session. Because you‚Äôve just started a new session, your environment should be empty.\nHistory. A record of all the commands you have executed during the current session.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can think of an R session like you would think of starting up a computer. Whenever a session starts, everything is blank and ready for computation in the same way that there aren‚Äôt any programs open when you first turn on your computer. In general, we encourage you to stop and start your R sessions regularly, you may just find that turning it off an on again will fix some of your bugs.\n\n\n\n\nBottom Right Corner\nTo the bottom right there will be another multi tab panel, including:\n\nFiles. A file explorer for the working directory, which is the folder location where R is currently working.\nPlots. A location where RStudio will display static visualizations; this tab should be empty for the moment.\nPackages. A list of all the R packages installed on your computer. Packages are collections of functions that help extend the functionality of R, and we will discuss them in greater detail in the next lesson.\nHelp. A place to read help pages and documentation for functions and packages.\nViewer. A location where RStudio will display html outputs such as tables, interactive widgets, or even full on dashboards.\n\n\n\nLeft Side\n\nTo the left (or bottom left if you have four panels) you should see the console, where R itself is run.\nTo the top left (if you have four panels) will be any open scripts.\n\nIn the next two sections, let‚Äôs talk about the console and scripts in more detail.\n\n\n\nThe Console\nThe console is where R itself is run.\nWhenever you open a new session, R will start by printing a bit of information about your set up, such as your R version number. Below this there should be a line containing the &gt; symbol and a blinking cursor. To run a command in R, you simply need to type it in after this &gt; and press Enter. R will then process your code and print the result (if there is one). A new &gt; line will then appear ready for the next command.\n\n\n\n\n\n\nImportant\n\n\n\nIf the last line shown in the console starts with a + instead of a &gt; that means the console is not ready for a new command either because it is still processing a previous one or because it received a bit of incomplete code. If at any point you would like to cancel an ongoing or incomplete command, press Esc.\n\n\n\nRun the following commands in the console one line at a time and observe the output.\n\n5 + 90\n\n6 * 171\n\n189 / 36.6\n\n92^3\n\n(12 + 9)^4 / 1000\n\nNow, run the following command. Note that the final ) is missing, making the command incomplete. What happens when you do this?\n\n3 / (2 + 97\n\n\nYou may have noticed in the above examples that our code includes a lot of spaces between characters. This is not by accident. It is considered best practice to include spaces around most operators, such as +, -, *, /, &lt;, &gt;, =, and &lt;-. Not only do these spaces make your code easier for other people to read and understand, in some (rare) cases they may even be necessary to avoid errors. That said, do be aware that there are a small number of operators that should not be surrounded by spaces, such as ^, . and :.\n\n1+29+4.8/3*3           # BAD\n1 + 29 + 4.8 / 3 * 3   # GOOD\n\n1 ^ 2 # BAD\n1^2   # GOOD\n\nWe can also run functions in the console. We will discuss functions in more depth later in this lesson, but meanwhile know that the idea of functions in R is very similar to the one in Excel, where you no doubt are familiar with functions such as SUM or MEAN.\n\nRun the following commands in the console (one line at a time).\n\n# Find the minimum value\nmin(5, 10)\nmin(1, 8, 56, 0.3)\n\n# Find the maximum value\nmax(568, 258, 314)\n\n\n\n\nScripts\nScripts are text files that contain a series of commands for a particular programming language. The extension of the file indicates which language the commands were written in, and we will be using .R. Scripts allow us to create code that can be reused, shared, and even automated.\n\nWriting Your First Script\n\n\n\nSteps to create a new script in the RStudio IDE\n\n\nTo create a new script, follow the menu File &gt; New File &gt; R Script. Alternatively, you can click on the small green + sign just below the File menu or use the keyboard shortcut CTRL+SHIFT+N. This new and unsaved script will appear as a blank document in the top left panel.\nTo save your script, either use the menu File &gt; Save As or the keyboard shortcut CTRL+S.\n\nCreate and save a new script called discovery.R. Don‚Äôt forget to include the .R extension. For now, you can save it on your Desktop or any convenient location, but we will talk more about organizing your scripts in the next session.\n\n\n\nExecuting Code from a Script\nTo run code from a script simply place your cursor on the line you wish to run (or select multiple lines) and do one of the following:\n\nClick the Run icon at the top right of the script panel\nUse the shortcut CTRL+Enter (cursor will move to the next line afterwards)\nUse the shortcut ALT+Enter (cursor will stay on the current line afterwards)\n\n\nCopy the code you ran in the previous exercises into your script and run it using each of the above methods.\nFrom now on, you will write your code in your script and execute it from there, unless told otherwise in the instructions.\n\n\n\nComments\nIn R, any text prefaced by a # (until the end of a line) is called a comment. R does not consider comments to be code and will ignore them whenever you run your scripts. This makes comments an excellent way to document your code.\n\n# This is a comment\n\n2 + 3  # This is also a comment\n\nIt is helpful to future you and others to start your scripts with a few commented lines providing some information about the file.\n\n#### IMPORT & PREPARE DATA ####\n# Author :  Mathilde Mousset\n# Creation Date : 23/11/2024\n# Last Update : 30/11/2024\n# Description : Import and clean measles surveillance data from Moissala\n\n\nAdd some comments to the top of your script describing it.\n\nComments are also a handy way to split longer scripts into thematic sections, such as ‚ÄúData Importation‚Äù, ‚ÄúAnalysis‚Äù, ‚ÄúVisualization‚Äù, etc. For example:\n\n# NAME OF SECTION 1 -----------------------------------------------             \n\n# NAME OF SECTION 2 -----------------------------------------------             \n\n\nUse comments to create sections in your script that correspond to the main sections in this tutorial.\n\nFinally, comments allow us write helpful notes for our colleagues (and our future selves) that can help them understand the code and why we wrote it the way we did. The general guidance is to focus on comments that explain the ‚Äúwhy‚Äù rather than the ‚Äúwhat‚Äù. This is because the ‚Äúwhat‚Äù of well written code should be relatively self explanatory.\nThis comment, for example, is completely superfluous:\n\n1 + 3  # Code to add one to three\n\nBy comparison, here are a few use cases that would warrant comments:\n\nYou define a constant, say a seroprevalence threshold value. You may want to add a comment providing the reference for where the value comes from.\nYour code contains a value or file name that needs to be updated every week. You should indicate this with a comment to ensure that anyone else using the code is aware.\nYou use a rare command or package that your colleague may not know or may find counter intuitive. You can use a comment to explain the rational behind that decision.\n\nThat being said, you are learning, and the scripts you are writing during this course are your notes, so feel free to us as many comments (of the ‚Äúwhat‚Äù and ‚Äúwhy‚Äù sort) as you need. You will naturally write less comments in the future, when some of the things that seem alien now become natural.\n\n\n\n\n\n\nTip\n\n\n\nYou can comment a selected line with the shortcut CTRL+SHIFT+C.\nYou can add a first level section with CTRL+SHIFT+R.\n\n\n\nAdd some comments to describe the code that you‚Äôve written thus far in your script."
  },
  {
    "objectID": "sessions_core/01_introduction.html#data-types",
    "href": "sessions_core/01_introduction.html#data-types",
    "title": "Introduction to R",
    "section": "Data Types",
    "text": "Data Types\nR has several different data types. The ones we will see most often in this course include:\n\nnumeric\nstring (text)\nboolean (TRUE / FALSE)\ndate\nfactor\n\n\nNumerics\nThe numeric type includes both integers and doubles (numbers that include a decimal) and can be created by simply writing the ‚Äúnaked‚Äù value into your script or console.\n\n\nStrings\nStrings are the R version of text and can be created by surrounding text with single or double quotation marks, for example \"district\" or 'cases' (double quotations are typically considered best practice).\n\nCompare the output in the console for the following commands:\n\n28         # numeric\n\"28\"       # text\n28 + \"28\"  # produces an error\n\n\nThe last command above will give an error because we cannot perform arithmetic operations that combine text and numbers.\n\n\n\n\n\n\nImportant\n\n\n\nR is case sensitive, meaning that the string \"ABC\" is not the same as \"abc\".\n\n\n\nIf you would like to create a string that contains a quotation mark the best practice is to escape the character by putting a \\ in front of it, ie: \"She said \\\"Hello\\\" then left\" or 'it\\'s a beautiful day'. Equivalently, if you used a double quotation to create the string you can use single quotes inside of it freely (ie: \"it's a beautiful day\") and vice versa (i.e.: 'She said \"Hello\" then left').\n\n\n\nBoolean (Logical)\nThe boolean (or ‚Äúlogical‚Äù) type stores true/false values and is created by writing either TRUE or FALSE without quotation marks.\nInternally, R thinks of TRUE and FALSE as being a special version of 1 and 0 respectively, and boolean values can be easily translated to these numeric equivalents for arithmetic operations.\n\n\n\n\n\n\nNote\n\n\n\nYou may find people using T or F but this is discouraged as T and F can also be used as object or variable names. TRUE and FALSE, however, are protected in R, meaning they cannot be reassigned to another value.\n\n\n\n\nDetermining the Type of an Object\nThere are several functions to determine the type of an object (often called the class of the object in R).\n\nType the following commands in your script and run them:\n\n# Get the Type of an Object\nclass(28)  \nclass(\"Mandoul\")\n\n# Test the Type of an Object\nis.numeric(28)\nis.numeric(\"Mandoul\")\nis.character(\"Mandoul\")\n\nis.numeric(TRUE)\nis.character(TRUE)\nis.logical(FALSE)"
  },
  {
    "objectID": "sessions_core/01_introduction.html#sec-assignement-operator",
    "href": "sessions_core/01_introduction.html#sec-assignement-operator",
    "title": "Introduction to R",
    "section": "Creating an Object",
    "text": "Creating an Object\nIn R, pretty much everything is an object, including functions, scalar values, and other more complex data structures. Before introducing these structures, let‚Äôs take an important detour to discuss how objects are saved into your environment.\nOften, we will want to reuse the same values or data throughout a script and it is therefore very useful to store them as objects in our environment. To do this we use the assignment operator, &lt;-.\n\nLook at the environment panel on the top right, verifying that it is empty, then type the following command in your script and run it to save a variable called cases into your environment.\n\ncases &lt;- 28\n\nLook at the environment again. Is it still empty?\n\nIf you‚Äôd like to access the value of your new object, cases, you simply need to execute its name.\n\ncases\n\n[1] 28\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe reason we need to wrap strings in quotation marks is actually to allow R to differentiate between strings (\"cases\" and object names cases).\n\n\nOnce created, objects can be used in other commands:\n\ncases + 5\n\n[1] 33\n\n\n\nFrom your script, create an object called region that contains the value \"Mandoul\". Do you see it in your environment?\n\n\n\n\n\n\n\nTip\n\n\n\nDon‚Äôt forget that we should always surround &lt;- with spaces to improve readability and avoid errors.\n\nx&lt;-3     # BAD\nx &lt;- 3   # GOOD\n\n\n\n\nUpdating an Object\nWe often want to update the value stored in an object. To do this, we simply assign a new value with the same syntax we used to create it in the first place:\n\ncases &lt;- 32\n\n\nUpdate the value of region to \"Moyen Chari\".\n\n\n\nObject Names\nWhen naming your objects, there are a few (relatively) hard rules:\n\nDon‚Äôt start with a number\nDon‚Äôt use spaces (use a _ instead)\nDon‚Äôt use protected values (like TRUE and FALSE) or function names (like mean)\nDon‚Äôt use capital letters\n\nBeyond these hard rules, there are also more subjective best practices and personal styles. In general aim for names that are short and descriptive:\n\na &lt;- 19                             # BAD (not informative)\nage_du_patient_a_l_admission &lt;- 19  # BAD (too long)\nage &lt;- 19                           # GOOD\n\nGiving your objects clear and informative names helps to make your code readable, making it easy for others to understand without the need for checking the data dictionary every two seconds."
  },
  {
    "objectID": "sessions_core/01_introduction.html#data-structures",
    "href": "sessions_core/01_introduction.html#data-structures",
    "title": "Introduction to R",
    "section": "Data Structures",
    "text": "Data Structures\nUp until now we have looked only at simple objects that store single values, let‚Äôs now turn our focus to more complex structures that can store entire datasets.\n\nVectors\nWe can collect multiple values (such as numerics or strings) into a single object, called a vector.\nTechnically, there are several types of vectors, for example:\n\nSimple vectors (or atomic vectors) can only contain one type of values. For example, a numeric vector 2, 4, 6 or a string vector \"Mandoul\", \"Moyen Chari\".\nRecursive vectors (usually called lists) are far more complex and can contain multiple dimensions and types of data. We will not learn about them in this lesson.\n\nThis course will not go into detail on the more abstract concepts behind these structures and instead focus only on those you will encounter most often in your daily work.\n\nSimple Vectors\nSimple vectors can contain one or more values of a single data type, they thus have two key properties: length and type. For the purpose of this class, we will use the terms ‚Äúsimple vector‚Äù and ‚Äúvector‚Äù interchangeably (as is typical in the R community).\nYou‚Äôve technically already created your first simple vector when you built cases and region. These were simply vectors with a length of one. To create a vector with more than one value, we will use the function c() (mnemonic):\n\ncases &lt;- c(2, 5, 8, 0, 4)\n\n\nUpdate cases with the above values and update region to create a string vector containing the values: Mandoul, Moyen-Chari, Logone Oriental, Tibesti, and Logone Occidental.\n\nWe can now use functions on the objects we have created:\n\nmean(cases)      # calculate the mean value of the cases vector\n\n[1] 3.8\n\ntoupper(region)  # convert all the values in region to upper case\n\n[1] \"MANDOUL\"           \"MOYEN-CHARI\"       \"LOGONE ORIENTAL\"  \n[4] \"TIBESTI\"           \"LOGONE OCCIDENTAL\"\n\n\n\nLet‚Äôs use some functions! Try to write code that does the following:\n\nCalculate the sum of cases using the function sum()\nConvert the text in region to lowercase using the function tolower()\n\n\n\n\n\nAccessing the Values of a Vector\nIt is possible to access the value of a vector using square brackets containing the index (position) of the desired value, ie: [3] or [189].\n\ncases[2]   # 2nd value of cases\n\n[1] 5\n\ncases[10]  # 10th value of cases\n\n[1] NA\n\n\nOoops it does not exist! We will come back to what this NA means in the Missing Values section.\nWe can also access a range of values, just as we might do in Excel. To create a range we use the : operator to separate the desired minimum and maximum index:\n\ncases[2:4]  # 2nd to 4th values of cases\n\n[1] 5 8 0\n\n\n\nGet the 3rd value of region.\nWrite code to access the values ‚ÄúMandoul‚Äù and ‚ÄúMoyen-Chari‚Äù in the vector region.\n\n\n\nData frames\nData frames are tabular structures / 2D tables with rows and columns. It is very similar to a ‚Äútable‚Äù structure in Excel. As epidemiologists, this type of data structure is perhaps the most useful and you will likely use them on a daily basis, to store linelist data for example.\n\nCreating a data frame\nWe can create a data frame using the function data.frame():\n\ndata.frame(col1 = c(1, 4, 2, 9),\n           col2 = c(\"a bit of text\", \"some more text\", \"hello\", \"epidemiologists!\"))\n\n  col1             col2\n1    1    a bit of text\n2    4   some more text\n3    2            hello\n4    9 epidemiologists!\n\n\nSee how col1 was created from a numeric vector, and col2 from a vector of strings. Here we chose the names of the columns (col1 and col2), which is the normal way, but you can run the code without to see how R handles names by default.\n\nIn your script, create a data frame called data_cases that contains cases in one column and region in the other.\n\n\n\nExploring a data frame\ndata_cases should now appear in your environment. You can click on the blue circle with a white triangle in to see some additional information, or click on its name to open the object in the same pane as the scripts to view it.\n\n\n\nThe data_case data frame now appears in the Environment pane\n\n\nThere are several handy functions we can use to explore a data frame:\n\nRun the following commands and try to determine what type of information they are returning.\n\nstr(data_cases)     # STRucture of the object\ndim(data_cases)     # DIMension of the object\nnrow(data_cases)    # Number of ROWs\nncol(data_cases)    # Number of COLumns\nnames(data_cases)   # column NAMES\n\n\nLet‚Äôs practice a bit more! R comes with several built in data sets that can be accessed directly, including one called iris. It is convenient today as we have not learned to import data in R yet (don‚Äôt worry, we will work on linelist data from the second session then onwards).\nWe can see the first few lines of this data frame using the function head():\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\nHow many rows and columns are there in iris? What are the names of its columns?\n\n\n\nAccessing Data in a data frame\nIn R, there are several methods for accessing the rows and/or columns of a data frame. In this introductory session we will focus on the [ ] syntax.\nWe use square brackets to access single values or ranges within our data frame. To do this we must give R both a row number (or range of rows) and column number/name (or range of columns), using the syntax [row, column].\n\ndata_cases[1, 2]          # the value of row one, column 2\n\n[1] \"Mandoul\"\n\ndata_cases[1, \"region\"]   # first value in the region column\n\n[1] \"Mandoul\"\n\n\nIf we want to access all of the rows (or columns) we can simple leave a space in the place of the number/name:\n\ndata_cases[1, ]           # values of all columns in row one\n\n  cases  region\n1     2 Mandoul\n\ndata_cases[2:4, ]         # values of all columns for rows 2 through 4\n\n  cases         region\n2     5       Sud Kivu\n3     8 Kasai oriental\n4     0          Kasai\n\ndata_cases[ , \"region\"]   # values of all rows for the region column\n\n[1] \"Mandoul\"        \"Sud Kivu\"       \"Kasai oriental\" \"Kasai\"         \n[5] \"Haut Katanga\"  \n\n\nWe can even select multiple non-consecutive indices by using a numeric vector:\n\ndata_cases[c(1, 3), ]  # lines 1 and 3 (all columns)\n\n  cases         region\n1     2        Mandoul\n3     8 Kasai oriental\n\n\nDo be careful, as the type of output returned when extracting data from a data frame can sometimes depend on the style of indexing used:\n\nstr(data_cases[1 , ])   # returns a data frame\n\n'data.frame':   1 obs. of  2 variables:\n $ cases : num 2\n $ region: chr \"Mandoul\"\n\nstr(data_cases[ , 1])   # returns a simple vector\n\n num [1:5] 2 5 8 0 4\n\n\nAnother syntaxt to extract the various columns of a data frame:\n\ndata_cases[2]           # returns the second column (as a data frame)\n\n          region\n1        Mandoul\n2       Sud Kivu\n3 Kasai oriental\n4          Kasai\n5   Haut Katanga\n\ndata_cases[\"region\"]    # returns the region column (as a data frame)\n\n          region\n1        Mandoul\n2       Sud Kivu\n3 Kasai oriental\n4          Kasai\n5   Haut Katanga\n\n\nNotice that these commands returned single-column data frames.\n\nWrite some code to:\n\nextract the third value in the region column of your data frame\n\nextract the second and third values of the cases column\n\ncalculate the sum of the cases column of your data frame"
  },
  {
    "objectID": "sessions_core/01_introduction.html#sec-missing-values",
    "href": "sessions_core/01_introduction.html#sec-missing-values",
    "title": "Introduction to R",
    "section": "Missing Values",
    "text": "Missing Values\nAs epidemiologists, we work with missing data all the time. In R, missing values are coded using a special value: NA (meaning Not Available). NA is somewhat unique in R as it doesn‚Äôt per se have a fixed type, rather, it will take on the type of the values around it. For example, an NA in a numeric column will then take on the numeric type. We will discuss the idea of missing data in more depth in later sessions of the course."
  },
  {
    "objectID": "sessions_core/01_introduction.html#sec-functions",
    "href": "sessions_core/01_introduction.html#sec-functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\nFunctions are objects that contain commands (instead of values) that are run whenever the function is called. You are without doubt familiar with functions in Excel such as SUM or MEAN and the idea of functions in R is exactly the same.\nMost functions require some sort of input, such as a dataset or parameter. These inputs are called arguments and are normally named. For example, when we ran sum(cases), we provided the vector cases as the first (and only) argument to the function sum().\nOften, a function will have a combination of both required and optional arguments. The first argument of a function is almost always required and is typically a dataset. As an obligatory and rather obvious argument, most people omit its name when calling a function; ie: i.e.¬†people will write mean(cases) instead of mean(x = cases). Optional arguments on the other hand are usually added using their name, i.e.: mean(x = cases, na.rm = TRUE).\nOptional arguments typically have default values and we only include them when we want to change their defaults (and thus change the default behavior of the function). For example, the na.rm argument of mean() determines whether R will ignore missing values when calculating a mean. The default state of the na.rm argument is FALSE, so by default, the mean performed on data with missing values will always return NA as the result:\n\nmean(c(1, 3, NA))\n\n[1] NA\n\n\nThis is true for many arithmetic operations in R. If we want R to calculate the mean on whatever data is available (and ignore the missing values) we need to explicitly set na.rm = TRUE:\n\nmean(c(1, 3, NA), na.rm = TRUE)\n\n[1] 2\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that arguments are separated by commas. These commas should always be followed by a space and whenever a named argument is used the = should be surrounded by spaces:\n\nmean(cases,na.rm=TRUE)     # BAD\nmean(cases, na.rm = TRUE)  # GOOD\n\nAs you work with increasingly complex functions, you may start to have a lot of arguments. For readability, it is typically recommended to split each argument onto its own line:\n\nmean(cases, \n     na.rm = TRUE) \n\n\n\nWhat happens if we put the arguments in the wrong order? If you provided the name of the arguments in you command, the function will still work exactly as expected. That being said, doing this would make your code harder to read and we encourage you to stick with a standard order of putting obligatory arguments like data first.\n\n# technically functional but hard to read:\nmean(na.rm = TRUE,  \n     x = cases) \n\n# better:\nmean(cases,         \n     na.rm = TRUE)\n\nOf course, if you mess up the ordering of arguments and didn‚Äôt include their names your code will not work as expected, or even throw an error:\n\nmean(TRUE, cases)   # not what you expect"
  },
  {
    "objectID": "sessions_core/01_introduction.html#done",
    "href": "sessions_core/01_introduction.html#done",
    "title": "Introduction to R",
    "section": "Done!",
    "text": "Done!\nThat‚Äôs all for this session, congratulations on taking your first steps with R and RStudio!\n\n\n\n Solutions file"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "This page will (eventually) contain external resources to continue your R learning journey."
  }
]